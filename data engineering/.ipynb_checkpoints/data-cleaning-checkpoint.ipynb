{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b1cb43-3be1-4f87-8c26-581f591afdce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922ce52-f8a8-47c3-b1fa-a8fdc1eaa87e",
   "metadata": {},
   "source": [
    "Here we will try to extract more data from the describtions of each job using simple NER, POS & Regex expressions<br>\n",
    "We will also need to clean the data after we extract it and make sure that there isn't any NANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14378b-e0c1-4b01-aa5e-f9cb7b615ba0",
   "metadata": {},
   "source": [
    "**What will I clean in the data:**\n",
    "- Salaries data\n",
    "- the Outliers\n",
    "- Useless columns\n",
    "- Tiny problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1713b-7169-4322-bc7b-f8c5acc5dfcc",
   "metadata": {},
   "source": [
    "\n",
    "**Things we will extract from the describtions:**\n",
    "- Removing useless columns\n",
    "- Cleaning linkedIn salary data\n",
    "- Requeird years of experience\n",
    "- Requeird programming languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e990-2723-45e1-92cf-3a59f97a2af8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Importing the <span style= 'color: #48e0dc'>Packeges</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9d4d17-4dbd-4c66-b031-8a00b62a4b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib\n",
    "\n",
    "import pandas             as pd\n",
    "import numpy              as np\n",
    "import seaborn            as sns\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "from io                   import StringIO \n",
    "from dotenv               import load_dotenv\n",
    "from bs4                  import BeautifulSoup\n",
    "from google.cloud         import bigquery\n",
    "from wordcloud            import WordCloud\n",
    "from IPython.display      import set_matplotlib_formats\n",
    "from collections          import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a93668f-55d4-4a1f-80a7-bb55ab0ba449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "load_dotenv()\n",
    "plt.rcParams['font.family'] = 'Candara'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(1, '../my_encrypter')\n",
    "import encrypt\n",
    "\n",
    "encrypt.decrypt_json_file('../credentials.json')\n",
    "\n",
    "credentials_path :str = '../credentials.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "\n",
    "FONT             :int  = 17\n",
    "COLORS           :list = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS         :list = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                          'uint16', 'uint32', 'uint64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e6b0e-1e94-4f3a-a984-86ecc216d425",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <center><strong>Preparing the data <span style= 'color: #5cd3f7'>Extracting</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff15e2b-3516-4616-b543-0476c0e90879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5210b397-3315-4f5f-97d7-a49f3040d50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id :str = 'data-jobs-analysis-db'\n",
    "dataset_id :str = 'data_jobs_analysis_db'\n",
    "\n",
    "client   = bigquery.Client(project= project_id)\n",
    "\n",
    "linkedin_jobs    = client.query(f'SELECT * FROM {dataset_id}.linkedin_jobs;').to_dataframe()\n",
    "upwork_profiles  = client.query(f'SELECT * FROM {dataset_id}.upwork_profiles;').to_dataframe()\n",
    "guru_profiles    = client.query(f'SELECT * FROM {dataset_id}.guru_profiles;').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87fdd6d5-784b-47b9-88e0-132461b49cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3455 entries, 0 to 3454\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   jobs_locations      3455 non-null   object\n",
      " 1   listing_dates       3349 non-null   object\n",
      " 2   jobs_titles         3455 non-null   object\n",
      " 3   companies_names     3455 non-null   object\n",
      " 4   jobs_links          3455 non-null   object\n",
      " 5   describtion         3455 non-null   object\n",
      " 6   location_type       3455 non-null   object\n",
      " 7   employment_type     3455 non-null   object\n",
      " 8   industry            3414 non-null   object\n",
      " 9   reqierd_credential  3455 non-null   object\n",
      " 10  country             3455 non-null   object\n",
      " 11  job_title           3455 non-null   object\n",
      " 12  total_jobs          3455 non-null   Int64 \n",
      "dtypes: Int64(1), object(12)\n",
      "memory usage: 354.4+ KB\n"
     ]
    }
   ],
   "source": [
    "linkedin_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38160ef1-2118-4fe1-9dd7-91188b0432c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4> <strong>Important note:</strong></h4>\n",
    "We will need to use a string pattern search algorith so I will use <strong><code>Boyer Moore search</code></strong> to find the<br> strings more fast\n",
    "and here's an image to explain it:<br>\n",
    "<center><img src=\"https://www.researchgate.net/publication/337265181/figure/fig2/AS:825303362437121@1573779063161/Intuition-of-the-Boyer-Moore-search-procedure.png\" alt=\"Intuition of the Boyer-Moore search procedure.\" itemprop=\"contentUrl\" class=\"figure-details-image__main-image\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53fdc751-216d-4efc-abc9-f5fdade5409b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boyer_moore_search(pattern: str, text: str) -> dict:\n",
    "    \n",
    "    m              : int  = len(pattern)\n",
    "    n              : int  = len(text)\n",
    "    match          : list = []\n",
    "    bad_char_table : dict = {}\n",
    "    \n",
    "    for i in range(m - 1):\n",
    "        bad_char_table[pattern[i]] = m - i - 1\n",
    "        \n",
    "    i : int = m - 1\n",
    "    \n",
    "    while i < n:\n",
    "        \n",
    "        k : int = 0\n",
    "        while k < m and pattern[m - 1 - k] == text[i - k]:\n",
    "            k += 1\n",
    "        if k == m:\n",
    "            match.append(i - m + 1)\n",
    "            i += m\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            char_shift = bad_char_table.get(text[i], m)\n",
    "            i += max(1, char_shift)\n",
    "\n",
    "    return match\n",
    "\n",
    "def find_strings_in_string(string_list: list, target_string: str) -> list:\n",
    "\n",
    "    matches = set()\n",
    "\n",
    "    for s in string_list:\n",
    "        if len(s) <= len(target_string):\n",
    "            if boyer_moore_search(s, target_string):\n",
    "                matches.add(s)\n",
    "\n",
    "    return list(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9524cfe-bff9-495f-85ef-798805913ebd",
   "metadata": {},
   "source": [
    "The function below is used to clean the `likedin_df` from the HTML tags so it's easier for the regex to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc027bf8-0d74-46c0-a2e4-4634387b0595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \n",
    "    text_with_raw_html : str = BeautifulSoup(text, 'lxml').text\n",
    "    text               : str = BeautifulSoup(text_with_raw_html, 'lxml').text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94218d-225d-4b69-90d5-aa4d9555a63a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Extracting features from <span style= 'color: #4895e0'>Jobs descriptions</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806f16d6-d6b7-47d6-a86d-13e6a2d57e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_salary(description) -> float:\n",
    "    \n",
    "    salary_regex = r'\\$([0-9,.]+(?:K|k)?(?:\\.\\d+)?)(?:\\s*(?:to|-)\\s*\\$([0-9,.]+(?:K|k)?(?:\\.\\d+)?))?'\n",
    "    salary_matches = re.findall(salary_regex, description)\n",
    "\n",
    "    \n",
    "    if salary_matches:\n",
    "        salary_range = tuple(float(value.replace(',', '').replace('.', '').lower().replace('k', '000')) for value in salary_matches[0] if value != '' )\n",
    "        salary = sum(salary_range) / len(salary_range)\n",
    "    else:\n",
    "        salary = np.nan\n",
    "\n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8d0e9f-e04d-4cc1-97c5-e2854d5fd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(description) -> float:\n",
    "    \n",
    "    def convert_to_float(value):\n",
    "        value = value.replace(',', '').replace('.', '').replace(' ', '')\n",
    "        return float(value)\n",
    "    \n",
    "    years_regex   :str  = r'(\\d+(?:[,.]\\d+)*(?:\\.\\d+)?)\\s*(?:\\+|-)?\\s*(?:years?|yrs?)'\n",
    "    years_matches :list = re.findall(years_regex, description, re.IGNORECASE)\n",
    "\n",
    "    years_list :list = [convert_to_float(value) for value in years_matches if convert_to_float(value) < 15]\n",
    "    years      :int  =  np.mean(years_list)\n",
    "    \n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365dc572-1709-4cee-ae5f-5ca8bccc9eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_programming_lang(description) -> list:\n",
    "\n",
    "    languages  = ['Python', 'R lang', 'SQL', 'Kotlin', 'Java', \n",
    "                  'Java script', 'Type script', 'C#', 'C++',\n",
    "                  'Rust', 'Js', 'Scala', 'HTML', 'CSS']\n",
    "\n",
    "\n",
    "    languages  = [lang.lower() for lang in languages]\n",
    "    languages  = find_strings_in_string(string_list=   languages,\n",
    "                                        target_string= description.lower())\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37830406-090a-436c-8079-75656a996d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(description) -> list:\n",
    "\n",
    "    data_analysis_skills = [\n",
    "        'Pandas', 'Excel',\n",
    "        'NumPy', 'Matplotlib',\n",
    "        'Seaborn', 'Tableau',\n",
    "        'Power BI', 'Data querying',\n",
    "        'Data mining', 'Data interpretation',\n",
    "        'Data modeling', 'Data reporting',\n",
    "        'Business intelligence', 'Data analytics',\n",
    "        'Data validation', 'Data profiling',\n",
    "        'Data aggregation', 'Data imputation',\n",
    "        'Data manipulation', 'Data auditing',\n",
    "        'Data quality management', 'SAS',\n",
    "        'Data cleaning', 'Data visualization',\n",
    "        'Statistical analysis', 'SPSS']\n",
    "\n",
    "    data_science_skills = [\n",
    "        'Machine learning', 'Deep learning', 'NLP',\n",
    "        'Natural language processing', 'Computer vision',\n",
    "        'Big data', 'Data wrangling',\n",
    "        'Feature engineering', 'Predictive modeling',\n",
    "        'Time series analysis', 'TensorFlow',\n",
    "        'Keras', 'PyTorch',\n",
    "        'Scikit-learn', 'Hadoop',\n",
    "        'Spark', 'Data storytelling',\n",
    "        'A/B testing', 'Data mining',\n",
    "        'Data cleaning', 'Data visualization',\n",
    "        'Data manipulation', 'Data pre-processing',\n",
    "        'Data analysis', 'Data presentation',\n",
    "        'Data-driven decision making', 'Model evaluation',\n",
    "        'Model deployment']\n",
    "\n",
    "    data_engineering_skills = [\n",
    "        'Data warehousing', 'Data pipelines',\n",
    "        'Database management', 'Data architecture',\n",
    "        'Data integration', 'Apache Kafka',\n",
    "        'Apache Airflow', 'Amazon Web Services',\n",
    "        'Google Cloud Platform', 'Microsoft Azure',\n",
    "        'Docker', 'Kubernetes',\n",
    "        'Data security', 'Data governance',\n",
    "        'Data scalability', 'Data storage',\n",
    "        'Data migration', 'Data transformation',\n",
    "        'Data orchestration', 'Data monitoring',\n",
    "        'Data lake', 'AWS', 'GCP', 'ETL']\n",
    "\n",
    "    data_entry_skills = [\n",
    "        'Typing speed', 'Data accuracy',\n",
    "        'Data entry software', 'Excel',\n",
    "        'Google Sheets', 'Data verification',\n",
    "        'Data organization', 'Attention to detail',\n",
    "        'Time management', 'Data maintenance',\n",
    "        'Database management', 'Copy typing',\n",
    "        'Data review', 'Data formatting',\n",
    "        'Data categorization', 'Data entry validation',\n",
    "        'Data cleansing', 'Data input',\n",
    "        'Data indexing', 'Data extraction',\n",
    "        'Data capture', 'Data filing',\n",
    "        'Data archiving']\n",
    "\n",
    "    all_skills : list = (\n",
    "        data_analysis_skills + data_science_skills +\n",
    "        data_engineering_skills + data_entry_skills)\n",
    "    \n",
    "    all_skills = [skill.lower() for skill in all_skills]\n",
    "    skills     = find_strings_in_string(string_list=   all_skills,\n",
    "                                        target_string= description.lower())\n",
    "    \n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377b70b4-1df6-45de-83e1-9b7e415f54b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linkedin_jobs['describtion']      : pd.Series = linkedin_jobs['describtion'].apply(remove_html_tags)\n",
    "\n",
    "linkedin_jobs['salary']           : pd.Series = linkedin_jobs['describtion'].apply(extract_salary)\n",
    "linkedin_jobs['skills']           : pd.Series = linkedin_jobs['describtion'].apply(extract_skills)\n",
    "linkedin_jobs['programming_lang'] : pd.Series = linkedin_jobs['describtion'].apply(extract_programming_lang)\n",
    "linkedin_jobs['exp_years']        : pd.Series = linkedin_jobs['describtion'].apply(extract_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afe505-faa3-4256-a1e4-54ed3453e67c",
   "metadata": {},
   "source": [
    "## <center><strong>Data <span style= 'color: #517afc'>Cleaning</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d845a-0102-4681-ab62-8c683a94e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tucky_method(array: np.array, indecies= True) -> np.array:\n",
    "    '''\n",
    "    This function works with any list-like numerical object\n",
    "    (don't work with pandas series's) and returns the indexes\n",
    "    of the found outliers in the array.\n",
    "    \n",
    "    :Params: Takes only the series.\n",
    "    :Returns: S list of the outliers indexes.\n",
    "    '''\n",
    "    \n",
    "    Q3 = np.quantile(array, 0.75)\n",
    "    Q1 = np.quantile(array, 0.25)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    upper_range = Q3 + (IQR * 1.5)\n",
    "    lower_range = Q1 - (IQR * 1.5)\n",
    "    \n",
    "    outliers = [x for x in array if ((x < lower_range) | (x > upper_range))]\n",
    "    print(f'Found {len(outliers)} outliers from {len(array)} length series!')\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133baebf-9dd4-4399-b1ae-f88289935675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the salary data\n",
    "linkedin_jobs['salary'] = linkedin_jobs['salary'].where((linkedin_jobs.salary < 70) |\n",
    "                                                        (linkedin_jobs.salary > 10_000) &\n",
    "                                                        (linkedin_jobs.salary > 5))\n",
    "\n",
    "per_hour_or_year_map = lambda salary: 'per year' if salary > 10_000 else 'per hour' if salary < 100 else np.nan\n",
    "linkedin_jobs['per_hour_or_year'] = linkedin_jobs['salary'].apply(per_hour_or_year_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcbaab-9b2e-4b9d-9996-e271faf520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless columns\n",
    "upwork_profiles.drop(['Unnamed__0', 'new_columns'], axis= 1, inplace= True)\n",
    "guru_profiles.drop(['Unnamed__0', 'new_columns'], axis= 1, inplace= True)\n",
    "linkedin_jobs.drop(['jobs_links'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78684c-cb07-4d6b-90a6-cc84bfd65dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping outliers\n",
    "outliers = tucky_method(linkedin_jobs[linkedin_jobs['per_hour_or_year'] == 'per year']['salary'].to_numpy())\n",
    "outliers_indexes = np.array(*np.where(np.isin(linkedin_jobs['salary'], outliers)))\n",
    "linkedin_jobs = linkedin_jobs.drop(outliers_indexes)\n",
    "\n",
    "guru_profiles['country'] = guru_profiles['addresse'].str.split(',').str[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df050063-7108-4339-9238-3bdf4adbbd45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <center><strong>Saving the <span style= 'color: #4895e0'>Data</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0052f7cc-1479-4138-857e-a9b0fef1864f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Firstly i'll apply `extract_salary` function on guru minimum pay to get it as integer\n",
    "guru_profiles['minimum_pay'] = guru_profiles['minimum_pay'].apply(extract_salary)\n",
    "\n",
    "# And also i will exxtract some data from upwork\n",
    "upwork_profiles['exp_years'] = upwork_profiles['describtion'].apply(extract_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19681266-7cc1-41cf-989a-306a30b73c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'linkedin_jobs' uploaded as table 'data-jobs-analysis-db.data_jobs_analysis_db.linkedin_jobs' in BigQuery.\n",
      "DataFrame 'guru_profiles' uploaded as table 'data-jobs-analysis-db.data_jobs_analysis_db.guru_profiles' in BigQuery.\n",
      "DataFrame 'upwork_profiles' uploaded as table 'data-jobs-analysis-db.data_jobs_analysis_db.upwork_profiles' in BigQuery.\n"
     ]
    }
   ],
   "source": [
    "dataframes = {\n",
    "    'linkedin_jobs'   : linkedin_jobs,\n",
    "    'guru_profiles'   : guru_profiles,\n",
    "    'upwork_profiles' : upwork_profiles}\n",
    "\n",
    "for table_name, df in dataframes.items():\n",
    "\n",
    "    table_id   = f'{project_id}.{dataset_id}.{table_name}'\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition= 'WRITE_TRUNCATE')\n",
    "    job        = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "\n",
    "    job.result()\n",
    "    print(f'DataFrame \\'{table_name}\\' uploaded as table \\'{table_id}\\' in BigQuery.')\n",
    "\n",
    "encrypt.encrypt_json_file('../credentials.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
