{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b1cb43-3be1-4f87-8c26-581f591afdce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922ce52-f8a8-47c3-b1fa-a8fdc1eaa87e",
   "metadata": {},
   "source": [
    "Here we will try to extract more data from the describtions of each job using simple NER, POS & Regex expressions<br>\n",
    "We will also need to clean the data after we extract it and make sure that there isn't any NANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1713b-7169-4322-bc7b-f8c5acc5dfcc",
   "metadata": {},
   "source": [
    "\n",
    "**Things we will extract from the dexcribtions:**\n",
    "- Salaries data\n",
    "- Mentioned skills\n",
    "- Requeird years of experience\n",
    "- Requeird programming languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e990-2723-45e1-92cf-3a59f97a2af8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Importing the <span style= 'color: #48e0dc'>Packeges</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce9d4d17-4dbd-4c66-b031-8a00b62a4b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib\n",
    "\n",
    "import pandas             as pd\n",
    "import numpy              as np\n",
    "import seaborn            as sns\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "from io                   import StringIO \n",
    "from bs4                  import BeautifulSoup\n",
    "from google.cloud         import bigquery\n",
    "from wordcloud            import WordCloud\n",
    "from IPython.display      import set_matplotlib_formats\n",
    "from collections          import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a93668f-55d4-4a1f-80a7-bb55ab0ba449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'Candara'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "credentials_path :str = '../credentials.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "\n",
    "FONT             :int  = 17\n",
    "COLORS           :list = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS         :list = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                          'uint16', 'uint32', 'uint64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e6b0e-1e94-4f3a-a984-86ecc216d425",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Preparing the data <span style= 'color: #5cd3f7'>Extracting</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ff15e2b-3516-4616-b543-0476c0e90879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5210b397-3315-4f5f-97d7-a49f3040d50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id :str = 'data-jobs-analysis-db'\n",
    "dataset_id :str = 'data_jobs_analysis_db'\n",
    "\n",
    "client   = bigquery.Client(project= project_id)\n",
    "\n",
    "linkedin_jobs    = client.query(f'SELECT * FROM {dataset_id}.linkedin_jobs;').to_dataframe()\n",
    "upwork_profiles  = client.query(f'SELECT * FROM {dataset_id}.upwork_profiles;').to_dataframe()\n",
    "guru_profiles    = client.query(f'SELECT * FROM {dataset_id}.guru_profiles;').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdd6d5-784b-47b9-88e0-132461b49cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linkedin_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38160ef1-2118-4fe1-9dd7-91188b0432c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4> <strong>Important note:</strong></h4>\n",
    "We will need to use a string pattern search algorith so I will use <strong><code>Boyer Moore search</code></strong> to find the<br> strings more fast\n",
    "and here's an image to explain it:<br>\n",
    "<center><img src=\"https://www.researchgate.net/publication/337265181/figure/fig2/AS:825303362437121@1573779063161/Intuition-of-the-Boyer-Moore-search-procedure.png\" alt=\"Intuition of the Boyer-Moore search procedure.\" itemprop=\"contentUrl\" class=\"figure-details-image__main-image\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53fdc751-216d-4efc-abc9-f5fdade5409b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boyer_moore_search(pattern: str, text: str) -> dict:\n",
    "    \n",
    "    m              : int  = len(pattern)\n",
    "    n              : int  = len(text)\n",
    "    match          : list = []\n",
    "    bad_char_table : dict = {}\n",
    "    \n",
    "    for i in range(m - 1):\n",
    "        bad_char_table[pattern[i]] = m - i - 1\n",
    "        \n",
    "    i : int = m - 1\n",
    "    \n",
    "    while i < n:\n",
    "        \n",
    "        k : int = 0\n",
    "        while k < m and pattern[m - 1 - k] == text[i - k]:\n",
    "            k += 1\n",
    "        if k == m:\n",
    "            match.append(i - m + 1)\n",
    "            i += m\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            char_shift = bad_char_table.get(text[i], m)\n",
    "            i += max(1, char_shift)\n",
    "\n",
    "    return match\n",
    "\n",
    "def find_strings_in_string(string_list: list, target_string: str) -> list:\n",
    "\n",
    "    matches = set()\n",
    "\n",
    "    for s in string_list:\n",
    "        if len(s) <= len(target_string):\n",
    "            if boyer_moore_search(s, target_string):\n",
    "                matches.add(s)\n",
    "\n",
    "    return list(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9524cfe-bff9-495f-85ef-798805913ebd",
   "metadata": {},
   "source": [
    "The function below is used to clean the `likedin_df` from the HTML tags so it's easier for the regex to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc027bf8-0d74-46c0-a2e4-4634387b0595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \n",
    "    text_with_raw_html : str = BeautifulSoup(text, 'lxml').text\n",
    "    text               : str = BeautifulSoup(text_with_raw_html, 'lxml').text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94218d-225d-4b69-90d5-aa4d9555a63a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Extracting features from <span style= 'color: #4895e0'>Jobs descriptions</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "806f16d6-d6b7-47d6-a86d-13e6a2d57e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_salary(description) -> float:\n",
    "    \n",
    "    salary_regex = r'\\$([0-9,.]+(?:K|k)?(?:\\.\\d+)?)(?:\\s*(?:to|-)\\s*\\$([0-9,.]+(?:K|k)?(?:\\.\\d+)?))?'\n",
    "    salary_matches = re.findall(salary_regex, description)\n",
    "\n",
    "    \n",
    "    if salary_matches:\n",
    "        salary_range = tuple(float(value.replace(',', '').replace('.', '').lower().replace('k', '000')) for value in salary_matches[0] if value != '' )\n",
    "        salary = sum(salary_range) / len(salary_range)\n",
    "    else:\n",
    "        salary = np.nan\n",
    "\n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa8d0e9f-e04d-4cc1-97c5-e2854d5fd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(description) -> float:\n",
    "    \n",
    "    def convert_to_float(value):\n",
    "        value = value.replace(',', '').replace('.', '').replace(' ', '')\n",
    "        return float(value)\n",
    "    \n",
    "    years_regex   :str  = r'(\\d+(?:[,.]\\d+)*(?:\\.\\d+)?)\\s*(?:\\+|-)?\\s*(?:years?|yrs?)'\n",
    "    years_matches :list = re.findall(years_regex, description, re.IGNORECASE)\n",
    "\n",
    "    years_list :list = [convert_to_float(value) for value in years_matches if convert_to_float(value) < 15]\n",
    "    years      :int  =  np.mean(years_list)\n",
    "    \n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "365dc572-1709-4cee-ae5f-5ca8bccc9eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_programming_lang(description) -> list:\n",
    "\n",
    "    languages  = ['Python', 'R lang', 'SQL', 'Kotlin', 'Java', \n",
    "                  'Java script', 'Type script', 'C#', 'C++',\n",
    "                  'Rust', 'Js', 'Scala', 'HTML', 'CSS']\n",
    "\n",
    "\n",
    "    languages  = [lang.lower() for lang in languages]\n",
    "    languages  = find_strings_in_string(string_list=   languages,\n",
    "                                        target_string= description.lower())\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37830406-090a-436c-8079-75656a996d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(description) -> list:\n",
    "\n",
    "    data_analysis_skills = [\n",
    "        'Pandas', 'Excel',\n",
    "        'NumPy', 'Matplotlib',\n",
    "        'Seaborn', 'Tableau',\n",
    "        'Power BI', 'Data querying',\n",
    "        'Data mining', 'Data interpretation',\n",
    "        'Data modeling', 'Data reporting',\n",
    "        'Business intelligence', 'Data analytics',\n",
    "        'Data validation', 'Data profiling',\n",
    "        'Data aggregation', 'Data imputation',\n",
    "        'Data manipulation', 'Data auditing',\n",
    "        'Data quality management', 'SAS',\n",
    "        'Data cleaning', 'Data visualization',\n",
    "        'Statistical analysis', 'SPSS']\n",
    "\n",
    "    data_science_skills = [\n",
    "        'Machine learning', 'Deep learning', 'NLP',\n",
    "        'Natural language processing', 'Computer vision',\n",
    "        'Big data', 'Data wrangling',\n",
    "        'Feature engineering', 'Predictive modeling',\n",
    "        'Time series analysis', 'TensorFlow',\n",
    "        'Keras', 'PyTorch',\n",
    "        'Scikit-learn', 'Hadoop',\n",
    "        'Spark', 'Data storytelling',\n",
    "        'A/B testing', 'Data mining',\n",
    "        'Data cleaning', 'Data visualization',\n",
    "        'Data manipulation', 'Data pre-processing',\n",
    "        'Data analysis', 'Data presentation',\n",
    "        'Data-driven decision making', 'Model evaluation',\n",
    "        'Model deployment']\n",
    "\n",
    "    data_engineering_skills = [\n",
    "        'Data warehousing', 'Data pipelines',\n",
    "        'Database management', 'Data architecture',\n",
    "        'Data integration', 'Apache Kafka',\n",
    "        'Apache Airflow', 'Amazon Web Services',\n",
    "        'Google Cloud Platform', 'Microsoft Azure',\n",
    "        'Docker', 'Kubernetes',\n",
    "        'Data security', 'Data governance',\n",
    "        'Data scalability', 'Data storage',\n",
    "        'Data migration', 'Data transformation',\n",
    "        'Data orchestration', 'Data monitoring',\n",
    "        'Data lake', 'AWS', 'GCP', 'ETL']\n",
    "\n",
    "    data_entry_skills = [\n",
    "        'Typing speed', 'Data accuracy',\n",
    "        'Data entry software', 'Excel',\n",
    "        'Google Sheets', 'Data verification',\n",
    "        'Data organization', 'Attention to detail',\n",
    "        'Time management', 'Data maintenance',\n",
    "        'Database management', 'Copy typing',\n",
    "        'Data review', 'Data formatting',\n",
    "        'Data categorization', 'Data entry validation',\n",
    "        'Data cleansing', 'Data input',\n",
    "        'Data indexing', 'Data extraction',\n",
    "        'Data capture', 'Data filing',\n",
    "        'Data archiving']\n",
    "\n",
    "    all_skills : list = (\n",
    "        data_analysis_skills + data_science_skills +\n",
    "        data_engineering_skills + data_entry_skills)\n",
    "    \n",
    "    all_skills = [skill.lower() for skill in all_skills]\n",
    "    skills     = find_strings_in_string(string_list=   all_skills,\n",
    "                                        target_string= description.lower())\n",
    "    \n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "377b70b4-1df6-45de-83e1-9b7e415f54b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linkedin_jobs['describtion']      : pd.Series = linkedin_jobs['describtion'].apply(remove_html_tags)\n",
    "\n",
    "linkedin_jobs['salary']           : pd.Series = linkedin_jobs['describtion'].apply(extract_salary)\n",
    "linkedin_jobs['skills']           : pd.Series = linkedin_jobs['describtion'].apply(extract_skills)\n",
    "linkedin_jobs['programming_lang'] : pd.Series = linkedin_jobs['describtion'].apply(extract_programming_lang)\n",
    "linkedin_jobs['exp_years']        : pd.Series = linkedin_jobs['describtion'].apply(extract_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df050063-7108-4339-9238-3bdf4adbbd45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Saving the <span style= 'color: #517afc'>Data</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0052f7cc-1479-4138-857e-a9b0fef1864f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Firstly i'll apply `extract_salary` function on guru minimum pay to get it as integer\n",
    "guru_profiles['minimum_pay'] = guru_profiles['minimum_pay'].apply(extract_salary)\n",
    "\n",
    "# And also i will exxtract some data from upwork\n",
    "upwork_profiles['exp_years'] = upwork_profiles['describtion'].apply(extract_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19681266-7cc1-41cf-989a-306a30b73c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'linkedin_jobs' uploaded as table 'data-jobs-analysis-db.data_jobs_analysis_db.linkedin_jobs' in BigQuery.\n",
      "DataFrame 'guru_profiles' uploaded as table 'data-jobs-analysis-db.data_jobs_analysis_db.guru_profiles' in BigQuery.\n",
      "DataFrame 'upwork_profiles' uploaded as table 'data-jobs-analysis-db.data_jobs_analysis_db.upwork_profiles' in BigQuery.\n"
     ]
    }
   ],
   "source": [
    "dataframes = {\n",
    "    'linkedin_jobs'   : linkedin_jobs,\n",
    "    'guru_profiles'   : guru_profiles,\n",
    "    'upwork_profiles' : upwork_profiles}\n",
    "\n",
    "for table_name, df in dataframes.items():\n",
    "\n",
    "    table_id   = f'{project_id}.{dataset_id}.{table_name}'\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition= 'WRITE_TRUNCATE')\n",
    "    job        = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "\n",
    "    job.result()\n",
    "    print(f'DataFrame \\'{table_name}\\' uploaded as table \\'{table_id}\\' in BigQuery.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
