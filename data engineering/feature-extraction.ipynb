{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b1cb43-3be1-4f87-8c26-581f591afdce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922ce52-f8a8-47c3-b1fa-a8fdc1eaa87e",
   "metadata": {},
   "source": [
    "Here we will try to extract more data from the describtions of each job using simple NER, POS & Regex expressions<br>\n",
    "We will also need to clean the data after we extract it and make sure that there isn't any NANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1713b-7169-4322-bc7b-f8c5acc5dfcc",
   "metadata": {},
   "source": [
    "**Things we will extract from the dexcribtions:**\n",
    "- Salaries data\n",
    "- Mentioned skills\n",
    "- Requeird years of experience\n",
    "- Requeird programming languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e990-2723-45e1-92cf-3a59f97a2af8",
   "metadata": {},
   "source": [
    "## <center><strong>Importing the <span style= 'color: #48e0dc'>Packeges</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce9d4d17-4dbd-4c66-b031-8a00b62a4b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib\n",
    "\n",
    "import pandas             as pd\n",
    "import numpy              as np\n",
    "import seaborn            as sns\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "from io                   import StringIO \n",
    "from bs4                  import BeautifulSoup\n",
    "from google.cloud         import bigquery\n",
    "from wordcloud            import WordCloud\n",
    "from IPython.display      import set_matplotlib_formats\n",
    "from collections          import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a93668f-55d4-4a1f-80a7-bb55ab0ba449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'Candara'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "credentials_path :str = '../credentials.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "\n",
    "FONT             :int  = 17\n",
    "COLORS           :list = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS         :list = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                          'uint16', 'uint32', 'uint64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e6b0e-1e94-4f3a-a984-86ecc216d425",
   "metadata": {},
   "source": [
    "## <center><strong>Extracting <span style= 'color: #48e0dc'>LinkedIn</span> data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6ff15e2b-3516-4616-b543-0476c0e90879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5210b397-3315-4f5f-97d7-a49f3040d50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id :str = 'data-jobs-analysis-db'\n",
    "dataset_id :str = 'data_jobs_analysis_db'\n",
    "\n",
    "client   = bigquery.Client(project= project_id)\n",
    "\n",
    "linkedin_jobs    = client.query(f'SELECT * FROM {dataset_id}.linkedin_jobs;').to_dataframe()\n",
    "upwork_profiles  = client.query(f'SELECT * FROM {dataset_id}.upwork_profiles;').to_dataframe()\n",
    "guru_profiles    = client.query(f'SELECT * FROM {dataset_id}.guru_profiles;').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87fdd6d5-784b-47b9-88e0-132461b49cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3455 entries, 0 to 3454\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   jobs_locations      3455 non-null   object\n",
      " 1   listing_dates       3349 non-null   object\n",
      " 2   jobs_titles         3455 non-null   object\n",
      " 3   companies_names     3455 non-null   object\n",
      " 4   jobs_links          3455 non-null   object\n",
      " 5   describtion         3455 non-null   object\n",
      " 6   location_type       3455 non-null   object\n",
      " 7   employment_type     3455 non-null   object\n",
      " 8   industry            3414 non-null   object\n",
      " 9   reqierd_credential  3455 non-null   object\n",
      " 10  country             3455 non-null   object\n",
      " 11  job_title           3455 non-null   object\n",
      " 12  total_jobs          3455 non-null   Int64 \n",
      "dtypes: Int64(1), object(12)\n",
      "memory usage: 354.4+ KB\n"
     ]
    }
   ],
   "source": [
    "linkedin_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38160ef1-2118-4fe1-9dd7-91188b0432c1",
   "metadata": {},
   "source": [
    "<h4> <strong>Important note:</strong></h4>\n",
    "We will need to use a string pattern search algorith so I will use <strong><code>Boyer Moore search</code></strong> to find the<br> strings more fast\n",
    "and here's an image to explain it:<br>\n",
    "<center><img src=\"https://www.researchgate.net/publication/337265181/figure/fig2/AS:825303362437121@1573779063161/Intuition-of-the-Boyer-Moore-search-procedure.png\" alt=\"Intuition of the Boyer-Moore search procedure.\" itemprop=\"contentUrl\" class=\"figure-details-image__main-image\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "53fdc751-216d-4efc-abc9-f5fdade5409b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boyer_moore_search(pattern: str, text: str) -> dict:\n",
    "    \n",
    "    m              : int  = len(pattern)\n",
    "    n              : int  = len(text)\n",
    "    matches        : list = []\n",
    "    bad_char_table : dict = {}\n",
    "    \n",
    "    for i in range(m - 1):\n",
    "        bad_char_table[pattern[i]] = m - i - 1\n",
    "        \n",
    "    i : int = m - 1\n",
    "    \n",
    "    while i < n:\n",
    "        \n",
    "        k : int = 0\n",
    "        while k < m and pattern[m - 1 - k] == text[i - k]:\n",
    "            k += 1\n",
    "        if k == m:\n",
    "            matches.append(i - m + 1)\n",
    "            i += m\n",
    "        else:\n",
    "            char_shift = bad_char_table.get(text[i], m)\n",
    "            i += max(1, char_shift)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def find_strings_in_string(string_list: list, target_string: str) -> list:\n",
    "    matches = set()\n",
    "    \n",
    "    for s in string_list:\n",
    "        if len(s) <= len(target_string):\n",
    "            if boyer_moore_search(s, target_string):\n",
    "                matches.add(s)\n",
    "\n",
    "    return list(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc027bf8-0d74-46c0-a2e4-4634387b0595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \n",
    "    text_with_raw_html : str = BeautifulSoup(text, 'lxml').text\n",
    "    text               : str = BeautifulSoup(text_with_raw_html, 'lxml').text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "806f16d6-d6b7-47d6-a86d-13e6a2d57e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_salary(description) -> float:\n",
    "    \n",
    "    salary_regex = r'\\$([0-9,.]+(?:K|k)?(?:\\.\\d+)?)(?:\\s*(?:to|-)\\s*\\$([0-9,.]+(?:K|k)?(?:\\.\\d+)?))?'\n",
    "    salary_matches = re.findall(salary_regex, description)\n",
    "\n",
    "    \n",
    "    if salary_matches:\n",
    "        salary_range = tuple(float(value.replace(',', '').replace('.', '').lower().replace('k', '000')) for value in salary_matches[0] if value != '' )\n",
    "        salary = sum(salary_range) / len(salary_range)\n",
    "    else:\n",
    "        salary = np.nan\n",
    "\n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d0e9f-e04d-4cc1-97c5-e2854d5fd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requird_exp_years(description) -> float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37830406-090a-436c-8079-75656a996d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requird_skills(description) -> list:\n",
    "\n",
    "    data_analysis_skills = [\n",
    "        'Pandas', 'Excel',\n",
    "        'NumPy', 'Matplotlib',\n",
    "        'Seaborn', 'Tableau',\n",
    "        'Power BI', 'Data querying',\n",
    "        'Data mining', 'Data interpretation',\n",
    "        'Data modeling', 'Data reporting',\n",
    "        'Business intelligence', 'Data analytics',\n",
    "        'Data validation', 'Data profiling',\n",
    "        'Data aggregation', 'Data imputation',\n",
    "        'Data manipulation', 'Data auditing',\n",
    "        'Data quality management',\n",
    "        'Data cleaning', 'Data visualization',\n",
    "        'Statistical analysis', 'SPSS']\n",
    "\n",
    "    data_science_skills = [\n",
    "        'Machine learning', 'Deep learning', 'NLP',\n",
    "        'Natural language processing', 'Computer vision',\n",
    "        'Big data', 'Data wrangling',\n",
    "        'Feature engineering', 'Predictive modeling',\n",
    "        'Time series analysis', 'TensorFlow',\n",
    "        'Keras', 'PyTorch',\n",
    "        'Scikit-learn', 'Hadoop',\n",
    "        'Spark', 'Data storytelling',\n",
    "        'A/B testing', 'Data mining',\n",
    "        'Data cleaning', 'Data visualization',\n",
    "        'Data manipulation', 'Data pre-processing',\n",
    "        'Data analysis', 'Data presentation',\n",
    "        'Data-driven decision making', 'Model evaluation',\n",
    "        'Model deployment']\n",
    "\n",
    "    data_engineering_skills = [\n",
    "        'ETL (Extract, Transform, Load)',\n",
    "        'Data warehousing', 'Data pipelines',\n",
    "        'Database management', 'Data architecture',\n",
    "        'Data integration', 'Apache Kafka',\n",
    "        'Apache Airflow', 'Amazon Web Services',\n",
    "        'Google Cloud Platform', 'Microsoft Azure',\n",
    "        'Docker', 'Kubernetes',\n",
    "        'Data security', 'Data governance',\n",
    "        'Data scalability', 'Data storage',\n",
    "        'Data migration', 'Data transformation',\n",
    "        'Data orchestration', 'Data monitoring',\n",
    "        'Data lake', 'AWS', 'GCP']\n",
    "\n",
    "    data_entry_skills = [\n",
    "        'Typing speed', 'Data accuracy',\n",
    "        'Data entry software', 'Excel',\n",
    "        'Google Sheets', 'Data verification',\n",
    "        'Data organization', 'Attention to detail',\n",
    "        'Time management', 'Data maintenance',\n",
    "        'Database management', 'Copy typing',\n",
    "        'Data review', 'Data formatting',\n",
    "        'Data categorization', 'Data entry validation',\n",
    "        'Data cleansing', 'Data input',\n",
    "        'Data indexing', 'Data extraction',\n",
    "        'Data capture', 'Data filing',\n",
    "        'Data archiving']\n",
    "\n",
    "    all_skills = (\n",
    "        data_analysis_skills + data_science_skills +\n",
    "        data_engineering_skills + data_entry_skills)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "377b70b4-1df6-45de-83e1-9b7e415f54b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linkedin_jobs['describtion'] : pd.Series = linkedin_jobs['describtion'].apply(remove_html_tags)\n",
    "linkedin_jobs['salary']      : pd.Series = linkedin_jobs['describtion'].apply(extract_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "59b9fc00-21ba-41f1-8a65-f5ca80f267aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We are seeking a Data Engineer to join the Arc family. As a senior member of the technical team, you'll work alongside the CTO to design, architect and implement data infrastructure and pipelines to power our automated underwriting process.You'll solve hard problems and have a deep impact on the end-product. Expect to work in a fast-paced environment, with amazing engineers who believe in our mission.Requirements4 years of professional experienceExperience with working on a modern data stack with tools such as AWS S3, Apache Spark, Jupyter, Databricks, SnowflakeCollaborative and enjoys working in a teamPassion for engineering and learning\""
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin_jobs['describtion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "76285237-1e22-4ba7-9422-ca7a628ab617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_years(description):\n",
    "    \n",
    "    def convert_to_float(value):\n",
    "        value = value.replace(',', '').replace('.', '').replace(' ', '')\n",
    "        return float(value)\n",
    "    \n",
    "    years_regex = r'(\\d+(?:[,.]?\\d+)*(?:\\.\\d+)?)\\s*\\+?\\s*(?:years?|yrs?)'\n",
    "    years_matches = re.findall(years_regex, description, re.IGNORECASE)\n",
    "\n",
    "    years_list = [convert_to_float(value) for value in years_matches]\n",
    "    return years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "c11d1f33-4db8-4738-83fd-cda283b84719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_years(linkedin_jobs['describtion'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "1a040eae-fe25-4e57-a806-17173b74a5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'years of hands-'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin_jobs['describtion'][10][1885:1885 +15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba73e7-f0a9-440c-9633-c6aef42749a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
