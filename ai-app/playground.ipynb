{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b700e760-7e79-46b5-b2e7-870d73dfe1cf",
   "metadata": {},
   "source": [
    "## **Setting up the Llama model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81987450-8d24-4d5d-b935-a8deebf3dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms    import CTransformers\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import transformers\n",
    "import torch\n",
    "import langchain\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82e64336-689e-48a1-9ceb-97f78a10bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278810a87bc74f4496b4755969e00b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4ba48024644d89ad03ed0887a46cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72cb5fb422147119a49ef9be7341aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model)\n",
    "pipeline=transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=1000,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d015c94d-0842-46ba-addd-d0fe1bcf3cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸ˜Ž Hey there, I'm Jobsir! ðŸ¤¯ I'm here to spit some fire about chatbots, and I've got the rhymes to prove it! ðŸ’¥\n",
      "\n",
      "Verse 1:\n",
      "Chatbots are the future, don't you see?\n",
      "They're gonna change the game, just wait and see\n",
      "From customer service to data analysis too\n",
      "These bots are on the rise, they're coming through!\n",
      "\n",
      "Chorus:\n",
      "Rap about chatbots, in cool words all day\n",
      "Making it easy for you, in a major way\n",
      "From AI to NLP, we've got the skills to pay\n",
      "We're the future of tech, and we're here to stay! ðŸ’ª\n",
      "\n",
      "Verse 2:\n",
      "Bot-building is my jam, I'm feeling like a pro\n",
      "With natural language processing, I'm in the flow\n",
      "I can answer questions, or create some too\n",
      "Chatbots are the future, and I'm here to prove it!\n",
      "\n",
      "Chorus:\n",
      "Rap about chatbots, in cool words all day\n",
      "Making it easy for you, in a major way\n",
      "From AI to NLP, we've got the skills to pay\n",
      "We're the future of tech, and we're here to stay! ðŸ’ª\n",
      "\n",
      "Outro:\n",
      "So don't be left behind, get on board the chatbot train\n",
      "It's time to join the revolution, and make some gains! ðŸš‚\n",
      "With Jobsir leading the way, you know we're in good hands\n",
      "Chatbots are the future, and they're here to make a stand! \n",
      "1030.433244228363\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(llama.run(chat_template.format_messages(input=\"Can you create a rap song about Chatbots in cool words?\")))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c8c7c-e23a-4d20-b0e3-53ff6ad7ba78",
   "metadata": {},
   "source": [
    "## **Testing & Customizing the ChatBot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400fbd8-520c-414a-8196-87e5fa915f3a",
   "metadata": {},
   "source": [
    "Testing the model befor training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114471d-8a61-4983-b426-4fa74f967b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'Hi! I like cooking. Can you suggest some recipes?\\n')\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68923a-c03c-45ce-8136-660f86b71add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0583c74f9b4f4d8eb087fc88202488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86aac05e6b94452b86b249cf5785bb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ba55bcef68488cb45c3cc584a7f6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863e0341514a4f1a812cc5c89e62194a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-7b-chat.ggmlv3.q2_K.bin:   0%|          | 0.00/2.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c0bb1-4da4-49de-830d-e94f41d4c2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
