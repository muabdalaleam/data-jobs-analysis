{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d87eb4b-c943-4562-866b-7c2dd151f3c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <center><strong>Notebook<span style= 'color: #51FCC6'> Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753d54c-1fd0-441c-b663-98ca78298128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59118b7-50cf-4738-8e35-cd9de6b6e46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b08419ad-46bc-42a6-a678-356c38c435fe",
   "metadata": {},
   "source": [
    "## <center><strong>Importing<span style= 'color: #48E0DC'> Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfca499a-0b6e-42a6-9901-a25b403a1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "import inspect\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import cloudscraper\n",
    "import numpy                         as np\n",
    "import pandas                        as pd\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "\n",
    "from itertools                       import count\n",
    "from bs4                             import BeautifulSoup\n",
    "from selenium                        import webdriver\n",
    "from selenium.webdriver.common.by    import By\n",
    "from selenium.webdriver.common.keys  import Keys\n",
    "from fake_useragent                  import UserAgent\n",
    "from IPython.display                 import set_matplotlib_formats\n",
    "from selenium.webdriver.support.ui   import WebDriverWait\n",
    "from selenium.webdriver.support      import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b8e77-3b20-4c92-8619-434de6cedf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "MAX_PAGES        :int   = 10\n",
    "COLORS           :list  = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS         :list  = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                           'uint16', 'uint32', 'uint64']\n",
    "\n",
    "data_jobs_titles :list  = ['Data entry', 'Data Engineering',\n",
    "                           'Data scientist', 'Data Analyst',\n",
    "                           'Machine Learning']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3e9fc-3746-4f76-be83-00f2b453be34",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5CD3F7'> Web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd35b8e-8c12-4e99-8819-4c6e338c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url: str) -> BeautifulSoup:\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(delay= 10,browser= {\n",
    "        'browser': 'chrome',\n",
    "        'platform': 'windows',\n",
    "        'desktop': True,\n",
    "        'mobile': False}) \n",
    "    \n",
    "    content = scraper.get(url).text\n",
    "    soup    = BeautifulSoup(content)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3908558f-1fe9-4c88-af63-731658ee2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upwork scraper\n",
    "def upwork_scraper(job_title : str,\n",
    "                   page      : int) -> list:\n",
    "\n",
    "    url         : str  = f'https://www.upwork.com/search/profiles/?page={page}&q={job_title}'\n",
    "    soup        : str  = scrape_page(url)\n",
    "    loaded_jobs : list = soup.find_all('div', class_= 'up-card-section up-card-hover')\n",
    "\n",
    "    return  loaded_jobs\n",
    "\n",
    "\n",
    "# Guru scraper\n",
    "def guru_scraper(job_title : str,\n",
    "                 page      : int) -> list:\n",
    "\n",
    "    url         : str  = f'https://www.guru.com/d/freelancers/skill/{job_title}/pg/{page}/'\n",
    "    soup        : str  = scrape_page(url)\n",
    "    loaded_jobs : list = soup.find_all('div', class_= 'record record--avatarCheck findGuruRecord')\n",
    "    loaded_page : int  = int(soup.find('li', class_= 'active').find('a').text)\n",
    "    \n",
    "    if loaded_page != page:\n",
    "        loaded_jobs = []\n",
    "\n",
    "    return loaded_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788705f-2f61-4a5c-ad84-ea24f56bccdb",
   "metadata": {},
   "source": [
    "Now we will try to calculate how much did each scraper take to collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0669639d-0404-4d98-aecf-1a626addee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86 s ± 376 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "925 ms ± 121 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit upwork_scraper('Data analytics', 1)\n",
    "%timeit guru_scraper('Data analytics', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cfd62-4513-4cd0-bea7-40c4638debc0",
   "metadata": {},
   "source": [
    "## <center><strong>Collecting the Websites <span style = 'color: #4895e0'> HTML</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11593e37-e845-41f0-b749-5f6c8d7a0914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b219d160-162a-4860-9116-60302878d539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started scraping Guru data.\n",
      "Finished loading Data entry jobs data from Guru\n",
      "Finished loading Data Engineering jobs data from Guru\n",
      "Finished loading Data scientist jobs data from Guru\n",
      "Finished loading Data Analyst jobs data from Guru\n",
      "Finished loading Machine Learning jobs data from Guru\n",
      "\n",
      "Started scraping Upwork data.\n",
      "Finished loading Data entry jobs data from Upwork\n",
      "Finished loading Data Engineering jobs data from Upwork\n",
      "Finished loading Data scientist jobs data from Upwork\n",
      "Finished loading Data Analyst jobs data from Upwork\n",
      "Finished loading Machine Learning jobs data from Upwork\n"
     ]
    }
   ],
   "source": [
    "soups : dict = {'jobs_soups'     : [],\n",
    "                'data_job_title' : [],\n",
    "                'platform'       : []}\n",
    "\n",
    "for scraper in [guru_scraper, upwork_scraper]:\n",
    "    scraper_name      : str  = retrieve_name(scraper)[0].split('_')[0].title()\n",
    "    print(f'\\nStarted scraping {scraper_name} data.')\n",
    "    \n",
    "    for job_title in data_jobs_titles:\n",
    "\n",
    "        concated_jobs : list = []\n",
    "\n",
    "        for i in count(0):\n",
    "            page      : int  = i + 1\n",
    "            jobs      : list = scraper(job_title, page)\n",
    "\n",
    "            if (len(jobs) == 0) or (page >= MAX_PAGES):\n",
    "                break\n",
    "\n",
    "            for job in jobs:\n",
    "                concated_jobs.append(job)\n",
    "\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs data from {scraper_name}')\n",
    "        \n",
    "        soups['jobs_soups']     .append(concated_jobs)\n",
    "        soups['data_job_title'] .append(job_title)\n",
    "        soups['platform']       .append(scraper_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00632aa1-9b7a-4e33-ab44-962f17b39b0b",
   "metadata": {},
   "source": [
    "## <center><strong> Collecting <span style= 'color: #517afc'>Cards</span> Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddfc02-f25f-4f61-9f26-6b13056422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "guru_df   = stacked_df[stacked_df['platform'] == 'Guru'].reset_index()\n",
    "upwork_df = stacked_df[stacked_df['platform'] == 'Upwork'].reset_index()\n",
    "\n",
    "guru_df['describtion']     = None\n",
    "guru_df['earnings_amount'] = None\n",
    "guru_df['feedback']        = None\n",
    "guru_df['name']            = None\n",
    "guru_df['job_title']       = None\n",
    "guru_df['addresse']        = None\n",
    "guru_df['hour_rate']       = None\n",
    "guru_df['minimum_pay']     = None\n",
    "guru_df['skills']          = None\n",
    "\n",
    "upwork_df['describtion']     = None\n",
    "upwork_df['earnings_amount'] = None\n",
    "upwork_df['feedback']        = None\n",
    "upwork_df['name']            = None\n",
    "upwork_df['job_title']       = None\n",
    "upwork_df['addresse']        = None\n",
    "upwork_df['hour_rate']       = None\n",
    "upwork_df['skills']          = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f86c767-78f2-46d0-8ddb-3b80bfcecacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, jobs in enumerate(guru_df['jobs_soups']):\n",
    "    \n",
    "    describtions     : list = []\n",
    "    earnings_amounts : list = []\n",
    "    feedbacks        : list = []\n",
    "    names            : list = []\n",
    "    job_titles       : list = []\n",
    "    addresses        : list = []\n",
    "    hour_rates       : list = []\n",
    "    minimum_pays     : list = []\n",
    "    skills           : list = []\n",
    "\n",
    "    for job in jobs:\n",
    "\n",
    "        try:      describtions     .append(job.find('p', 'serviceListing__desc').text.strip())\n",
    "        except:   describtions     .append(np.nan)\n",
    "\n",
    "        # Earnings per year\n",
    "        try:      earnings_amounts .append(job.find('span', 'earnings__amount').text.strip())\n",
    "        except:   earnings_amounts .append(np.nan)\n",
    "\n",
    "        # Feedback\n",
    "        try:      feedbacks        .append(job.find('span', 'freelancerAvatar__feedback').text.strip())\n",
    "        except:   feedbacks        .append(np.nan)\n",
    "\n",
    "        # Name\n",
    "        try:      names            .append(job.find('h3', 'freelancerAvatar__screenName').find('a').text.strip())\n",
    "        except:   names            .append(np.nan)\n",
    "\n",
    "        # Job title\n",
    "        try:      job_titles       .append(job.find('h2', 'serviceListing__title serviceListing__title--dark').find('a').text.strip())\n",
    "        except:   job_titles       .append(np.nan)\n",
    "\n",
    "        # Addresse\n",
    "        try:      addresses        .append(job.find('span', 'freelancerAvatar__location')['title'].strip())\n",
    "        except:   addresses        .append(np.nan)\n",
    "        \n",
    "        # Hour rate\n",
    "        try:      hour_rates       .append(job.find('p', 'serviceListing__rates').text.split('.')[0].strip())\n",
    "        except:   hour_rates       .append(np.nan)\n",
    "        \n",
    "        # Minimum pays\n",
    "        try:      minimum_pays     .append(job.find('p', 'serviceListing__rates').text.split('·')[-1].strip())\n",
    "        except:   minimum_pays     .append(np.nan)\n",
    "\n",
    "        # Skills\n",
    "        try:\n",
    "            skills_temp = job.find_all('a', 'skillsList__skill skillsList__skill--hasHover')\n",
    "            skills                 .append([skill.text.strip() for skill in skills_temp])\n",
    "            \n",
    "        except:   skills           .append(np.nan)\n",
    "        \n",
    "    # Now we are going to convert list into tuples.\n",
    "    guru_df['describtion']     [i] = tuple(describtions)\n",
    "    guru_df['earnings_amount'] [i] = tuple(earnings_amounts)\n",
    "    guru_df['feedback']        [i] = tuple(feedbacks)\n",
    "    guru_df['name']            [i] = tuple(names)\n",
    "    guru_df['job_title']       [i] = tuple(job_titles)\n",
    "    guru_df['addresse']        [i] = tuple(addresses)\n",
    "    guru_df['hour_rate']       [i] = tuple(hour_rates)\n",
    "    guru_df['minimum_pay']     [i] = tuple(minimum_pays)\n",
    "    guru_df['skills']          [i] = tuple(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3ba47-4b3a-471d-a402-a9505773b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, jobs in enumerate(upwork_df['jobs_soups']):\n",
    "    \n",
    "    describtions     : list = []\n",
    "    earnings_amounts : list = []\n",
    "    feedbacks        : list = []\n",
    "    names            : list = []\n",
    "    job_titles       : list = []\n",
    "    countries        : list = []\n",
    "    hour_rates       : list = []\n",
    "    skills           : list = []\n",
    "\n",
    "    for job in jobs:\n",
    "\n",
    "        try:      describtions     .append(job.find('div', 'up-line-clamp-v2 clamped').text.strip())\n",
    "        except:   describtions     .append(np.nan)\n",
    "\n",
    "        # Earnings per year\n",
    "        try:      earnings_amounts .append(job.find('span',attrs= {'data-test', 'earned-amount-formatted'}).text.strip())\n",
    "        except:   earnings_amounts .append(np.nan)\n",
    "\n",
    "        # Feedback\n",
    "        try:      feedbacks        .append(job.find('span', 'up-job-success-text').text.split('%')[0].strip())\n",
    "        except:   feedbacks        .append(np.nan)\n",
    "\n",
    "        # Name\n",
    "        try:      names            .append(job.find('div', 'identity-name').text.strip())\n",
    "        except:   names            .append(np.nan)\n",
    "\n",
    "        # Job title\n",
    "        try:      job_titles       .append(job.find('p', 'my-0 freelancer-title').find('strong').text.strip())\n",
    "        except:   job_titles       .append(np.nan)\n",
    "\n",
    "        # Country\n",
    "        try:      countries        .append(job.find('span', attrs= {'itemprop': 'country-name'}).text.strip())\n",
    "        except:   countries        .append(np.nan)\n",
    "        \n",
    "        # Hour rate\n",
    "        try:      hour_rates       .append(job.find('p', 'serviceListing__rates').text.split('.')[0].strip())\n",
    "        except:   hour_rates       .append(np.nan)\n",
    "        \n",
    "        # Skills\n",
    "        try:\n",
    "            skills_temp = job.find_all('a', 'skillsList__skill skillsList__skill--hasHover')\n",
    "            skills                 .append([skill.text.strip() for skill in skills_temp])\n",
    "            \n",
    "        except:   skills           .append(np.nan)\n",
    "        \n",
    "    # Now we are going to convert list into tuples.\n",
    "    upwork_df['describtion']     [i] = tuple(describtions)\n",
    "    upwork_df['earnings_amount'] [i] = tuple(earnings_amounts)\n",
    "    upwork_df['feedback']        [i] = tuple(feedbacks)\n",
    "    upwork_df['name']            [i] = tuple(names)\n",
    "    upwork_df['job_title']       [i] = tuple(job_titles)\n",
    "    upwork_df['addresse']        [i] = tuple(addresses)\n",
    "    upwork_df['hour_rate']       [i] = tuple(hour_rates)\n",
    "    upwork_df['minimum_pay']     [i] = tuple(minimum_pays)\n",
    "    upwork_df['skills']          [i] = tuple(skills)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
