{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d87eb4b-c943-4562-866b-7c2dd151f3c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong>Notebook<span style= \"color: #51FCC6\"> Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753d54c-1fd0-441c-b663-98ca78298128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59118b7-50cf-4738-8e35-cd9de6b6e46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b08419ad-46bc-42a6-a678-356c38c435fe",
   "metadata": {},
   "source": [
    "## <center><strong>Importing<span style= \"color: #48E0DC\"> Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfca499a-0b6e-42a6-9901-a25b403a1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b8e77-3b20-4c92-8619-434de6cedf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_9856\\2138414856.py:8: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "COLORS = [\"#51fcc6\", \"#48e0dc\", \"#5cd3f7\", \"#4895e0\", \"#517afc\"]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3e9fc-3746-4f76-be83-00f2b453be34",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= \"color: #5CD3F7\"> Web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd35b8e-8c12-4e99-8819-4c6e338c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_titles = [\"data-entry\", \"data-processing\", \"data-engineering\"]\n",
    "                    # \"data-science\", \"data-analytics\", \"data-visualization\"]\n",
    "\n",
    "upwork_types = [\"profiles\", \"jobs\", \"services\"]\n",
    "pph_types = [\"hire-freelancers\", \"freelance-jobs\", \"services\"]\n",
    "guru_service_types = [\"freelancers\", \"jobs\"]\n",
    "\n",
    "\n",
    "def scrape_page(url: str, retrieve_new_url= False) -> BeautifulSoup:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    # Wait until the page is fully loaded\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    \n",
    "    html = driver.page_source\n",
    "\n",
    "    if retrieve_new_url:\n",
    "        \n",
    "        new_url = driver.current_url\n",
    "        driver.quit()\n",
    "        \n",
    "        return BeautifulSoup(html), new_url\n",
    "        \n",
    "    driver.quit()\n",
    "\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3908558f-1fe9-4c88-af63-731658ee2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `type` param indicates what is the nature of the job is it hiring freelancers \n",
    "# or paying services or othr thing based on the platform.\n",
    "\n",
    "\n",
    "def upwork_scraper(page, job_title, type):\n",
    "\n",
    "    global upwork_url\n",
    "\n",
    "    if  type == 'services':\n",
    "\n",
    "        upwork_url =  f\"https://www.upwork.com/services/search?nbs=1&q={job_title}\"\n",
    "\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(upwork_url)\n",
    "\n",
    "        if page > 1:\n",
    "            for i in range(page):\n",
    "\n",
    "                try:\n",
    "                    wait = WebDriverWait(driver, 10)\n",
    "                    element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.up-btn.up-btn-default.mx-0.load-more-btn.nuxt-link-active')))\n",
    "                    element.click()\n",
    "                    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "                except:\n",
    "                    raise  KeyError\n",
    "\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        return  html\n",
    "\n",
    "    else:\n",
    "        upwork_url = f\"https://www.upwork.com/search/{type}/?page={page}&q={job_title}&user_pref=1\"\n",
    "        html, loaded_url = scrape_page(upwork_url, retrieve_new_url= True)\n",
    "        loaded_page = int(loaded_url.split('&')[0].split('=')[-1])\n",
    "        \n",
    "        if type == \"profiles\":\n",
    "            if html.find(\"h2\", class_=\"mt-30 text-muted\"):\n",
    "                # We are going to use the keyerror to indicate that the page param is more than the\n",
    "                # max possible page whitch we will use to control the other scrapers.\n",
    "                raise  KeyError\n",
    "\n",
    "        elif type == \"jobs\":\n",
    "\n",
    "            if page > loaded_page:\n",
    "                raise  KeyError\n",
    "        \n",
    "        print(upwork_url)\n",
    "        print(str(loaded_page))\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16ff1bdf-99da-46f6-ab1e-b41c542daaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guru_scraper(page, job_title, type):\n",
    "\n",
    "    global guru_url\n",
    "    guru_url = f\"https://www.guru.com/d/{type}/skill/{job_title}/pg/{page}/\"\n",
    "    html = scrape_page(guru_url)\n",
    "\n",
    "    if len(html.find_all(\"div\", class_= \"record jobRecord\")) == 0 and type == \"jobs\":\n",
    "        raise  KeyError\n",
    "\n",
    "    elif type == \"freelancers\":\n",
    "        try:\n",
    "            html.find(\"div\", \"record record--avatarCheck findGuruRecord\")\n",
    "            \n",
    "        except:\n",
    "            raise  KeyError\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def fivver_scraper(page, job_title):\n",
    "\n",
    "    global fivver_url\n",
    "    fivver_url = f\"https://www.fiverr.com/categories/data/{job_title}?source=pagination&page={page}\"\n",
    "    html = scrape_page(fivver_url)\n",
    "\n",
    "    if html.find(\"h3\", class_= \"title\"):\n",
    "        if \"Well, this isn't what you were looking for\" in html.find(\"h3\", class_= \"title\"):\n",
    "            raise  KeyError\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def pph_scraper(page, job_title, type):\n",
    "\n",
    "    global peopleperhour_url\n",
    "    \n",
    "    pph_url = f\"https://www.peopleperhour.com/{type}/{job_title.replace('-', '+')}?page={page}\"\n",
    "    html = scrape_page(pph_url)\n",
    "\n",
    "    if type == \"freelance-jobs\":\n",
    "        loaded_page = [int(element.text) for element in html.find_all(\n",
    "            'a', class_= 'pagination__link⤍SimplePagination⤚1MYH2') if \"current page\" in str(element)][0]\n",
    "    \n",
    "        if page > loaded_page:\n",
    "            raise  KeyError\n",
    "\n",
    "    if type in (\"hire-freelancers\", \"services\"):\n",
    "        loaded_pages = [element.text for element in html.find_all(\n",
    "            'a', class_= 'pagination__link⤍SimplePagination⤚1MYH2')]\n",
    "\n",
    "        loaded_page = max([int(page) for page in loaded_pages if len(page) > 0])\n",
    "    \n",
    "        if page > loaded_page:\n",
    "            raise  KeyError\n",
    "            \n",
    "    return  html\n",
    "\n",
    "# For sorry freelancer.com 's search via url is semi broken so we will search manually\n",
    "#mwith selenuim.\n",
    "\n",
    "def freelancer_scraper(page, job_title):\n",
    "\n",
    "    global freelancer_url\n",
    "    freelancer_url = f\"https://www.freelancer.com/jobs/{page}\"\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(freelancer_url)\n",
    "    \n",
    "    if page > 200:\n",
    "        driver.quit()\n",
    "        raise KeyError\n",
    "        \n",
    "    search_bar = driver.find_element(By.ID, \"keyword-input\")\n",
    "    search_bar.send_keys(job_title.replace(\"-\", \" \"))\n",
    "    \n",
    "    search_button = driver.find_element(By.ID, \"search-submit\") \n",
    "    search_button.click()\n",
    "\n",
    "    # wait = WebDriverWait(driver, 30)  # Wait for a maximum of 10 seconds\n",
    "    # wait.until(EC.staleness_of(driver.find_element(By.TAG_NAME, \"body\")))\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    if page != 1:\n",
    "        current_url = driver.current_url\n",
    "        loaded_page = int(current_url.split(\"/\")[-2])\n",
    "    \n",
    "        if page > loaded_page:\n",
    "            driver.quit()\n",
    "            raise KeyError\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    return  html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c6ab6-42a2-4ad5-8900-e44e88fcc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "freelancer_scraper(2, \"data-analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788705f-2f61-4a5c-ad84-ea24f56bccdb",
   "metadata": {},
   "source": [
    "Now we will try to calculate how much did each scraper take to collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0141299-f023-4a63-8e8b-52320c0b3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_variable_name = lambda var: [name for name in globals() if globals()[name] is var][0]\n",
    "\n",
    "def get_runtime(func, *args, **kwargs):\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    output = func(*args, **kwargs)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    run_time = t2 - t1\n",
    "\n",
    "    print(\"{} took about {:.2f}s to run.\\n\".format(\n",
    "        get_variable_name(func).replace(\"_\", \" \").title(), run_time))\n",
    "\n",
    "    # return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619ad86-0c3f-4288-a5cb-8b3591513b88",
   "metadata": {},
   "source": [
    "Now we will just try to scrape **only one page** from each scraper to just **test** the data extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0669639d-0404-4d98-aecf-1a626addee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.freelancer.com/jobs/5/?keyword=data%20analytics\n",
      "Freelancer Scraper took about 20.28s to run.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_runtime(pph_scraper, 10, \"data-analytics\", \"services\")\n",
    "# get_runtime(upwork_scraper, 1, \"data-analytics\", \"services\")\n",
    "# get_runtime(fivver_scraper, 10, \"data-analytics\")\n",
    "# get_runtime(guru_scraper, 1, \"data-analytics\", \"freelancers\")\n",
    "get_runtime(freelancer_scraper, 5, \"data-analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cfd62-4513-4cd0-bea7-40c4638debc0",
   "metadata": {},
   "source": [
    "## <center><strong>Collecting the Websites <span style = \"color: #4895e0\"> HTML</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b219d160-162a-4860-9116-60302878d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = {\"soup_name\": [],\n",
    "         \"soup_job_type\": [],\n",
    "         \"soup_job_title\": [],\n",
    "         \"soup_page\":  [],\n",
    "         \"soup\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffd0dd29-0acc-41d6-a681-1a6e30edcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pph_html = pph_scraper(10, \"data-entry\", \"freelance-jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d79a77b-f579-4750-984a-599fa9df13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the job type is services in upwork the pages paremeter will refer to how many pages \n",
    "# will render at once while the other scrapers will render the spesfic spesfied page only.\n",
    "\n",
    "def get_all_pages_soups(pph_type, upwork_type, guru_type, max_page: int= 999) -> dict:\n",
    "\n",
    "    print(\"Strating ...\")\n",
    "    flag = False\n",
    "    \n",
    "    for i in count(0):\n",
    "        \n",
    "        page = i + 1\n",
    "\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "        print(f\"Loading page {page} of each website ...\")\n",
    "\n",
    "        for data_job_title in data_jobs_titles:\n",
    "\n",
    "            lines = [f'freelancer_html = freelancer_scraper({page}, \"{data_job_title}\")',\n",
    "                     f'pph_html = pph_scraper({page}, \"{data_job_title}\", \"{pph_type}\")']\n",
    "            \n",
    "            # f'fivver_html = fivver_scraper({page}, \"{data_job_title}\")',\n",
    "            # f'upwork_html = fivver_scraper({page}, \"{data_job_title}\")',\n",
    "            # f'guru_html = fivver_scraper({page}, \"{data_job_title}\")',\n",
    "\n",
    "            if len(lines) < 1:\n",
    "\n",
    "                print(\"Finished ...\")\n",
    "                flag = True\n",
    "\n",
    "            if flag:\n",
    "                break\n",
    "            \n",
    "\n",
    "            for line_idx, line in enumerate(lines):\n",
    "\n",
    "                equall_sign_idx = line.find('=') - 1\n",
    "                first_underscore_idx = line.find('_')\n",
    "\n",
    "                if page > max_page:\n",
    "                    del lines[line_idx]\n",
    "\n",
    "\n",
    "                try:\n",
    "                    exec(line)\n",
    "                    print(line)\n",
    "                    \n",
    "                    soups[\"soup_job_title\"].append(data_job_title)\n",
    "                    soups[\"soup_name\"].append(line[:equall_sign_idx])\n",
    "                    soups[\"soup_page\"].append(page)\n",
    "                    soups[\"soup\"].append(eval(str(line[:equall_sign_idx])))\n",
    "                    soups[\"soup_job_type\"].append(eval(f'{line[:first_underscore_idx]}_type'))\n",
    "\n",
    "\n",
    "                except KeyError as e:\n",
    "\n",
    "                    print(e)\n",
    "                    del lines[line_idx]\n",
    "\n",
    "                # except Exception as e:\n",
    "                #     print(e)\n",
    "\n",
    "                    # continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3c985db-6dab-4034-998f-aa6645e2a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strating ...\n",
      "Loading page 1 of each website ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_all_pages_soups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfreelance-jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupwork_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mguru_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 46\u001b[0m, in \u001b[0;36mget_all_pages_soups\u001b[1;34m(pph_type, upwork_type, guru_type, max_page)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m lines[line_idx]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line)\n\u001b[0;32m     49\u001b[0m     soups[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoup_job_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(data_job_title)\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[37], line 85\u001b[0m, in \u001b[0;36mfreelancer_scraper\u001b[1;34m(page, job_title)\u001b[0m\n\u001b[0;32m     82\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     84\u001b[0m current_url \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mcurrent_url\n\u001b[1;32m---> 85\u001b[0m loaded_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurrent_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m page \u001b[38;5;241m>\u001b[39m loaded_page:\n\u001b[0;32m     88\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'jobs'"
     ]
    }
   ],
   "source": [
    "get_all_pages_soups(pph_type= \"freelance-jobs\", upwork_type= \"jobs\",\n",
    "                    guru_type= \"jobs\", max_page= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85626097-1809-40b9-a17c-ff3538cb1f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col1': [1, 2, 3, 5, 5], 'col2': [1, 2, 3, 9, nan]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(lst) for lst in soups.values())\n",
    "soups = {key: lst + [float('nan')] * (max_length - len(lst)) for key, lst in soups.items()}\n",
    "\n",
    "df = pd.DataFrame(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7ce831-bd81-4a23-b771-30457a6a1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the title of each page to check if it's loaded correctly.\n",
      "\n",
      "-\t Freelance Data-analytics Jobs - Upwork\n",
      "-\t Data Analytics Services Online | Fiverr\n",
      "-\t Machine Learning Freelance Jobs Online - Guru\n",
      "-\t Get Data Analytics Offers & Services | PeoplePerHour\n",
      "-\t Users Jobs, Employment | Freelancer\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the title of each page to check if it's loaded correctly.\\n\")\n",
    "\n",
    "for soup in soups[\"soup\"]:\n",
    "    print(\"-\\t\", soup.title.text.replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "\n",
    "soups = pd.DataFrame(soups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db860af4-9005-4f07-8bfa-cab4cdb3aa87",
   "metadata": {},
   "source": [
    "Looks like everything is working as expected now let's start reel work by extracting the HTML tags data from the scraped<br>\n",
    "pages.<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac5af7-1f3f-44a7-b93b-778e70096067",
   "metadata": {},
   "source": [
    "But we will need to collect the data from each one of those pages with unique way further more we need to optimize the<br>\n",
    "data extractor to collect the data from other pages from the same site like \"jobs\" site and \"profiles\" sites on upwork.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dfb3b7e-ed6c-49db-a27b-05e963d689ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards_links(soup_name, tag, link_template,\n",
    "                    link_attr, tag_class, attr= \"class\") -> list:\n",
    "\n",
    "    soup = soups[soups[\"soup_name\"] == soup_name][\"soup\"].values[0]\n",
    "\n",
    "    cards = soup.find_all(tag, attrs= {attr: tag_class})\n",
    "    cards_ids = []\n",
    "\n",
    "    if link_attr != \"\":\n",
    "\n",
    "        for card in cards:\n",
    "\n",
    "            card_id = link_template.replace(\"###\", card[link_attr])\n",
    "\n",
    "            if len(card_id) > len(link_template):\n",
    "                cards_ids.append(card_id)\n",
    "\n",
    "        return  cards_ids\n",
    "\n",
    "    else:\n",
    "        return  cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "710987e9-6eb6-47cd-a8ec-8d1777445501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't collect guru data this wat becuase it has slightly deffrint case in terms\n",
    "# of collecting the jobs URLs\n",
    "\n",
    "## --------------------------Upwork--------------------------\n",
    "upwork_gigs_links = get_cards_links('upwork_soup', 'a', 'https://upwork.com###', 'href',\n",
    "                                        'up-n-link project-tile up-card m-0 p-0 up-card-section up-card-hover project-tile_flexible')\n",
    "\n",
    "upwork_profiles_links = get_cards_links('upwork_soup', 'div', 'https://www.upwork.com/search/profiles/?profile=###', 'data-test-key',\n",
    "                                         'up-card-section up-card-hover')\n",
    "\n",
    "upwork_jobs_parents = get_cards_links('upwork_soup', 'h3', 'https://www.upwork.com###', '', 'my-0 p-sm-right job-tile-title')\n",
    "upwork_jobs_links = []\n",
    "\n",
    "for parent in upwork_jobs_parents:\n",
    "    upwork_jobs_links.append(\"https://www.upwork.com\" + parent.find('a', class_= \"up-n-link\")['href'])\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## --------------------------Fivver--------------------------\n",
    "fivver_gigs_links = get_cards_links('fivver_soup', 'a', 'https://www.fiverr.com###', 'href', 'tbody-5 p-t-4 YLycza2 u9KHmsf')\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## ---------------------People per hour----------------------\n",
    "pph_services_links = get_cards_links(\"peopleperhour_soup\", \"a\", \"https://www.peopleperhour.com###\", \"href\", \n",
    "                                     \"card__title-wrapper⤍FreelancerCard⤚2-NWk\")\n",
    "\n",
    "pph_freelancers_links = get_cards_links(\"peopleperhour_soup\", \"a\", \"https://www.peopleperhour.com###\", \"href\",\n",
    "                                        \"card__title-wrapper⤍FreelancerCard⤚2-NWk\")\n",
    "\n",
    "pph_jobs_links = get_cards_links(\"peopleperhour_soup\", \"a\", \"https://www.peopleperhour.com###\", \"href\",\n",
    "                                 \"item__url⤍ListItem⤚20ULx\")\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## ------------------------Guru------------------------------\n",
    "guru_cards_parents_profiles = get_cards_links(\"guru_soup\", \"h2\", \"https://guru.com###\", \"\",\n",
    "                                              \"jobRecord__title jobRecord__title--changeVisited\")\n",
    "\n",
    "guru_cards_parents_jobs = get_cards_links(\"guru_soup\", \"h2\", \"https://guru.com###\", \"\",\n",
    "                                          \"jobRecord__title jobRecord__title--changeVisited\")\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## ----------------------Freelancer---------------------------\n",
    "freelancer_jobs_links = get_cards_links(\"freelancer_soup\", \"a\", \"https://www.freelancer.com###\", \"href\",\n",
    "                                        \"LinkElement ng-star-inserted\")\n",
    "\n",
    "freelancer_freelancers_links = get_cards_links(\"freelancer_soup\", \"a\", \"https://www.freelancer.com###\", \"href\",\n",
    "                                              \"LinkElement ng-star-inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1caa82-9049-40af-9248-5a6dcf4d404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upwork_links(job_type: str) -> list:  \n",
    "\n",
    "    card_tag = \"a\"\n",
    "    link_template = \"https://www.upwork.com###\"\n",
    "    link_attr = \"href\"\n",
    "\n",
    "    if job_type == \"profiles\":\n",
    "\n",
    "        card_tag = \"div\"\n",
    "        card_tag_class = \"up-card-section up-card-hover\"\n",
    "        link_template = \"https://www.upwork.com/search/profiles/?q=profiles&profile=###\"\n",
    "        link_attr = \"data-test-key\"\n",
    "    \n",
    "    elif job_type == \"jobs\":\n",
    "        card_tag_class = \"up-n-link\"\n",
    "\n",
    "\n",
    "    elif job_type == \"services\":\n",
    "        card_tag_class = \"up-n-link project-tile up-card m-0 p-0 up-card-section up-card-hover project-tile_flexible\"\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Please input correct work type from ['profiles', 'jobs', 'services'].\")\n",
    "\n",
    "\n",
    "    soup = soups[soups[\"soup_name\"] == \"upwork_soup\"][\"soup\"][0]\n",
    "\n",
    "    cards = soup.find_all(card_tag, class_= card_tag_class)\n",
    "    cards_ids = []\n",
    "\n",
    "    for card in cards:\n",
    "        \n",
    "        card_id = link_template.replace(\"###\", card[link_attr])\n",
    "        \n",
    "        if len(card_id) > len(link_template):\n",
    "            cards_ids.append(card_id)\n",
    "\n",
    "    return  cards_ids\n",
    "\n",
    "\n",
    "def get_fivver_links() -> list:\n",
    "\n",
    "    card_tag = \"div\"\n",
    "    card_tag_class = \"basic-gig-card\"\n",
    "    link_template = \"https://www.fiverr.com###\"\n",
    "    link_attr = \"href\"\n",
    "\n",
    "    cards = soup.find_all(card_tag, class_= card_tag_class)\n",
    "    cards_ids = []\n",
    "    \n",
    "    for card in cards:\n",
    "        \n",
    "        card_id = link_template.replace(\"###\", card[link_attr])\n",
    "        \n",
    "        if len(card_id) > len(link_template):\n",
    "            cards_ids.append(card_id)\n",
    "\n",
    "    return  cards_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8189c7-4417-4b58-82f6-e1b634d4aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guru_links(job_type: str) -> list:  \n",
    "\n",
    "    soup = soups[soups[\"soup_name\"] == \"guru_soup\"][\"soup\"][2]\n",
    "    link_template = \"https://www.guru.com###\"\n",
    "\n",
    "    if job_type == \"freelancers\":\n",
    "        cards_parents = soup.find_all(\n",
    "            'h2', class_= 'serviceListing__title serviceListing__title--dark')\n",
    "\n",
    "        cards = []\n",
    "\n",
    "        for parent in cards_parents:\n",
    "            card = parent.find('a')\n",
    "            cards.append(card)\n",
    "\n",
    "    elif job_type == \"jobs\":\n",
    "        cards_parents = soup.find_all(\n",
    "            'h2', class_ = 'jobRecord__title jobRecord__title--changeVisited')\n",
    "\n",
    "        cards = []\n",
    "\n",
    "        for parent in cards_parents:\n",
    "            card = parent.find('a')\n",
    "            cards.append(card)\n",
    "\n",
    "    cards_ids = []\n",
    "\n",
    "    for card in cards:\n",
    "        card_id = link_template.replace(\"###\", card['href'])\n",
    "        cards_ids.append(card_id)\n",
    "\n",
    "    return  cards_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd419a-7ce6-4b9f-a30b-43b66e59de47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
