{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Importing <span style= 'color: #48e0dc'>Packeges</span>\n",
    "<sub>*And setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "import cloudscraper\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from fp.fp                            import FreeProxy\n",
    "from itertools                          import count\n",
    "from bs4                                import BeautifulSoup\n",
    "from selenium                           import webdriver\n",
    "from google.cloud                       import bigquery\n",
    "from urllib.request                     import urlopen\n",
    "from selenium.webdriver.common.by       import By\n",
    "from scrapy.utils.log                   import configure_logging\n",
    "from selenium.webdriver.common.keys     import Keys\n",
    "from IPython.display                    import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service  import Service\n",
    "from selenium.webdriver.chrome.options  import Options\n",
    "from selenium.webdriver.support.ui      import WebDriverWait\n",
    "from webdriver_manager.chrome           import ChromeDriverManager\n",
    "from selenium.webdriver.support         import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(1, '../my_encrypter')\n",
    "import encrypt\n",
    "\n",
    "encrypt.decrypt_json_file('../credentials.json')\n",
    "\n",
    "MAX_PAGES :int  = 15\n",
    "COLORS    :list = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS  :list = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                'uint16', 'uint32', 'uint64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209e763-b75d-4020-a8ba-db561004ec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_jobs_titles = ['Data entry', 'Data engineer',\n",
    "                    'Data scientist', 'Data analyst',\n",
    "                    'ML developer']\n",
    "\n",
    "# indeed_countries = ['de', 'uk', 'www']# USACOLORS    :list = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS  :list = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                   'uint16', 'uint32', 'uint64'] don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5cd3f7'> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_fast(url: str) -> BeautifulSoup:\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(delay= 10,browser= {\n",
    "        'browser': 'chrome',\n",
    "        'platform': 'windows',\n",
    "        'desktop': True,\n",
    "        'mobile': False}) \n",
    "    \n",
    "    content = scraper.get(url).text \n",
    "    return BeautifulSoup(content)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_page(url: str, retrieve_new_url= None, user_agent= None) -> BeautifulSoup:\n",
    "    \n",
    "    # chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  # Optional: Run Chrome in headless mode\n",
    "    # chrome_options.add_argument(f\"--user-agent={ua.random}\")\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "\n",
    "    html = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    if retrieve_new_url:\n",
    "        new_url = driver.current_url\n",
    "\n",
    "        driver.quit()\n",
    "        return html, new_url\n",
    "        \n",
    "    driver.quit()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05c858c-9b35-48c2-8a2e-f166081768f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 s Â± 356 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "13 s Â± 2.12 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Comparing the two scrapers speed.\n",
    "%timeit scrape_page_fast('https://linkedin.com')\n",
    "%timeit scrape_page('https://linkedin.com')\n",
    "\n",
    "# We can find that there's about 10X speed for the `scrape_page_fast` ðŸ¤¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# def indeed_scraper(country   : str,\n",
    "#                    job_title : str\n",
    "#                    page      : int) -> BeautifulSoup:\n",
    "\n",
    "#     ua           : str = UserAgent.random\n",
    "#     modefid_page : int = (int(page) - 1) * 10\n",
    "#     headers      : dict = {\n",
    "#         'User-Agent': ua,\n",
    "#         'Accept-Encoding': '*',\n",
    "#         'Connection': 'keep-alive'}\n",
    "        \n",
    "#     url          : str = f'https://{country}.indeed.com/jobs?q={job_title}&start={modefid_page}'\n",
    "\n",
    "#     response = requests.get(url, headers= headers)\n",
    "#     soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "#     return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(country:   str, # This scraper scrapes all pages at once\n",
    "                     job_title: str,\n",
    "                     page:      int) -> BeautifulSoup:\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "        'Accept-Encoding': '*',\n",
    "        'Connection': 'keep-alive'}\n",
    "\n",
    "    page: int = (page - 1) * 25\n",
    "    url: str = f'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={job_title}&' + \\\n",
    "    f'location={country}&geoId=90000084&trk=public_jobs_j%20obs-search-bar_search-submit&position=1&pageNum=0&start={page}'\n",
    "\n",
    "    response = requests.get(url, headers= headers)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951bed7-9819-4466-8604-9660043e7744",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <center><strong>Collecting<span style= 'color: #4895e0'> jobs </span> data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452ce6e-7b8b-4a82-b3e5-e6911937dea4",
   "metadata": {},
   "source": [
    "**What we will do in this section:**\n",
    "\n",
    "1. Collect all pages we need from each scraper and stack them together as HTML code.\n",
    "2. Scrape for jobs cards links from linkedin and indeed.\n",
    "3. Scrape for the data in those links and store the data in a DataFrame\n",
    "4. We may also consider collecting total results per each platform, country etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3d2-25cd-4321-b091-d2a7914786f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Stack pages together as HTML code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bf4c2b-d50b-494d-b2e1-5f41012401a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_integer(number_string):\n",
    "\n",
    "    cleaned_string = ''.join(filter(str.isdigit, number_string))\n",
    "    cleaned_string = cleaned_string.rstrip('+')\n",
    "    integer_value = int(cleaned_string)\n",
    "\n",
    "    return integer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791acf9d-694a-417f-9015-31fceed0babc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LinkedIn scraper.\n",
      "Loading the European Union data.\n",
      "Finished loading Data entry jobs from the European Union.\n",
      "Finished loading Data engineer jobs from the European Union.\n",
      "Finished loading Data scientist jobs from the European Union.\n",
      "Finished loading Data analyst jobs from the European Union.\n",
      "Finished loading ML developer jobs from the European Union.\n",
      "\n",
      "\n",
      "Loading the United States data.\n",
      "Finished loading Data entry jobs from the United States.\n",
      "Finished loading Data engineer jobs from the United States.\n",
      "Finished loading Data scientist jobs from the United States.\n",
      "Finished loading Data analyst jobs from the United States.\n",
      "Finished loading ML developer jobs from the United States.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stacked_df: dict = {'soup':        [],\n",
    "                    'job_title':   [],\n",
    "                    'country':     [],\n",
    "                    'platform':    [],\n",
    "                    'total_jobs':  []}\n",
    "\n",
    "print('Starting LinkedIn scraper.')\n",
    "\n",
    "for country in linkedin_countries:\n",
    "\n",
    "    print(f'Loading the {country} data.')\n",
    "    for job_title in data_jobs_titles:\n",
    "        \n",
    "        full_soup   : str = ''\n",
    "        total_jobs  : list = convert_to_integer(scrape_page(f'https://www.linkedin.com/jobs/search?keywords={job_title}&location={country}&' + \n",
    "                                                            f'\\geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0').find(\n",
    "                                                                   'span', 'results-context-header__job-count').text)\n",
    "\n",
    "        for i in count(0):\n",
    "            page = i + 1\n",
    "            page_soup: BeautifulSoup = linkedin_scraper(country= country,\n",
    "                                                        job_title= job_title,\n",
    "                                                        page= i + page)\n",
    "            full_soup += str(page_soup)\n",
    "            full_soup += ' <br> '\n",
    "\n",
    "            if (page_soup.find_all('li') is None) or (page == MAX_PAGES):\n",
    "                break\n",
    "\n",
    "        stacked_df['platform']      .append('LinkedIn')\n",
    "        stacked_df['total_jobs']    .append(total_jobs)\n",
    "        stacked_df['soup']          .append(full_soup)\n",
    "        stacked_df['job_title']     .append(job_title)\n",
    "        stacked_df['country']       .append(country)\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs from the {country}.')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ed47b4-02cd-49ea-9eb2-96e4e377b417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indeed scraping TODO\n",
    "# for country in indeed_countries:\n",
    "#     for job_title in data_jobs_titles:\n",
    "#         stacked_pages_soup: str = ''\n",
    "\n",
    "#         for i in count(0):\n",
    "\n",
    "#             try:\n",
    "#                 page = i + 1\n",
    "#                 soup: BeautifulSoup = indeed_scraper(country= country,\n",
    "#                                                      job_title= job_title,\n",
    "#                                                      page= page)\n",
    "\n",
    "#                 stacked_pages_soup += '\\n<br> ' + str(soup)\n",
    "#                 if page > max_page:\n",
    "#                     print('Finished loading current country or job title.\\n')\n",
    "#                     break\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 print('Finished loading current country or job title.\\n')\n",
    "#                 break\n",
    "\n",
    "#         stacked_df['platform'].append('Indeed')\n",
    "#         stacked_df['soup'].append(stacked_pages_soup)\n",
    "#         stacked_df['job_title'].append(job_title)\n",
    "#         stacked_df['country'].append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5dc7-509a-490c-bd59-b341487cb5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Collecting jobs & companies basic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6983a18a-d68e-47f3-b27f-a48420b30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_df = pd.DataFrame(stacked_df)\n",
    "\n",
    "stacked_df['jobs_locations']  = None\n",
    "stacked_df['listing_dates']   = None\n",
    "stacked_df['jobs_titles']     = None\n",
    "stacked_df['companies_names'] = None\n",
    "\n",
    "stacked_df['soups']           = None\n",
    "stacked_df['jobs_links']      = None\n",
    "\n",
    "\n",
    "# Collecting linkedIn jobs links\n",
    "for i, soup in enumerate(stacked_df[stacked_df['platform'] == 'LinkedIn']['soup']):\n",
    "\n",
    "    soup = BeautifulSoup(soup)\n",
    "    jobs = soup.find_all('div', \n",
    "           'base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "\n",
    "    linkedin_jobs_locations       = []\n",
    "    linkedin_jobs_listing_dates   = []\n",
    "    linkedin_jobs_titles          = []\n",
    "    linkedin_jobs_companies_names = []\n",
    "    linkedin_jobs_links           = []\n",
    "    linkedin_companies_links      = []\n",
    "    \n",
    "    for job in jobs:\n",
    "\n",
    "        try: linkedin_jobs_listing_dates               .append(job.find('time', 'job-search-card__listdate')['datetime'])\n",
    "        except TypeError:  linkedin_jobs_listing_dates .append(np.nan)\n",
    "        \n",
    "        linkedin_jobs_locations            .append(job.find('span', 'job-search-card__location').text.strip())\n",
    "        linkedin_jobs_titles               .append(job.find('h3', 'base-search-card__title').text.strip())\n",
    "        linkedin_jobs_companies_names      .append(job.find('a', 'hidden-nested-link').text.strip())\n",
    "        linkedin_companies_links           .append(job.find('a', 'hidden-nested-link')['href'])\n",
    "        linkedin_jobs_links                .append(job.find\n",
    "                                                   ('a', 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')['href'])\n",
    "\n",
    "    # I will use sets and tuples instead of lists because they are faster.\n",
    "    stacked_df['jobs_locations']    [i] = tuple(linkedin_jobs_locations)\n",
    "    stacked_df['listing_dates']     [i] = tuple(linkedin_jobs_listing_dates)\n",
    "    stacked_df['jobs_titles']       [i] = tuple(linkedin_jobs_titles)\n",
    "    stacked_df['companies_names']   [i] = tuple(linkedin_jobs_companies_names)\n",
    "\n",
    "    stacked_df['jobs_links']        [i] = tuple(linkedin_jobs_links)\n",
    "    stacked_df['soups']             [i] = tuple(linkedin_companies_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a112b4b4-9ceb-49c7-a66c-e3d3da3990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting indeed jobs links TOFIX\n",
    "# for i, soup in enumerate(stacked_df[stacked_df['platform'] == 'Indeed']['soup']):\n",
    "    \n",
    "#     indeed_jobs_links: list = []\n",
    "#     indeed_companies_links: list = []\n",
    "\n",
    "#     for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "#         indeed_jobs_links.append(job_link)\n",
    "\n",
    "#     for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "#         company_link = BeautifulSoup(scrape_page(job_link)).find('a', 'css-775knl emf9s7v0')\n",
    "#         indeed_companies_links.append(company_link)\n",
    "#         print(company_link)\n",
    "        \n",
    "#     stacked_df['total_jobs'][i] = len(indeed_jobs_links)\n",
    "#     stacked_df['jobs_links'][i] = indeed_jobs_links\n",
    "#     stacked_df['soups'][i] = indeed_companies_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6883647-4384-4b32-b25b-672a6d704aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Loading more jobs data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b598359-33f4-4698-893b-aafc3fb1fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting more LinkedIn data\n",
    "\n",
    "# We will collect the: \n",
    "#    describtion, credential category, location type\n",
    "\n",
    "stacked_df['describtion']         = None\n",
    "stacked_df['reqierd_credential']  = None\n",
    "stacked_df['location_type']       = None\n",
    "stacked_df['employment_type']     = None\n",
    "stacked_df['industry']            = None\n",
    "\n",
    "for i, jobs in enumerate(stacked_df['jobs_links']): # TOREMOVE remove the loop limit\n",
    "\n",
    "    describtions:            list = []\n",
    "    reqierd_credentials:     list = []\n",
    "    location_types:          list = []\n",
    "    employment_types:        list = []\n",
    "    industries:              list = []\n",
    "    \n",
    "    for job in jobs:\n",
    "\n",
    "        try:\n",
    "            job_data:  dict = json.loads(scrape_page_fast(job).find(\n",
    "                'script', attrs= {'type': 'application/ld+json'}).text)\n",
    "        except:\n",
    "            job_data = np.nan\n",
    "\n",
    "        # Describtion\n",
    "        try:      describtions         .append(job_data['description'].strip())\n",
    "        except:   describtions         .append(np.nan)\n",
    "\n",
    "        # Reqierd credential\n",
    "        try:      reqierd_credentials  .append(job_data['educationRequirements']['credentialCategory'])\n",
    "        except:   reqierd_credentials  .append(np.nan)\n",
    "\n",
    "        # Employment type\n",
    "        try:      employment_types     .append(job_data['employmentType'].strip())\n",
    "        except:   employment_types     .append(np.nan)\n",
    "\n",
    "        # Location type\n",
    "        try:      location_types       .append(job_data['jobLocationType'])\n",
    "        except:   location_types       .append(np.nan)\n",
    "\n",
    "        # Industry\n",
    "        try:      industries           .append(job_data['industry'].strip())\n",
    "        except:   industries           .append(np.nan)\n",
    "\n",
    "    # Now we are going to convert list into tuples.\n",
    "    stacked_df['describtion']          [i] = tuple(describtions)\n",
    "    stacked_df['reqierd_credential']   [i] = tuple(reqierd_credentials)\n",
    "    stacked_df['location_type']        [i] = tuple(location_types)\n",
    "    stacked_df['employment_type']      [i] = tuple(employment_types)\n",
    "    stacked_df['industry']             [i] = tuple(industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee10424-575b-402a-bf45-c450f73b5da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>jobs_locations</th>\n",
       "      <th>listing_dates</th>\n",
       "      <th>jobs_titles</th>\n",
       "      <th>companies_names</th>\n",
       "      <th>soups</th>\n",
       "      <th>jobs_links</th>\n",
       "      <th>describtion</th>\n",
       "      <th>reqierd_credential</th>\n",
       "      <th>location_type</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>264000</td>\n",
       "      <td>(San Jose, CA, Belmont, CA, San Jose, CA, Stoc...</td>\n",
       "      <td>(nan, 2023-07-14, 2023-07-22, 2023-07-20, 2023...</td>\n",
       "      <td>(Data Entry Operator Junior / Part Time (Remot...</td>\n",
       "      <td>(Seekmate, Get.It Recruit - Administrative, Cu...</td>\n",
       "      <td>(https://www.linkedin.com/company/seekmate?trk...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-entry...</td>\n",
       "      <td>(This is a remote position.&amp;lt;p&amp;gt;&amp;lt;br&amp;gt;...</td>\n",
       "      <td>(high school, high school, high school, high s...</td>\n",
       "      <td>(TELECOMMUTE, TELECOMMUTE, TELECOMMUTE, TELECO...</td>\n",
       "      <td>(PART_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...</td>\n",
       "      <td>(Computer Networking Products, Human Resources...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>39000</td>\n",
       "      <td>(San Francisco, CA, San Francisco, CA, San Fra...</td>\n",
       "      <td>(2023-04-07, 2023-04-28, 2023-03-29, 2023-05-2...</td>\n",
       "      <td>(Data Engineer, Data Engineer, Data Engineer, ...</td>\n",
       "      <td>(Patreon, SEPHORA, Laguna Games, Applied Intui...</td>\n",
       "      <td>(https://www.linkedin.com/company/patreon?trk=...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-engin...</td>\n",
       "      <td>(Patreon is the best place for creators to bui...</td>\n",
       "      <td>(bachelor degree, bachelor degree, bachelor de...</td>\n",
       "      <td>(nan, nan, nan, nan, TELECOMMUTE, nan, TELECOM...</td>\n",
       "      <td>(FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...</td>\n",
       "      <td>(Technology, Information and Internet, Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>142000</td>\n",
       "      <td>(San Francisco, CA, Palo Alto, CA, Emeryville,...</td>\n",
       "      <td>(2023-06-16, 2023-07-10, 2023-05-01, 2023-07-0...</td>\n",
       "      <td>(Data Scientist, Data Scientist, Machine Learn...</td>\n",
       "      <td>(Experfy, Glean, Liminal, Qualia, Tavus, Softw...</td>\n",
       "      <td>(https://www.linkedin.com/company/experfy?trk=...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-scien...</td>\n",
       "      <td>(The successful candidate will have a strong b...</td>\n",
       "      <td>(bachelor degree, bachelor degree, bachelor de...</td>\n",
       "      <td>(nan, nan, nan, nan, TELECOMMUTE, nan, TELECOM...</td>\n",
       "      <td>(FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...</td>\n",
       "      <td>(Technology, Information and Internet, Technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>156000</td>\n",
       "      <td>(San Francisco, CA, Cupertino, CA, Cupertino, ...</td>\n",
       "      <td>(2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...</td>\n",
       "      <td>(Data Analyst, Data Analyst III, Data Analyst ...</td>\n",
       "      <td>(Digital Janet, WinMax, WinMax, ExaTech Inc, F...</td>\n",
       "      <td>(https://www.linkedin.com/company/digitaljanet...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-analy...</td>\n",
       "      <td>(&amp;lt;strong&amp;gt;Title: Data Analyst&amp;lt;br&amp;gt;&amp;l...</td>\n",
       "      <td>(bachelor degree, bachelor degree, bachelor de...</td>\n",
       "      <td>(nan, nan, nan, nan, nan, nan, nan, TELECOMMUT...</td>\n",
       "      <td>(FULL_TIME, CONTRACTOR, CONTRACTOR, CONTRACTOR...</td>\n",
       "      <td>(Staffing and Recruiting, Staffing and Recruit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>ML developer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>989</td>\n",
       "      <td>(San Francisco, CA, Mountain View, CA, San Fra...</td>\n",
       "      <td>(2023-06-23, 2023-07-12, 2023-06-05, 2023-07-2...</td>\n",
       "      <td>(Software Engineer - AI/ML, Machine Learning S...</td>\n",
       "      <td>(Descript, Enterprise Minds, Inc, Sieve, Auror...</td>\n",
       "      <td>(https://www.linkedin.com/company/descript?trk...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/software-e...</td>\n",
       "      <td>(Our vision is to build the next generation pl...</td>\n",
       "      <td>(bachelor degree, nan, bachelor degree, bachel...</td>\n",
       "      <td>(TELECOMMUTE, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>(FULL_TIME, nan, FULL_TIME, FULL_TIME, CONTRAC...</td>\n",
       "      <td>(Technology, Information and Internet, nan, So...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup       job_title  \\\n",
       "0  \\n<li>\\n<div class=\"base-card relative w-full ...      Data entry   \n",
       "1  \\n<li>\\n<div class=\"base-card relative w-full ...   Data engineer   \n",
       "2  \\n<li>\\n<div class=\"base-card relative w-full ...  Data scientist   \n",
       "3  \\n<li>\\n<div class=\"base-card relative w-full ...    Data analyst   \n",
       "4  \\n<li>\\n<div class=\"base-card relative w-full ...    ML developer   \n",
       "\n",
       "          country  platform  total_jobs  \\\n",
       "0  European Union  LinkedIn      264000   \n",
       "1  European Union  LinkedIn       39000   \n",
       "2  European Union  LinkedIn      142000   \n",
       "3  European Union  LinkedIn      156000   \n",
       "4  European Union  LinkedIn         989   \n",
       "\n",
       "                                      jobs_locations  \\\n",
       "0  (San Jose, CA, Belmont, CA, San Jose, CA, Stoc...   \n",
       "1  (San Francisco, CA, San Francisco, CA, San Fra...   \n",
       "2  (San Francisco, CA, Palo Alto, CA, Emeryville,...   \n",
       "3  (San Francisco, CA, Cupertino, CA, Cupertino, ...   \n",
       "4  (San Francisco, CA, Mountain View, CA, San Fra...   \n",
       "\n",
       "                                       listing_dates  \\\n",
       "0  (nan, 2023-07-14, 2023-07-22, 2023-07-20, 2023...   \n",
       "1  (2023-04-07, 2023-04-28, 2023-03-29, 2023-05-2...   \n",
       "2  (2023-06-16, 2023-07-10, 2023-05-01, 2023-07-0...   \n",
       "3  (2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...   \n",
       "4  (2023-06-23, 2023-07-12, 2023-06-05, 2023-07-2...   \n",
       "\n",
       "                                         jobs_titles  \\\n",
       "0  (Data Entry Operator Junior / Part Time (Remot...   \n",
       "1  (Data Engineer, Data Engineer, Data Engineer, ...   \n",
       "2  (Data Scientist, Data Scientist, Machine Learn...   \n",
       "3  (Data Analyst, Data Analyst III, Data Analyst ...   \n",
       "4  (Software Engineer - AI/ML, Machine Learning S...   \n",
       "\n",
       "                                     companies_names  \\\n",
       "0  (Seekmate, Get.It Recruit - Administrative, Cu...   \n",
       "1  (Patreon, SEPHORA, Laguna Games, Applied Intui...   \n",
       "2  (Experfy, Glean, Liminal, Qualia, Tavus, Softw...   \n",
       "3  (Digital Janet, WinMax, WinMax, ExaTech Inc, F...   \n",
       "4  (Descript, Enterprise Minds, Inc, Sieve, Auror...   \n",
       "\n",
       "                                               soups  \\\n",
       "0  (https://www.linkedin.com/company/seekmate?trk...   \n",
       "1  (https://www.linkedin.com/company/patreon?trk=...   \n",
       "2  (https://www.linkedin.com/company/experfy?trk=...   \n",
       "3  (https://www.linkedin.com/company/digitaljanet...   \n",
       "4  (https://www.linkedin.com/company/descript?trk...   \n",
       "\n",
       "                                          jobs_links  \\\n",
       "0  (https://www.linkedin.com/jobs/view/data-entry...   \n",
       "1  (https://www.linkedin.com/jobs/view/data-engin...   \n",
       "2  (https://www.linkedin.com/jobs/view/data-scien...   \n",
       "3  (https://www.linkedin.com/jobs/view/data-analy...   \n",
       "4  (https://www.linkedin.com/jobs/view/software-e...   \n",
       "\n",
       "                                         describtion  \\\n",
       "0  (This is a remote position.&lt;p&gt;&lt;br&gt;...   \n",
       "1  (Patreon is the best place for creators to bui...   \n",
       "2  (The successful candidate will have a strong b...   \n",
       "3  (&lt;strong&gt;Title: Data Analyst&lt;br&gt;&l...   \n",
       "4  (Our vision is to build the next generation pl...   \n",
       "\n",
       "                                  reqierd_credential  \\\n",
       "0  (high school, high school, high school, high s...   \n",
       "1  (bachelor degree, bachelor degree, bachelor de...   \n",
       "2  (bachelor degree, bachelor degree, bachelor de...   \n",
       "3  (bachelor degree, bachelor degree, bachelor de...   \n",
       "4  (bachelor degree, nan, bachelor degree, bachel...   \n",
       "\n",
       "                                       location_type  \\\n",
       "0  (TELECOMMUTE, TELECOMMUTE, TELECOMMUTE, TELECO...   \n",
       "1  (nan, nan, nan, nan, TELECOMMUTE, nan, TELECOM...   \n",
       "2  (nan, nan, nan, nan, TELECOMMUTE, nan, TELECOM...   \n",
       "3  (nan, nan, nan, nan, nan, nan, nan, TELECOMMUT...   \n",
       "4  (TELECOMMUTE, nan, nan, nan, nan, nan, nan, na...   \n",
       "\n",
       "                                     employment_type  \\\n",
       "0  (PART_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...   \n",
       "1  (FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...   \n",
       "2  (FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...   \n",
       "3  (FULL_TIME, CONTRACTOR, CONTRACTOR, CONTRACTOR...   \n",
       "4  (FULL_TIME, nan, FULL_TIME, FULL_TIME, CONTRAC...   \n",
       "\n",
       "                                            industry  \n",
       "0  (Computer Networking Products, Human Resources...  \n",
       "1  (Technology, Information and Internet, Persona...  \n",
       "2  (Technology, Information and Internet, Technol...  \n",
       "3  (Staffing and Recruiting, Staffing and Recruit...  \n",
       "4  (Technology, Information and Internet, nan, So...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653a6b6-5d38-44a8-b3ee-aadc5b900e40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style= 'color: #517afc'>Saving</span> the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "871a51ac-194d-4fbb-bf56-90a61c8a88d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linkedin_dfs: list = [] # We will group all rows as dfs to stack them later.\n",
    "\n",
    "for _, row in stacked_df.iterrows():\n",
    "    del _\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'jobs_locations'      : row['jobs_locations'],\n",
    "        'listing_dates'       : row['listing_dates'],\n",
    "        'jobs_titles'         : row['jobs_titles'],\n",
    "        'companies_names'     : row['companies_names'],\n",
    "        'jobs_links'          : row['jobs_links'],\n",
    "        'describtion'         : row['describtion'],\n",
    "        'location_type'       : row['location_type'],\n",
    "        'employment_type'     : row['employment_type'],\n",
    "        'industry'            : row['industry'],\n",
    "        'reqierd_credential'  : row['reqierd_credential']})\n",
    "\n",
    "    df['country']     = row['country']\n",
    "    df['job_title']   = row['job_title']\n",
    "    df['total_jobs']  = row['total_jobs'] # This column indicates total jobs grouped by country & job title.\n",
    "\n",
    "    linkedin_dfs.append(df)\n",
    "\n",
    "linckedin_df = pd.concat(linkedin_dfs, ignore_index= True, sort= False)\n",
    "linckedin_df.to_parquet(r'data/linckedin_jobs.parquet')\n",
    "linckedin_df.to_csv(r'data/linckedin_jobs.csv')\n",
    "\n",
    "encrypt.encrypt_json_file('../credentials.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
