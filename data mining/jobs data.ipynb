{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Importing <span style= 'color: #48e0dc'>Packeges</span>\n",
    "<sub>*And setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import scrapy\n",
    "import pickle\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fp.fp import FreeProxy\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "from urllib.request import urlopen\n",
    "from fake_useragent import UserAgent\n",
    "from scrapy.http import HtmlResponse\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from selenium.webdriver.common.by import By\n",
    "from scrapy.utils.log import configure_logging\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_9288\\1776924631.py:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "COLORS = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "MAX_PAGES = 300\n",
    "\n",
    "proxies = {'http': 'http://109.254.67.104:9090',\n",
    "           'http': 'http://103.49.202.252:80'}\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1209e763-b75d-4020-a8ba-db561004ec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_jobs_titles = ['Data entry', 'Data engineer',\n",
    "                    'Data scientist', 'Data analyst',\n",
    "                    'ML developer']\n",
    "\n",
    "indeed_countries = ['de', 'uk']#, 'www'] USA don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5cd3f7'> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_fast(url: str) -> BeautifulSoup:\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(delay= 10,browser= {\n",
    "        'browser': 'chrome',\n",
    "        'platform': 'windows',\n",
    "        'desktop': True,\n",
    "        'mobile': False}) \n",
    "    \n",
    "    content = scraper.get(url).text \n",
    "    return BeautifulSoup(content)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_page(url: str, retrieve_new_url= None, user_agent= None) -> BeautifulSoup:\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    \n",
    "    # chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  # Optional: Run Chrome in headless mode\n",
    "    # chrome_options.add_argument(f\"--user-agent={ua.random}\")\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "\n",
    "    html = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    if retrieve_new_url:\n",
    "        new_url = driver.current_url\n",
    "\n",
    "        driver.quit()\n",
    "        return html, new_url\n",
    "        \n",
    "    driver.quit()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05c858c-9b35-48c2-8a2e-f166081768f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 s Â± 778 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "The slowest run took 4.43 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "13.7 s Â± 9.63 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Comparing the two scrapers speed.\n",
    "%timeit scrape_page_fast('https://linkedin.com')\n",
    "%timeit scrape_page('https://linkedin.com')\n",
    "\n",
    "# We can find that there's about 10X speed for the `scrape_page_fast` ðŸ¤¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_scraper(page: int,\n",
    "                   country: str,\n",
    "                   job_title: str) -> BeautifulSoup:\n",
    "\n",
    "    ua           : str = UserAgent.random\n",
    "    modefid_page : int = (page - 1) * 10\n",
    "    headers      : dict = {\n",
    "        'User-Agent': ua,\n",
    "        'Accept-Encoding': '*',\n",
    "        'Connection': 'keep-alive'}\n",
    "        \n",
    "    url          : str = f'https://{country}.indeed.com/jobs?q={job_title}&start={modefid_page}'\n",
    "\n",
    "    soup = scrape_page(url)\n",
    "    loaded_page = int(soup.find('button', 'css-ns2mzi e8ju0x51').text)\n",
    "\n",
    "    # available_pages = []\n",
    "\n",
    "    # for button in soup.find_all('button', 'css-1qt7hdn e8ju0x50'):\n",
    "    #     available_pages.append(int(button.text))\n",
    "\n",
    "    if (int(page) != loaded_page):\n",
    "        # We will use KeyError as standard to represent getting out of the max pages.\n",
    "        raise KeyError\n",
    "        \n",
    "    print(f'Page {page} Loaded successfully.')\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(country:   str, # This scraper scrapes all pages at once\n",
    "                     job_title: str,\n",
    "                     page:      int) -> BeautifulSoup:\n",
    "    \n",
    "    ua = UserAgent.random\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': ua,\n",
    "        'Accept-Encoding': '*',\n",
    "        'Connection': 'keep-alive'}\n",
    "\n",
    "    page: int = (page - 1) * 25\n",
    "    url: str = f'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={job_title}&' + \\\n",
    "    f'location={country}&geoId=90000084&trk=public_jobs_j%20obs-search-bar_search-submit&position=1&pageNum=0&start={page}'\n",
    "\n",
    "    response = requests.get(url, headers= headers)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951bed7-9819-4466-8604-9660043e7744",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Collecting<span style= 'color: #4895e0'> jobs </span> data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452ce6e-7b8b-4a82-b3e5-e6911937dea4",
   "metadata": {},
   "source": [
    "**What we will do in this section:**\n",
    "\n",
    "1. Collect all pages we need from each scraper and stack them together as HTML code.\n",
    "2. Scrape for jobs cards links from linkedin and indeed.\n",
    "3. Scrape for the data in those links and store the data in a DataFrame\n",
    "4. We may also consider collecting total results per each platform, country etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3d2-25cd-4321-b091-d2a7914786f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Stack pages together as HTML code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bf4c2b-d50b-494d-b2e1-5f41012401a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_integer(number_string):\n",
    "\n",
    "    cleaned_string = ''.join(filter(str.isdigit, number_string))\n",
    "    cleaned_string = cleaned_string.rstrip('+')\n",
    "    integer_value = int(cleaned_string)\n",
    "\n",
    "    return integer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791acf9d-694a-417f-9015-31fceed0babc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LinkedIn scraper.\n",
      "Loading the European Union data.\n",
      "Finished loading Data entry jobs from the European Union.\n",
      "Finished loading Data engineer jobs from the European Union.\n",
      "Finished loading Data scientist jobs from the European Union.\n",
      "Finished loading Data analyst jobs from the European Union.\n",
      "Finished loading ML developer jobs from the European Union.\n",
      "\n",
      "\n",
      "Loading the United States data.\n",
      "Finished loading Data entry jobs from the United States.\n",
      "Finished loading Data engineer jobs from the United States.\n",
      "Finished loading Data scientist jobs from the United States.\n",
      "Finished loading Data analyst jobs from the United States.\n",
      "Finished loading ML developer jobs from the United States.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stacked_df: dict = {'soup':        [],\n",
    "                    'job_title':   [],\n",
    "                    'country':     [],\n",
    "                    'platform':    [],\n",
    "                    'total_jobs':  []}\n",
    "\n",
    "flag     : bool = False\n",
    "max_page : int = 4 # TOREMOVE\n",
    "\n",
    "# Linked In\n",
    "print('Starting LinkedIn scraper.')\n",
    "\n",
    "for country in linkedin_countries:\n",
    "\n",
    "    print(f'Loading the {country} data.')\n",
    "    for job_title in data_jobs_titles:\n",
    "        \n",
    "        full_soup   : str = ''\n",
    "        total_jobs  : list = convert_to_integer(scrape_page(f'https://www.linkedin.com/jobs/search?keywords={job_title}&location={country}&' + \n",
    "                                                            f'\\geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0').find(\n",
    "                                                                   'span', 'results-context-header__job-count').text)\n",
    "\n",
    "        for i in count(0):\n",
    "            page = i + 1\n",
    "            page_soup: BeautifulSoup = linkedin_scraper(country= country,\n",
    "                                                        job_title= job_title,\n",
    "                                                        page= i + page)\n",
    "            full_soup += str(page_soup)\n",
    "            full_soup += ' <br> '\n",
    "\n",
    "            if (page_soup.find_all('li') is None) or (page == max_page):\n",
    "                break\n",
    "\n",
    "        stacked_df['platform']      .append('LinkedIn')\n",
    "        stacked_df['total_jobs']    .append(total_jobs)\n",
    "        stacked_df['soup']          .append(full_soup)\n",
    "        stacked_df['job_title']     .append(job_title)\n",
    "        stacked_df['country']       .append(country)\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs from the {country}.')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ed47b4-02cd-49ea-9eb2-96e4e377b417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indeed scraping TODO\n",
    "# for country in indeed_countries:\n",
    "#     for job_title in data_jobs_titles:\n",
    "#         stacked_pages_soup: str = ''\n",
    "\n",
    "#         for i in count(0):\n",
    "\n",
    "#             try:\n",
    "#                 page = i + 1\n",
    "#                 soup: BeautifulSoup = indeed_scraper(country= country,\n",
    "#                                                      job_title= job_title,\n",
    "#                                                      page= page)\n",
    "\n",
    "#                 stacked_pages_soup += '\\n<br> ' + str(soup)\n",
    "#                 if page > max_page:\n",
    "#                     print('Finished loading current country or job title.\\n')\n",
    "#                     break\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 print('Finished loading current country or job title.\\n')\n",
    "#                 break\n",
    "\n",
    "#         stacked_df['platform'].append('Indeed')\n",
    "#         stacked_df['soup'].append(stacked_pages_soup)\n",
    "#         stacked_df['job_title'].append(job_title)\n",
    "#         stacked_df['country'].append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5dc7-509a-490c-bd59-b341487cb5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Collecting jobs & companies basic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6983a18a-d68e-47f3-b27f-a48420b30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_df = pd.DataFrame(stacked_df)\n",
    "\n",
    "stacked_df['jobs_locations']  = None\n",
    "stacked_df['listing_dates']   = None\n",
    "stacked_df['jobs_titles']     = None\n",
    "stacked_df['companies_names'] = None\n",
    "\n",
    "stacked_df['soups']           = None\n",
    "stacked_df['jobs_links']      = None\n",
    "\n",
    "\n",
    "# Collecting linkedIn jobs links\n",
    "for i, soup in enumerate(stacked_df[stacked_df['platform'] == 'LinkedIn']['soup']):\n",
    "\n",
    "    soup = BeautifulSoup(soup)\n",
    "    jobs = soup.find_all('div', \n",
    "           'base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "\n",
    "    linkedin_jobs_locations       = []\n",
    "    linkedin_jobs_listing_dates   = []\n",
    "    linkedin_jobs_titles          = []\n",
    "    linkedin_jobs_companies_names = []\n",
    "    linkedin_jobs_links           = []\n",
    "    linkedin_companies_links      = []\n",
    "    \n",
    "    for job in jobs:\n",
    "\n",
    "        try: linkedin_jobs_listing_dates               .append(job.find('time', 'job-search-card__listdate')['datetime'])\n",
    "        except TypeError:  linkedin_jobs_listing_dates .append(np.nan)\n",
    "        \n",
    "        linkedin_jobs_locations            .append(job.find('span', 'job-search-card__location').text.strip())\n",
    "        linkedin_jobs_titles               .append(job.find('h3', 'base-search-card__title').text.strip())\n",
    "        linkedin_jobs_companies_names      .append(job.find('a', 'hidden-nested-link').text.strip())\n",
    "        linkedin_companies_links           .append(job.find('a', 'hidden-nested-link')['href'])\n",
    "        linkedin_jobs_links                .append(job.find\n",
    "                                                   ('a', 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')['href'])\n",
    "\n",
    "    # I will use sets and tuples instead of lists because they are faster.\n",
    "    stacked_df['jobs_locations']    [i] = tuple(linkedin_jobs_locations)\n",
    "    stacked_df['listing_dates']     [i] = tuple(linkedin_jobs_listing_dates)\n",
    "    stacked_df['jobs_titles']       [i] = tuple(linkedin_jobs_titles)\n",
    "    stacked_df['companies_names']   [i] = tuple(linkedin_jobs_companies_names)\n",
    "\n",
    "    stacked_df['jobs_links']        [i] = tuple(linkedin_jobs_links)\n",
    "    stacked_df['soups']             [i] = tuple(linkedin_companies_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a112b4b4-9ceb-49c7-a66c-e3d3da3990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting indeed jobs links TOFIX\n",
    "# for i, soup in enumerate(stacked_df[stacked_df['platform'] == 'Indeed']['soup']):\n",
    "    \n",
    "#     indeed_jobs_links: list = []\n",
    "#     indeed_companies_links: list = []\n",
    "\n",
    "#     for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "#         indeed_jobs_links.append(job_link)\n",
    "\n",
    "#     for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "#         company_link = BeautifulSoup(scrape_page(job_link)).find('a', 'css-775knl emf9s7v0')\n",
    "#         indeed_companies_links.append(company_link)\n",
    "#         print(company_link)\n",
    "        \n",
    "#     stacked_df['total_jobs'][i] = len(indeed_jobs_links)\n",
    "#     stacked_df['jobs_links'][i] = indeed_jobs_links\n",
    "#     stacked_df['soups'][i] = indeed_companies_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6883647-4384-4b32-b25b-672a6d704aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Loading more jobs data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b598359-33f4-4698-893b-aafc3fb1fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting more LinkedIn data\n",
    "\n",
    "# We will collect the: \n",
    "#    describtion, credential category, location type\n",
    "\n",
    "stacked_df['describtion']         = None\n",
    "stacked_df['reqierd_credential']  = None\n",
    "stacked_df['location_type']       = None\n",
    "stacked_df['employment_type']     = None\n",
    "stacked_df['industry']            = None\n",
    "\n",
    "for i, jobs in enumerate(stacked_df['jobs_links'][:2]): # TOREMOVE remove the loop limit\n",
    "\n",
    "    describtions:            list = []\n",
    "    reqierd_credentials:     list = []\n",
    "    location_types:          list = []\n",
    "    employment_types:        list = []\n",
    "    industries:              list = []\n",
    "    \n",
    "    for job in jobs:\n",
    "\n",
    "        try:\n",
    "            job_data:  dict = json.loads(scrape_page_fast(job).find(\n",
    "                'script', attrs= {'type': 'application/ld+json'}).text)\n",
    "        except:\n",
    "            job_data = np.nan\n",
    "\n",
    "        # Describtion\n",
    "        try:      describtions         .append(job_data['description'].strip())\n",
    "        except:   describtions         .append(np.nan)\n",
    "\n",
    "        # Reqierd credential\n",
    "        try:      reqierd_credentials  .append(job_data['educationRequirements']['credentialCategory'])\n",
    "        except:   reqierd_credentials  .append(np.nan)\n",
    "\n",
    "        # Employment type\n",
    "        try:      employment_types     .append(job_data['employmentType'].strip())\n",
    "        except:   employment_types     .append(np.nan)\n",
    "\n",
    "        # Location type\n",
    "        try:      location_types       .append(job_data['jobLocationType'])\n",
    "        except:   location_types       .append(np.nan)\n",
    "\n",
    "        # Industry\n",
    "        try:      industries           .append(job_data['industry'].strip())\n",
    "        except:   industries           .append(np.nan)\n",
    "\n",
    "    # Now we are going to convert list into tuples.\n",
    "    stacked_df['describtion']          [i] = tuple(describtions)\n",
    "    stacked_df['reqierd_credential']   [i] = tuple(reqierd_credentials)\n",
    "    stacked_df['location_type']        [i] = tuple(location_types)\n",
    "    stacked_df['employment_type']      [i] = tuple(employment_types)\n",
    "    stacked_df['industry']             [i] = tuple(industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eee10424-575b-402a-bf45-c450f73b5da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>jobs_locations</th>\n",
       "      <th>listing_dates</th>\n",
       "      <th>jobs_titles</th>\n",
       "      <th>companies_names</th>\n",
       "      <th>jobs_links</th>\n",
       "      <th>describtion</th>\n",
       "      <th>location_type</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "      <th>reqierd_credential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>310000</td>\n",
       "      <td>(Belmont, CA, Freedom, CA, Stockton, CA, Lathr...</td>\n",
       "      <td>(2023-07-14, 2023-07-14, nan, 2023-07-14, 2023...</td>\n",
       "      <td>(Data Entry Clerk (Typist) - Remote | WFH, Ent...</td>\n",
       "      <td>(Get.It Recruit - Administrative, Get.It Recru...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-entry...</td>\n",
       "      <td>(Are you searching for a flexible and rewardin...</td>\n",
       "      <td>(TELECOMMUTE, TELECOMMUTE, TELECOMMUTE, TELECO...</td>\n",
       "      <td>(FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, P...</td>\n",
       "      <td>(Human Resources Services, Human Resources Ser...</td>\n",
       "      <td>(high school, high school, high school, nan, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>39000</td>\n",
       "      <td>(San Francisco, CA, San Francisco, CA, San Mat...</td>\n",
       "      <td>(2023-04-07, 2023-04-28, 2023-05-28, 2023-03-2...</td>\n",
       "      <td>(Data Engineer, Data Engineer, Data Engineer (...</td>\n",
       "      <td>(Patreon, SEPHORA, Booster, Laguna Games, Appl...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-engin...</td>\n",
       "      <td>(Patreon is the best place for creators to bui...</td>\n",
       "      <td>(nan, nan, TELECOMMUTE, nan, nan, TELECOMMUTE,...</td>\n",
       "      <td>(FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...</td>\n",
       "      <td>(Technology, Information and Internet, Persona...</td>\n",
       "      <td>(bachelor degree, bachelor degree, bachelor de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data scientist</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>148000</td>\n",
       "      <td>(San Francisco, CA, Palo Alto, CA, Emeryville,...</td>\n",
       "      <td>(2023-06-16, 2023-07-10, 2023-05-01, 2023-07-0...</td>\n",
       "      <td>(Data Scientist, Data Scientist, Machine Learn...</td>\n",
       "      <td>(Experfy, Glean, Liminal, Qualia, Tavus, Glass...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-scien...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data analyst</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>162000</td>\n",
       "      <td>(San Francisco, CA, Cupertino, CA, Cupertino, ...</td>\n",
       "      <td>(2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...</td>\n",
       "      <td>(Data Analyst, Data Analyst III, Data Analyst ...</td>\n",
       "      <td>(Digital Janet, WinMax, WinMax, ExaTech Inc, F...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-analy...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ML developer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>995</td>\n",
       "      <td>(San Francisco, CA, Mountain View, CA, San Fra...</td>\n",
       "      <td>(2023-06-23, 2023-07-12, 2023-06-05, 2023-06-2...</td>\n",
       "      <td>(Software Engineer - AI/ML, Machine Learning S...</td>\n",
       "      <td>(Descript, Enterprise Minds, Inc, Sieve, Auror...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/software-e...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        job_title         country  platform  total_jobs  \\\n",
       "0      Data entry  European Union  LinkedIn      310000   \n",
       "1   Data engineer  European Union  LinkedIn       39000   \n",
       "2  Data scientist  European Union  LinkedIn      148000   \n",
       "3    Data analyst  European Union  LinkedIn      162000   \n",
       "4    ML developer  European Union  LinkedIn         995   \n",
       "\n",
       "                                      jobs_locations  \\\n",
       "0  (Belmont, CA, Freedom, CA, Stockton, CA, Lathr...   \n",
       "1  (San Francisco, CA, San Francisco, CA, San Mat...   \n",
       "2  (San Francisco, CA, Palo Alto, CA, Emeryville,...   \n",
       "3  (San Francisco, CA, Cupertino, CA, Cupertino, ...   \n",
       "4  (San Francisco, CA, Mountain View, CA, San Fra...   \n",
       "\n",
       "                                       listing_dates  \\\n",
       "0  (2023-07-14, 2023-07-14, nan, 2023-07-14, 2023...   \n",
       "1  (2023-04-07, 2023-04-28, 2023-05-28, 2023-03-2...   \n",
       "2  (2023-06-16, 2023-07-10, 2023-05-01, 2023-07-0...   \n",
       "3  (2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...   \n",
       "4  (2023-06-23, 2023-07-12, 2023-06-05, 2023-06-2...   \n",
       "\n",
       "                                         jobs_titles  \\\n",
       "0  (Data Entry Clerk (Typist) - Remote | WFH, Ent...   \n",
       "1  (Data Engineer, Data Engineer, Data Engineer (...   \n",
       "2  (Data Scientist, Data Scientist, Machine Learn...   \n",
       "3  (Data Analyst, Data Analyst III, Data Analyst ...   \n",
       "4  (Software Engineer - AI/ML, Machine Learning S...   \n",
       "\n",
       "                                     companies_names  \\\n",
       "0  (Get.It Recruit - Administrative, Get.It Recru...   \n",
       "1  (Patreon, SEPHORA, Booster, Laguna Games, Appl...   \n",
       "2  (Experfy, Glean, Liminal, Qualia, Tavus, Glass...   \n",
       "3  (Digital Janet, WinMax, WinMax, ExaTech Inc, F...   \n",
       "4  (Descript, Enterprise Minds, Inc, Sieve, Auror...   \n",
       "\n",
       "                                          jobs_links  \\\n",
       "0  (https://www.linkedin.com/jobs/view/data-entry...   \n",
       "1  (https://www.linkedin.com/jobs/view/data-engin...   \n",
       "2  (https://www.linkedin.com/jobs/view/data-scien...   \n",
       "3  (https://www.linkedin.com/jobs/view/data-analy...   \n",
       "4  (https://www.linkedin.com/jobs/view/software-e...   \n",
       "\n",
       "                                         describtion  \\\n",
       "0  (Are you searching for a flexible and rewardin...   \n",
       "1  (Patreon is the best place for creators to bui...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                       location_type  \\\n",
       "0  (TELECOMMUTE, TELECOMMUTE, TELECOMMUTE, TELECO...   \n",
       "1  (nan, nan, TELECOMMUTE, nan, nan, TELECOMMUTE,...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                     employment_type  \\\n",
       "0  (FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, P...   \n",
       "1  (FULL_TIME, FULL_TIME, FULL_TIME, FULL_TIME, F...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            industry  \\\n",
       "0  (Human Resources Services, Human Resources Ser...   \n",
       "1  (Technology, Information and Internet, Persona...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                  reqierd_credential  \n",
       "0  (high school, high school, high school, nan, b...  \n",
       "1  (bachelor degree, bachelor degree, bachelor de...  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f91752dd-2834-4e33-8351-3b3e5f7df845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping use less columns\n",
    "# stacked_df.drop(['soups', 'soup', 'jobs_links'], inplace= True, axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653a6b6-5d38-44a8-b3ee-aadc5b900e40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style= 'color: #517afc'>Saving</span> the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "871a51ac-194d-4fbb-bf56-90a61c8a88d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     linkedin_dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     24\u001b[0m linckedin_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(linkedin_dfs, ignore_index\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mlinckedin_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/linckedin_jobs.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m linckedin_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/linckedin_jobs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2891\u001b[0m     path,\n\u001b[0;32m   2892\u001b[0m     engine,\n\u001b[0;32m   2893\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   2894\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   2895\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   2896\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2897\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2898\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    409\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 411\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    412\u001b[0m     df,\n\u001b[0;32m    413\u001b[0m     path_or_buf,\n\u001b[0;32m    414\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    415\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    416\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    417\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    419\u001b[0m )\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parquet.py:161\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m    159\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pandas_kwargs)\n\u001b[1;32m--> 161\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[0;32m    172\u001b[0m ):\n\u001b[0;32m    173\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m path_or_handle\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parquet.py:110\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 737\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    741\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    598\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data'"
     ]
    }
   ],
   "source": [
    "linkedin_dfs: list = [] # We will group all rows as dfs to stack them later.\n",
    "\n",
    "for _, row in stacked_df[:2].iterrows():\n",
    "    del _\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'jobs_locations'      : row['jobs_locations'],\n",
    "        'listing_dates'       : row['listing_dates'],\n",
    "        'jobs_titles'         : row['jobs_titles'],\n",
    "        'companies_names'     : row['companies_names'],\n",
    "        'jobs_links'          : row['jobs_links'],\n",
    "        'describtion'         : row['describtion'],\n",
    "        'location_type'       : row['location_type'],\n",
    "        'employment_type'     : row['employment_type'],\n",
    "        'industry'            : row['industry'],\n",
    "        'reqierd_credential'  : row['reqierd_credential']})\n",
    "\n",
    "    df['country']     = row['country']\n",
    "    df['job_title']   = row['job_title']\n",
    "    df['total_jobs']  = row['total_jobs'] # This column indicates total jobs grouped by country & job title.\n",
    "\n",
    "    linkedin_dfs.append(df)\n",
    "\n",
    "linckedin_df = pd.concat(linkedin_dfs, ignore_index= True, sort= False)\n",
    "linckedin_df.to_parquet(r'data/linckedin_jobs.parquet')\n",
    "linckedin_df.to_csv(r'data/linckedin_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d40c5-f3ed-41bd-b444-85cd7ddc46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('myfile.parquet')z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
