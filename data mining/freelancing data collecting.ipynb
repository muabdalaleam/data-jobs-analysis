{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d87eb4b-c943-4562-866b-7c2dd151f3c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong>Notebook<span style= \"color: #51FCC6\"> Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753d54c-1fd0-441c-b663-98ca78298128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59118b7-50cf-4738-8e35-cd9de6b6e46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b08419ad-46bc-42a6-a678-356c38c435fe",
   "metadata": {},
   "source": [
    "## <center><strong>Importing<span style= \"color: #48E0DC\"> Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfca499a-0b6e-42a6-9901-a25b403a1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.common.by import By\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b8e77-3b20-4c92-8619-434de6cedf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_3640\\2138414856.py:8: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "COLORS = [\"#51fcc6\", \"#48e0dc\", \"#5cd3f7\", \"#4895e0\", \"#517afc\"]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3e9fc-3746-4f76-be83-00f2b453be34",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= \"color: #5CD3F7\"> Web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd35b8e-8c12-4e99-8819-4c6e338c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_titles = [\"data-entry\", \"data-processing\", \"data-engineering\",\n",
    "                    \"data-science\", \"data-analytics\", \"data-visualization\"]\n",
    "\n",
    "upwork_types = [\"profiles\", \"jobs\", \"services\"]\n",
    "freelancer_types = [\"users\", \"projects\"]\n",
    "peapleperhour_types = [\"hire-freelancers\", \"freelance-jobs\", \"services\"]\n",
    "guru_service_types = [\"freelancers\", \"jobs\"]\n",
    "\n",
    "\n",
    "def scrape_page(url: str) -> BeautifulSoup:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    # Wait until the page is fully loaded\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    \n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16ff1bdf-99da-46f6-ab1e-b41c542daaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `type` param indicates what is the nature of the job is it hiring freelancers \n",
    "# or paying services or othr thing based on the platform.\n",
    "\n",
    "def upwork_scraper(page, job_title, type):\n",
    "\n",
    "    global upwork_url\n",
    "\n",
    "    if  type == 'services':\n",
    "\n",
    "        upwork_url =  f\"https://www.upwork.com/services/search?nbs=1&q={job_title}\"\n",
    "\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(upwork_url)\n",
    "\n",
    "        if page > 1:\n",
    "            for i in ragne(page):\n",
    "                \n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.up-btn.up-btn-default.mx-0.load-more-btn.nuxt-link-active')))\n",
    "                element.click()\n",
    "                \n",
    "                time.sleep(1)\n",
    "\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        \n",
    "        return  html\n",
    "\n",
    "    elif type == \"profiles\" or type == \"jobs\":\n",
    "        \n",
    "        upwork_url = f\"https://www.upwork.com/search/{type}/?page={page}&q={job_title}&user_pref=1\"\n",
    "        \n",
    "        return scrape_page(upwork_url)\n",
    "\n",
    "\n",
    "def guru_scraper(page, job_title, type):\n",
    "\n",
    "    global guru_url\n",
    "    guru_url = f\"https://www.guru.com/d/{type}/skill/{job_title}/pg/{page}/\"\n",
    "\n",
    "    return scrape_page(guru_url)\n",
    "\n",
    "\n",
    "def fivver_scraper(page, job_title):\n",
    "\n",
    "    global fivver_url\n",
    "    fivver_url = f\"https://www.fiverr.com/categories/data/{job_title}?source=pagination&page={page}\"\n",
    "\n",
    "    return scrape_page(fivver_url)\n",
    "\n",
    "\n",
    "def peopleperhour_scraper(page, job_title, type):\n",
    "\n",
    "    global peopleperhour_url\n",
    "\n",
    "    if type == \"hire-freelancers\":\n",
    "        peopleperhour_url = f\"https://www.peopleperhour.com/hire-freelancers/{job_title.replace('-', '+')}?page={page}&ref=search\"\n",
    "\n",
    "    else:\n",
    "        peopleperhour_url = f\"https://www.peopleperhour.com/{type}-{job_title.replace('-', '+')}\"\n",
    "\n",
    "    return  scrape_page(peopleperhour_url)\n",
    "\n",
    "# We will not analyze requested jobs data in freelancer.com becuase it's search feature \n",
    "# isn't accurate with the it.\n",
    "\n",
    "def freelancer_scraper(page, job_title, type):\n",
    "\n",
    "    global freelancer_url\n",
    "    freelancer_url = f\"https://www.freelancer.com/search/{type}?q={job_title.replace('-', '%20')}&page={page}\"\n",
    "    \n",
    "    return scrape_page(freelancer_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788705f-2f61-4a5c-ad84-ea24f56bccdb",
   "metadata": {},
   "source": [
    "Now we will try to calculate how much did each scraper take to collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0141299-f023-4a63-8e8b-52320c0b3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_variable_name = lambda var: [name for name in globals() if globals()[name] is var][0]\n",
    "\n",
    "def get_runtime(func, *args, **kwargs):\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    output = func(*args, **kwargs)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    run_time = t2 - t1\n",
    "\n",
    "    print(\"{} took about {:.2f}s to run.\\n\".format(\n",
    "        get_variable_name(func).replace(\"_\", \" \").title(), run_time))\n",
    "\n",
    "    return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619ad86-0c3f-4288-a5cb-8b3591513b88",
   "metadata": {},
   "source": [
    "Now we will just try to scrape **only one page** from each scraper *(temporary action)* to just test the data extractors with<br>\n",
    "more efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cfd62-4513-4cd0-bea7-40c4638debc0",
   "metadata": {},
   "source": [
    "## <center><strong>Extracting jobs <span style = \"color: #4895e0\"> Links</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b219d160-162a-4860-9116-60302878d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soups = {\"soup_name\": [],\n",
    "         \"soup_job_type\": [],\n",
    "         \"soup_job_title\": [],\n",
    "         \"soup_page\":  [],\n",
    "         \"soup\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d79a77b-f579-4750-984a-599fa9df13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the job type is services in upwork the pages paremeter will refer to how many pages \n",
    "# will render at once while the other scrapers will render the spesfic spesfied page only.\n",
    "\n",
    "def get_all_pages_soups(pph_type, freelancer_type, upwork_type, guru_type, max_page: int= 999) -> dict:\n",
    "\n",
    "    print(\"Strating ...\")\n",
    "    for i in count(0):\n",
    "\n",
    "        global orgnaize_data_in_dict\n",
    "        page = i + 1\n",
    "\n",
    "        print(f\"Loading page {page} of each website ...\")\n",
    "\n",
    "        for data_job_title in data_jobs_titles:\n",
    "\n",
    "            lines = [f'peopleperhour_html = peopleperhour_scraper({page}, {data_job_title}, {pph_type})',\n",
    "                     f'fivver_html = fivver_scraper({page}, {data_job_title})',\n",
    "                     f'freelancer_html = freelancer_scraper({page}, {data_job_title}, {freelancer_type})',\n",
    "                     f'upwork_html = upwork_scraper({page}, {data_job_title}, {upwork_type})',\n",
    "                     f'guru_html = guru_scraper({page}, {guru_data_job_title}, {guru_type})']\n",
    "\n",
    "            for line_idx, line in enumerate(lines):\n",
    "\n",
    "                equall_sign_idx = line.find('=')\n",
    "\n",
    "                def orgnaize_data_in_dict():\n",
    "                    soups[\"soup_job_title\"].append(data_job_title)\n",
    "                    soups[\"soup_name\"].append(line[:equall_sign_idx])\n",
    "                    soups[\"soup_page\"].append(page)\n",
    "                    soups[\"soup\"].append(exec(line[:equall_sign_idx]))\n",
    "                    soups[\"soup_job_type\"].append(job_type)\n",
    "\n",
    "                try:\n",
    "                    exec(line)\n",
    "                    orgnaize_data_in_dict()\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    del lines[line_idx]\n",
    "                    if len(lines) < 1 or page >= max_page:\n",
    "\n",
    "                        print(\"Finished ...\")\n",
    "                        return  soups\n",
    "                        break\n",
    "\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85626097-1809-40b9-a17c-ff3538cb1f7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "max_length = max(len(lst) for lst in soups.values())\n",
    "soups = {key: lst + [float('nan')] * (max_length - len(lst)) for key, lst in soups.items()}\n",
    "\n",
    "df = pd.DataFrame(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7ce831-bd81-4a23-b771-30457a6a1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the title of each page to check if it's loaded correctly.\n",
      "\n",
      "-\t Freelance Data-analytics Jobs - Upwork\n",
      "-\t Data Analytics Services Online | Fiverr\n",
      "-\t Machine Learning Freelance Jobs Online - Guru\n",
      "-\t Get Data Analytics Offers & Services | PeoplePerHour\n",
      "-\t Users Jobs, Employment | Freelancer\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the title of each page to check if it's loaded correctly.\\n\")\n",
    "\n",
    "for soup in soups[\"soup\"]:\n",
    "    print(\"-\\t\", soup.title.text.replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "\n",
    "soups = pd.DataFrame(soups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db860af4-9005-4f07-8bfa-cab4cdb3aa87",
   "metadata": {},
   "source": [
    "Looks like everything is working as expected now let's start reel work by extracting the HTML tags data from the scraped<br>\n",
    "pages.<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac5af7-1f3f-44a7-b93b-778e70096067",
   "metadata": {},
   "source": [
    "But we will need to collect the data from each one of those pages with unique way further more we need to optimize the<br>\n",
    "data extractor to collect the data from other pages from the same site like \"jobs\" site and \"profiles\" sites on upwork.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dfb3b7e-ed6c-49db-a27b-05e963d689ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards_links(soup_name, tag, link_template,\n",
    "                    link_attr, tag_class, attr= \"class\") -> list:\n",
    "\n",
    "    soup = soups[soups[\"soup_name\"] == soup_name][\"soup\"].values[0]\n",
    "\n",
    "    cards = soup.find_all(tag, attrs= {attr: tag_class})\n",
    "    cards_ids = []\n",
    "\n",
    "    if link_attr != \"\":\n",
    "\n",
    "        for card in cards:\n",
    "\n",
    "            card_id = link_template.replace(\"###\", card[link_attr])\n",
    "\n",
    "            if len(card_id) > len(link_template):\n",
    "                cards_ids.append(card_id)\n",
    "\n",
    "        return  cards_ids\n",
    "\n",
    "    else:\n",
    "        return  cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "710987e9-6eb6-47cd-a8ec-8d1777445501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't collect guru data this wat becuase it has slightly deffrint case in terms\n",
    "# of collecting the jobs URLs\n",
    "\n",
    "## --------------------------Upwork--------------------------\n",
    "upwork_gigs_links = get_cards_links('upwork_soup', 'a', 'https://upwork.com###', 'href',\n",
    "                                        'up-n-link project-tile up-card m-0 p-0 up-card-section up-card-hover project-tile_flexible')\n",
    "\n",
    "upwork_profiles_links = get_cards_links('upwork_soup', 'div', 'https://www.upwork.com/search/profiles/?profile=###', 'data-test-key',\n",
    "                                         'up-card-section up-card-hover')\n",
    "\n",
    "upwork_jobs_parents = get_cards_links('upwork_soup', 'h3', 'https://www.upwork.com###', '', 'my-0 p-sm-right job-tile-title')\n",
    "upwork_jobs_links = []\n",
    "\n",
    "for parent in upwork_jobs_parents:\n",
    "    upwork_jobs_links.append(\"https://www.upwork.com\" + parent.find('a', class_= \"up-n-link\")['href'])\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## --------------------------Fivver--------------------------\n",
    "fivver_gigs_links = get_cards_links('fivver_soup', 'a', 'https://www.fiverr.com###', 'href', 'tbody-5 p-t-4 YLycza2 u9KHmsf')\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## ---------------------People per hour----------------------\n",
    "pph_services_links = get_cards_links(\"peopleperhour_soup\", \"a\", \"https://www.peopleperhour.com###\", \"href\", \n",
    "                                     \"card__title-wrapper⤍FreelancerCard⤚2-NWk\")\n",
    "\n",
    "pph_freelancers_links = get_cards_links(\"peopleperhour_soup\", \"a\", \"https://www.peopleperhour.com###\", \"href\",\n",
    "                                        \"card__title-wrapper⤍FreelancerCard⤚2-NWk\")\n",
    "\n",
    "pph_jobs_links = get_cards_links(\"peopleperhour_soup\", \"a\", \"https://www.peopleperhour.com###\", \"href\",\n",
    "                                 \"item__url⤍ListItem⤚20ULx\")\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## ------------------------Guru------------------------------\n",
    "guru_cards_parents_profiles = get_cards_links(\"guru_soup\", \"h2\", \"https://guru.com###\", \"\",\n",
    "                                              \"jobRecord__title jobRecord__title--changeVisited\")\n",
    "\n",
    "guru_cards_parents_jobs = get_cards_links(\"guru_soup\", \"h2\", \"https://guru.com###\", \"\",\n",
    "                                          \"jobRecord__title jobRecord__title--changeVisited\")\n",
    "## ----------------------------------------------------------\n",
    "\n",
    "\n",
    "## ----------------------Freelancer---------------------------\n",
    "freelancer_jobs_links = get_cards_links(\"freelancer_soup\", \"a\", \"https://www.freelancer.com###\", \"href\",\n",
    "                                        \"LinkElement ng-star-inserted\")\n",
    "\n",
    "freelancer_freelancers_links = get_cards_links(\"freelancer_soup\", \"a\", \"https://www.freelancer.com###\", \"href\",\n",
    "                                              \"LinkElement ng-star-inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04a0cdb0-c6ac-426d-a893-ce8897ccbc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freelancer_freelancers_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1caa82-9049-40af-9248-5a6dcf4d404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upwork_links(job_type: str) -> list:  \n",
    "\n",
    "    card_tag = \"a\"\n",
    "    link_template = \"https://www.upwork.com###\"\n",
    "    link_attr = \"href\"\n",
    "\n",
    "    if job_type == \"profiles\":\n",
    "\n",
    "        card_tag = \"div\"\n",
    "        card_tag_class = \"up-card-section up-card-hover\"\n",
    "        link_template = \"https://www.upwork.com/search/profiles/?q=profiles&profile=###\"\n",
    "        link_attr = \"data-test-key\"\n",
    "    \n",
    "    elif job_type == \"jobs\":\n",
    "        card_tag_class = \"up-n-link\"\n",
    "\n",
    "\n",
    "    elif job_type == \"services\":\n",
    "        card_tag_class = \"up-n-link project-tile up-card m-0 p-0 up-card-section up-card-hover project-tile_flexible\"\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Please input correct work type from ['profiles', 'jobs', 'services'].\")\n",
    "\n",
    "\n",
    "    soup = soups[soups[\"soup_name\"] == \"upwork_soup\"][\"soup\"][0]\n",
    "\n",
    "    cards = soup.find_all(card_tag, class_= card_tag_class)\n",
    "    cards_ids = []\n",
    "\n",
    "    for card in cards:\n",
    "        \n",
    "        card_id = link_template.replace(\"###\", card[link_attr])\n",
    "        \n",
    "        if len(card_id) > len(link_template):\n",
    "            cards_ids.append(card_id)\n",
    "\n",
    "    return  cards_ids\n",
    "\n",
    "\n",
    "def get_fivver_links() -> list:\n",
    "\n",
    "    card_tag = \"div\"\n",
    "    card_tag_class = \"basic-gig-card\"\n",
    "    link_template = \"https://www.fiverr.com###\"\n",
    "    link_attr = \"href\"\n",
    "\n",
    "    cards = soup.find_all(card_tag, class_= card_tag_class)\n",
    "    cards_ids = []\n",
    "    \n",
    "    for card in cards:\n",
    "        \n",
    "        card_id = link_template.replace(\"###\", card[link_attr])\n",
    "        \n",
    "        if len(card_id) > len(link_template):\n",
    "            cards_ids.append(card_id)\n",
    "\n",
    "    return  cards_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8189c7-4417-4b58-82f6-e1b634d4aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guru_links(job_type: str) -> list:  \n",
    "\n",
    "    soup = soups[soups[\"soup_name\"] == \"guru_soup\"][\"soup\"][2]\n",
    "    link_template = \"https://www.guru.com###\"\n",
    "\n",
    "    if job_type == \"freelancers\":\n",
    "        cards_parents = soup.find_all(\n",
    "            'h2', class_= 'serviceListing__title serviceListing__title--dark')\n",
    "\n",
    "        cards = []\n",
    "\n",
    "        for parent in cards_parents:\n",
    "            card = parent.find('a')\n",
    "            cards.append(card)\n",
    "\n",
    "    elif job_type == \"jobs\":\n",
    "        cards_parents = soup.find_all(\n",
    "            'h2', class_ = 'jobRecord__title jobRecord__title--changeVisited')\n",
    "\n",
    "        cards = []\n",
    "\n",
    "        for parent in cards_parents:\n",
    "            card = parent.find('a')\n",
    "            cards.append(card)\n",
    "\n",
    "    cards_ids = []\n",
    "\n",
    "    for card in cards:\n",
    "        card_id = link_template.replace(\"###\", card['href'])\n",
    "        cards_ids.append(card_id)\n",
    "\n",
    "    return  cards_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e58f11-9b8e-4ee0-bf80-8405af334fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_guru_links('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd419a-7ce6-4b9f-a30b-43b66e59de47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
