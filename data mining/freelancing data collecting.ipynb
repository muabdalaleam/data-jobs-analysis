{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d87eb4b-c943-4562-866b-7c2dd151f3c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <center><strong>Notebook<span style= 'color: #51FCC6'> Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753d54c-1fd0-441c-b663-98ca78298128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59118b7-50cf-4738-8e35-cd9de6b6e46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b08419ad-46bc-42a6-a678-356c38c435fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Importing<span style= 'color: #48E0DC'> Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfca499a-0b6e-42a6-9901-a25b403a1dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import inspect\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import cloudscraper\n",
    "import numpy                         as np\n",
    "import pandas                        as pd\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "\n",
    "from itertools                       import count\n",
    "from bs4                             import BeautifulSoup\n",
    "from selenium                        import webdriver\n",
    "from selenium.webdriver.common.by    import By\n",
    "from selenium.webdriver.common.keys  import Keys\n",
    "from fake_useragent                  import UserAgent\n",
    "from IPython.display                 import set_matplotlib_formats\n",
    "from selenium.webdriver.support.ui   import WebDriverWait\n",
    "from selenium.webdriver.support      import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b8e77-3b20-4c92-8619-434de6cedf05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "MAX_PAGES        :int   = 20\n",
    "COLORS           :list  = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "NUMERICS         :list  = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "                            'uint16', 'uint32', 'uint64']\n",
    "\n",
    "sys.path.insert(1, '../my_encrypter')\n",
    "import encrypt\n",
    "\n",
    "encrypt.decrypt_json_file('../credentials.json')\n",
    "\n",
    "data_jobs_titles :list  = ['Data entry', 'Data engineer',\n",
    "                            'Data scientist', 'Data analyst',\n",
    "                            'Machine Learning']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3e9fc-3746-4f76-be83-00f2b453be34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5CD3F7'> Web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd35b8e-8c12-4e99-8819-4c6e338c3951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_page(url: str) -> BeautifulSoup:\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(delay= 10,browser= {\n",
    "        'browser': 'chrome',\n",
    "        'platform': 'windows',\n",
    "        'desktop': True,\n",
    "        'mobile': False}) \n",
    "    \n",
    "    content = scraper.get(url).text\n",
    "    soup    = BeautifulSoup(content)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3908558f-1fe9-4c88-af63-731658ee2217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upwork scraper\n",
    "def upwork_scraper(job_title : str,\n",
    "                    page      : int) -> list:\n",
    "\n",
    "    url         : str  = f'https://www.upwork.com/search/profiles/?page={page}&q={job_title}'\n",
    "    soup        : str  = scrape_page(url)\n",
    "    loaded_jobs : list = soup.find_all('div', class_= 'up-card-section up-card-hover')\n",
    "\n",
    "    return  loaded_jobs\n",
    "\n",
    "\n",
    "# Guru scraper\n",
    "def guru_scraper(job_title : str,\n",
    "                    page      : int) -> list:\n",
    "\n",
    "    url         : str  = f'https://www.guru.com/d/freelancers/skill/{job_title}/pg/{page}/'\n",
    "    soup        : str  = scrape_page(url)\n",
    "    loaded_jobs : list = soup.find_all('div', class_= 'record record--avatarCheck findGuruRecord')\n",
    "    loaded_page : int  = int(soup.find('li', class_= 'active').find('a').text)\n",
    "    \n",
    "    if loaded_page != page:\n",
    "        loaded_jobs = []\n",
    "\n",
    "    return loaded_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788705f-2f61-4a5c-ad84-ea24f56bccdb",
   "metadata": {},
   "source": [
    "Now we will try to calculate how much did each scraper take to collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0669639d-0404-4d98-aecf-1a626addee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4 s ± 180 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.67 s ± 400 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit upwork_scraper('Data analytics', 1)\n",
    "%timeit guru_scraper('Data analytics', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cfd62-4513-4cd0-bea7-40c4638debc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Collecting the Websites <span style = 'color: #4895e0'> HTML</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11593e37-e845-41f0-b749-5f6c8d7a0914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b219d160-162a-4860-9116-60302878d539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started scraping Guru data.\n",
      "Finished loading Data entry jobs data from Guru\n",
      "Finished loading Data engineer jobs data from Guru\n",
      "Finished loading Data scientist jobs data from Guru\n",
      "Finished loading Data analyst jobs data from Guru\n",
      "Finished loading Machine Learning jobs data from Guru\n",
      "\n",
      "Started scraping Upwork data.\n",
      "Finished loading Data entry jobs data from Upwork\n",
      "Finished loading Data engineer jobs data from Upwork\n",
      "Finished loading Data scientist jobs data from Upwork\n",
      "Finished loading Data analyst jobs data from Upwork\n",
      "Finished loading Machine Learning jobs data from Upwork\n"
     ]
    }
   ],
   "source": [
    "soups : dict = {'jobs_soups'     : [],\n",
    "                'job_title'      : [],\n",
    "                'platform'       : []}\n",
    "\n",
    "for scraper in [guru_scraper, upwork_scraper]:\n",
    "    scraper_name      : str  = retrieve_name(scraper)[0].split('_')[0].title()\n",
    "    print(f'\\nStarted scraping {scraper_name} data.')\n",
    "    \n",
    "    for job_title in data_jobs_titles:\n",
    "\n",
    "        concated_jobs : list = []\n",
    "\n",
    "        for i in count(0):\n",
    "            page      : int  = i + 1\n",
    "            jobs      : list = scraper(job_title, page)\n",
    "\n",
    "            if (len(jobs) == 0) or (page >= MAX_PAGES):\n",
    "                break\n",
    "\n",
    "            for job in jobs:\n",
    "                concated_jobs.append(job)\n",
    "\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs data from {scraper_name}')\n",
    "        \n",
    "        soups['jobs_soups']     .append(concated_jobs)\n",
    "        soups['job_title']      .append(job_title)\n",
    "        soups['platform']       .append(scraper_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23b374d-46b0-4b96-b3c4-9f9675358899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(soups['job_title'])):\n",
    "    if soups['job_title'][i] == 'Machine Learning':\n",
    "        soups['job_title'][i] = 'ML developer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00632aa1-9b7a-4e33-ab44-962f17b39b0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong> Collecting <span style= 'color: #517afc'>Cards</span> Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dddfc02-f25f-4f61-9f26-6b13056422bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked_df = pd.DataFrame(soups)\n",
    "\n",
    "guru_df   = stacked_df[stacked_df['platform'] == 'Guru'].reset_index()\n",
    "upwork_df = stacked_df[stacked_df['platform'] == 'Upwork'].reset_index()\n",
    "\n",
    "guru_df['describtion']     = None\n",
    "guru_df['earnings_amount'] = None\n",
    "guru_df['feedback']        = None\n",
    "guru_df['name']            = None\n",
    "guru_df['job_titles']      = None\n",
    "guru_df['addresse']        = None\n",
    "guru_df['hour_rate']       = None\n",
    "guru_df['minimum_pay']     = None\n",
    "guru_df['skills']          = None\n",
    "\n",
    "upwork_df['describtion']          = None\n",
    "upwork_df['earnings_amount']      = None\n",
    "upwork_df['feedback']             = None\n",
    "upwork_df['name']                 = None\n",
    "upwork_df['job_titles']           = None\n",
    "upwork_df['country']              = None\n",
    "upwork_df['hour_rate']            = None\n",
    "upwork_df['consultations_offers'] = None\n",
    "upwork_df['skills']               = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe538ed-36ee-4147-aeac-03dcca50ad96",
   "metadata": {},
   "source": [
    "#### **Loading Guru data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f86c767-78f2-46d0-8ddb-3b80bfcecacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, jobs in enumerate(guru_df['jobs_soups']):\n",
    "    \n",
    "    describtions     : list = []\n",
    "    earnings_amounts : list = []\n",
    "    feedbacks        : list = []\n",
    "    names            : list = []\n",
    "    job_titles       : list = []\n",
    "    addresses        : list = []\n",
    "    hour_rates       : list = []\n",
    "    minimum_pays     : list = []\n",
    "    skills           : list = []\n",
    "\n",
    "    for job in jobs:\n",
    "\n",
    "        try:      describtions     .append(job.find('p', 'serviceListing__desc').text.strip())\n",
    "        except:   describtions     .append(np.nan)\n",
    "\n",
    "        # Earnings per year\n",
    "        try:      earnings_amounts .append(job.find('span', 'earnings__amount').text.strip())\n",
    "        except:   earnings_amounts .append(np.nan)\n",
    "\n",
    "        # Feedback\n",
    "        try:      feedbacks        .append(job.find('span', 'freelancerAvatar__feedback').text.strip())\n",
    "        except:   feedbacks        .append(np.nan)\n",
    "\n",
    "        # Name\n",
    "        try:      names            .append(job.find('h3', 'freelancerAvatar__screenName').find('a').text.strip())\n",
    "        except:   names            .append(np.nan)\n",
    "\n",
    "        # Job title\n",
    "        try:      job_titles       .append(job.find('h2', 'serviceListing__title serviceListing__title--dark').find('a').text.strip())\n",
    "        except:   job_titles       .append(np.nan)\n",
    "\n",
    "        # Addresse\n",
    "        try:      addresses        .append(job.find('span', 'freelancerAvatar__location')['title'].strip())\n",
    "        except:   addresses        .append(np.nan)\n",
    "        \n",
    "        # Hour rate\n",
    "        try:      hour_rates       .append(job.find('p', 'serviceListing__rates').text.split('.')[0].strip())\n",
    "        except:   hour_rates       .append(np.nan)\n",
    "        \n",
    "        # Minimum pays\n",
    "        try:      minimum_pays     .append(job.find('p', 'serviceListing__rates').text.split('·')[-1].strip())\n",
    "        except:   minimum_pays     .append(np.nan)\n",
    "\n",
    "        # Skills\n",
    "        try:\n",
    "            skills_temp = job.find_all('a', 'skillsList__skill skillsList__skill--hasHover')\n",
    "            skills                 .append([skill.text.strip() for skill in skills_temp])\n",
    "            \n",
    "        except:   skills           .append(np.nan)\n",
    "        \n",
    "    # Now we are going to convert list into tuples.\n",
    "    guru_df['describtion']     [i] = tuple(describtions)\n",
    "    guru_df['earnings_amount'] [i] = tuple(earnings_amounts)\n",
    "    guru_df['feedback']        [i] = tuple(feedbacks)\n",
    "    guru_df['name']            [i] = tuple(names)\n",
    "    guru_df['job_titles']      [i] = tuple(job_titles)\n",
    "    guru_df['addresse']        [i] = tuple(addresses)\n",
    "    guru_df['hour_rate']       [i] = tuple(hour_rates)\n",
    "    guru_df['minimum_pay']     [i] = tuple(minimum_pays)\n",
    "    guru_df['skills']          [i] = tuple(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0ad81-d278-4b04-95b3-7bb7c585d41d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Loading Upwork data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb3ba47-4b3a-471d-a402-a9505773b6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, jobs in enumerate(upwork_df['jobs_soups']):\n",
    "    \n",
    "    describtions         : list = []\n",
    "    earnings_amounts     : list = []\n",
    "    feedbacks            : list = []\n",
    "    names                : list = []\n",
    "    job_titles           : list = []\n",
    "    countries            : list = []\n",
    "    hour_rates           : list = []\n",
    "    skills               : list = []\n",
    "    consultations_offers : list = []\n",
    "    \n",
    "\n",
    "    for job in jobs:\n",
    "\n",
    "        try:      describtions          .append(job.find('div', 'up-line-clamp-v2 clamped').text.strip())\n",
    "        except:   describtions          .append(np.nan)\n",
    "\n",
    "        # Earnings per year\n",
    "        try:      earnings_amounts      .append(job.find('p', 'mb-0').find('strong').text.strip())\n",
    "        except:   earnings_amounts      .append(np.nan)\n",
    "\n",
    "        # Feedback\n",
    "        try:      feedbacks             .append(job.find('span', 'up-job-success-text').text.split('%')[0].strip())\n",
    "        except:   feedbacks             .append(np.nan)\n",
    "\n",
    "        # Name\n",
    "        try:      names                 .append(job.find('div', 'identity-name').text.strip())\n",
    "        except:   names                 .append(np.nan)\n",
    "\n",
    "        # Job title\n",
    "        try:      job_titles            .append(job.find('p', 'my-0 freelancer-title').find('strong').text.strip())\n",
    "        except:   job_titles            .append(np.nan)\n",
    "\n",
    "        # Country\n",
    "        try:      countries             .append(job.find('span', attrs= {'itemprop': 'country-name'}).text.strip())\n",
    "        except:   countries             .append(np.nan)\n",
    "        \n",
    "        # Hour rate\n",
    "        try:      hour_rates            .append(job.find('div', 'grid-col-1 grid-col-sm-1 justify-self-start nowrap').find('strong').text.strip())\n",
    "        except:   hour_rates            .append(np.nan)\n",
    "        \n",
    "        # Offers consultations\n",
    "        try:      consultations_offers  .append(job.find('div', 'up-skill-badge up-badge up-badge-highlight up-badge-rounded-inverse').text.strip())\n",
    "        except:   consultations_offers  .append('Doesn\\'t offer consultations')\n",
    "        \n",
    "        # Skills\n",
    "        try:\n",
    "            skills_temp = job.find_all('div', 'up-skill-badge')\n",
    "            skills                 .append([skill.text.strip() for skill in skills_temp])\n",
    "            \n",
    "        except:   skills           .append(np.nan)\n",
    "\n",
    "    # Now we are going to convert list into tuples.\n",
    "    upwork_df['describtion']          [i] = tuple(describtions)\n",
    "    upwork_df['earnings_amount']      [i] = tuple(earnings_amounts)\n",
    "    upwork_df['feedback']             [i] = tuple(feedbacks)\n",
    "    upwork_df['name']                 [i] = tuple(names)\n",
    "    upwork_df['job_titles']           [i] = tuple(job_titles)\n",
    "    upwork_df['country']              [i] = tuple(countries)\n",
    "    upwork_df['hour_rate']            [i] = tuple(hour_rates)\n",
    "    upwork_df['consultations_offers'] [i] = tuple(consultations_offers)\n",
    "    upwork_df['skills']               [i] = tuple(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d22aa-a267-4e5c-994d-5b0bb532e472",
   "metadata": {},
   "source": [
    "## <center><strong><span style= 'color: #517afc'>Saving</span> the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0362e83c-ce11-4e70-a7f0-b5343c1a4b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upwork_dfs: list = [] # We will group all rows as dfs to stack them later.\n",
    "\n",
    "for _, row in upwork_df.iterrows():\n",
    "    del _\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'describtion'           : row['describtion'],\n",
    "        'earnings_amount'       : row['earnings_amount'],\n",
    "        'feedback'              : row['feedback'],\n",
    "        'name'                  : row['name'],\n",
    "        'job_titles'            : row['job_titles'],\n",
    "        'country'               : row['country'],\n",
    "        'hour_rate'             : row['hour_rate'],\n",
    "        'consultations_offers'  : row['consultations_offers'],\n",
    "        'skills'                : row['skills']})\n",
    "\n",
    "    df['job_title']   = row['job_title']\n",
    "\n",
    "    upwork_dfs.append(df)\n",
    "\n",
    "upwork_df = pd.concat(upwork_dfs, ignore_index= True, sort= False)\n",
    "upwork_df.to_parquet(r'data/upwork_freelancers.parquet')\n",
    "upwork_df.to_csv(r'data/upwork_freelancers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c7954f-fc27-4b53-9e5d-e86372247642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "guru_dfs: list = [] # We will group all rows as dfs to stack them later.\n",
    "\n",
    "for _, row in guru_df.iterrows():\n",
    "    del _\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'describtion'       : row['describtion'],\n",
    "        'earnings_amount'   : row['earnings_amount'],\n",
    "        'feedback'          : row['feedback'],\n",
    "        'name'              : row['name'],\n",
    "        'job_titles'        : row['job_titles'],\n",
    "        'addresse'          : row['addresse'],\n",
    "        'hour_rate'         : row['hour_rate'],\n",
    "        'minimum_pay'       : row['minimum_pay'],\n",
    "        'skills'            : row['skills']})\n",
    "\n",
    "    df['job_title']   = row['job_title']\n",
    "\n",
    "    guru_dfs.append(df)\n",
    "\n",
    "guru_df = pd.concat(guru_dfs, ignore_index= True, sort= False)\n",
    "guru_df.to_parquet(r'data/guru_freelancers.parquet')\n",
    "guru_df.to_csv(r'data/guru_freelancers.csv')\n",
    "\n",
    "encrypt.encrypt_json_file('../credentials.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
