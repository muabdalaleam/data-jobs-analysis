{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {},
   "source": [
    "## <center><strong>Importing <span style= 'color: #48e0dc'>Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import scrapy\n",
    "import pickle\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urljoin\n",
    "from scrapy.http import HtmlResponse\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_6464\\1277823779.py:8: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "COLORS = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "MAX_PAGES = 300\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ip_port_list = [('75.84.199.80', '80'),\n",
    "                ('172.173.241.207', '80'),\n",
    "                ('209.145.60.213', '80'),\n",
    "                ('86.109.3.28', '80'),\n",
    "                ('65.111.241.211', '80'),\n",
    "                ('144.34.162.125', '80')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5cd3f7'> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95942737-c384-4a2a-980d-02c24e559224",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_titles = ['Data entry', 'Data engineer',\n",
    "                    'Data scientist', 'Data analyst',\n",
    "                    'ML developer']\n",
    "\n",
    "indeed_countries = ['de', 'uk']#, 'www'] USA don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']\n",
    "ip_port = random.choice(ip_port_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper \n",
    "\n",
    "# I will use it for when i need to scrape a lot of data fast.\n",
    "def scrape_page_fast(url: str) -> BeautifulSoup:\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(delay=10, browser=\"chrome\") \n",
    "    content = scraper.get(url).text \n",
    "\n",
    "    return BeautifulSoup(content)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_page(url: str, retrieve_new_url= False) -> BeautifulSoup:\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    if retrieve_new_url:\n",
    "        new_url = driver.current_url\n",
    "        \n",
    "        driver.quit()\n",
    "        return BeautifulSoup(html), new_url\n",
    "        \n",
    "    driver.quit()\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indeed_scraper(page: int,\n",
    "#                    country: str,\n",
    "#                    job_title: str) -> BeautifulSoup:\n",
    "\n",
    "#     modefid_page = (page - 1) * 10\n",
    "\n",
    "#     data_job_title = job_title.replace(' ', '+')\n",
    "\n",
    "#     # Pages in indeed.com are zero based indexed\n",
    "#     url: str = f'https://{country}.indeed.com/jobs?q={data_job_title}&sc=0kf%3Aattr%28DSQF7%29%3B&start={modefid_page}' + \\\n",
    "#     '&pp=gQClAAAAAAAAAAAAAAACCXklWwCSAQIBK7YBBwDUbujiRGBahYW5TppjdUz7DjXn3aPZSbT47IoJ5LLbuzpYcXwZdzJ6rKHf' + \\\n",
    "#     '6gPWFkXVGxKKGxW-JAKb8BFo_hZAkBd7trBBTY32J2CrOuA3V9dGD_bre-lArmi9DRYlcah6hvoRfsYUNYSoQwIa8VOMZMxvH-s2Dlh' + \\\n",
    "#     'UPvUP-_Dz9ls4i-OLqVGpGh4AAA&vjk=b997bb0dddadea'\n",
    "\n",
    "#     soup = scrape_page(url)\n",
    "#     loaded_page = int(soup.find('button', 'css-ns2mzi e8ju0x51').text)\n",
    "\n",
    "#     # available_pages = []\n",
    "\n",
    "#     # for button in soup.find_all('button', 'css-1qt7hdn e8ju0x50'):\n",
    "#     #     available_pages.append(int(button.text))\n",
    "\n",
    "#     if (int(page) != loaded_page):\n",
    "#         # We will use KeyError as standard to represent getting out of the max pages.\n",
    "#         raise KeyError\n",
    "        \n",
    "#     print(f'Page {page} Loaded successfully.')\n",
    "\n",
    "#     return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(country: str, # This scraper scrapes all pages at once\n",
    "                     job_title: str,\n",
    "                     page: int) -> BeautifulSoup:\n",
    "\n",
    "    page: int = (page - 1) * 25\n",
    "    url: str = f'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={job_title}&' + \\\n",
    "    f'location={country}&geoId=90000084&trk=public_jobs_j%20obs-search-bar_search-submit&position=1&pageNum=0&start={page}'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951bed7-9819-4466-8604-9660043e7744",
   "metadata": {},
   "source": [
    "## <center><strong>Collecting<span style= 'color: #5cd3f7'> jobs </span> data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452ce6e-7b8b-4a82-b3e5-e6911937dea4",
   "metadata": {},
   "source": [
    "**What we will do in this section:**\n",
    "\n",
    "1. Collect all pages we need from each scraper and stack them together as HTML code.\n",
    "2. Scrape for jobs cards links from linkedin and indeed.\n",
    "3. Scrape for the data in those links and store the data in a DataFrame\n",
    "4. We may also consider collecting total results per each platform, country etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3d2-25cd-4321-b091-d2a7914786f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Stack pages together as HTML code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bf4c2b-d50b-494d-b2e1-5f41012401a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_integer(number_string):\n",
    "\n",
    "    cleaned_string = ''.join(filter(str.isdigit, number_string))\n",
    "    cleaned_string = cleaned_string.rstrip('+')\n",
    "    integer_value = int(cleaned_string)\n",
    "\n",
    "    return integer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791acf9d-694a-417f-9015-31fceed0babc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LinkedIn scraper.\n",
      "Loading the European Union data.\n",
      "Finished loading Data entry jobs from the European Union.\n",
      "Finished loading Data engineer jobs from the European Union.\n",
      "Finished loading Data scientist jobs from the European Union.\n",
      "Finished loading Data analyst jobs from the European Union.\n",
      "Finished loading ML developer jobs from the European Union.\n",
      "\n",
      "\n",
      "Loading the United States data.\n",
      "Finished loading Data entry jobs from the United States.\n",
      "Finished loading Data engineer jobs from the United States.\n",
      "Finished loading Data scientist jobs from the United States.\n",
      "Finished loading Data analyst jobs from the United States.\n",
      "Finished loading ML developer jobs from the United States.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soups = {'soup': [],\n",
    "         'job_title': [],\n",
    "         'country': [],\n",
    "         'platform': [],\n",
    "         'total_jobs': []}\n",
    "\n",
    "flag: bool = False\n",
    "max_page: int = 4 # TOREMVE\n",
    "\n",
    "# Linked In\n",
    "print('Starting LinkedIn scraper.')\n",
    "for country in linkedin_countries:\n",
    "\n",
    "    print(f'Loading the {country} data.')\n",
    "    for job_title in data_jobs_titles:\n",
    "        \n",
    "        full_soup: str = ''\n",
    "        total_jobs = convert_to_integer(scrape_page(f'https://www.linkedin.com/jobs/search?keywords={job_title}&location={country}&' + \n",
    "                                                    f'\\geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0').find(\n",
    "                                                        'span', 'results-context-header__job-count').text)\n",
    "        \n",
    "        for i in count(0):\n",
    "            page = i + 1\n",
    "            page_soup: BeautifulSoup = linkedin_scraper(country= country,\n",
    "                                                        job_title= job_title,\n",
    "                                                        page= i + page)\n",
    "            full_soup += str(page_soup)\n",
    "            full_soup += ' <br> '\n",
    "\n",
    "            if (page_soup.find_all('li') is None) or (page == max_page):\n",
    "                break\n",
    "\n",
    "        soups['platform'].append('LinkedIn')\n",
    "        soups['total_jobs'].append(total_jobs) \n",
    "        soups['soup'].append(full_soup)\n",
    "        soups['job_title'].append(job_title)\n",
    "        soups['country'].append(country)\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs from the {country}.')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ed47b4-02cd-49ea-9eb2-96e4e377b417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indeed scraping TODO\n",
    "# for country in indeed_countries:\n",
    "#     for job_title in data_jobs_titles:\n",
    "#         stacked_pages_soup: str = ''\n",
    "\n",
    "#         for i in count(0):\n",
    "\n",
    "#             try:\n",
    "#                 page = i + 1\n",
    "#                 soup: BeautifulSoup = indeed_scraper(country= country,\n",
    "#                                                      job_title= job_title,\n",
    "#                                                      page= page)\n",
    "\n",
    "#                 stacked_pages_soup += '\\n<br> ' + str(soup)\n",
    "#                 if page > max_page:\n",
    "#                     print('Finished loading current country or job title.\\n')\n",
    "#                     break\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 print('Finished loading current country or job title.\\n')\n",
    "#                 break\n",
    "\n",
    "#         soups['platform'].append('Indeed')\n",
    "#         soups['soup'].append(stacked_pages_soup)\n",
    "#         soups['job_title'].append(job_title)\n",
    "#         soups['country'].append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5dc7-509a-490c-bd59-b341487cb5b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Collecting jobs & companies basic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e4128b-06d4-4612-ba0a-3b27b6f134c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_escape_sequences(strings):\n",
    "\n",
    "    new_strings = []\n",
    "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
    "    translator = str.maketrans('', '', escapes)\n",
    "\n",
    "    for string in strings:\n",
    "        new_string = string.translate(translator)\n",
    "        new_strings.append(new_string)\n",
    "        \n",
    "    return tuple(new_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6983a18a-d68e-47f3-b27f-a48420b30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = pd.DataFrame(soups)\n",
    "\n",
    "soups['jobs_locations'] = np.nan\n",
    "soups['listing_dates'] = np.nan\n",
    "soups['jobs_titles'] = np.nan\n",
    "soups['companies_names'] = np.nan\n",
    "\n",
    "soups['companies_links'] = np.nan\n",
    "soups['jobs_links'] = np.nan\n",
    "\n",
    "\n",
    "# Collecting linkedIn jobs links\n",
    "for i, soup in enumerate(soups[soups['platform'] == 'LinkedIn']['soup']):\n",
    "\n",
    "    soup = BeautifulSoup(soup)\n",
    "\n",
    "    linkedin_jobs_locations = tuple(\n",
    "        [element.text for element in soup.find_all('span', 'job-search-card__location')])\n",
    "\n",
    "    linkedin_jobs_listing_dates = tuple(\n",
    "        [element['datetime'] for element in soup.find_all('time', 'job-search-card__listdate')])\n",
    "\n",
    "    linkedin_jobs_titles = tuple(\n",
    "        [element.text for element in soup.find_all('h3', 'base-search-card__title')])\n",
    "\n",
    "    linkedin_jobs_companies_names = tuple(\n",
    "        [element.text for element in soup.find_all('a', 'hidden-nested-link')])\n",
    "\n",
    "    \n",
    "    linkedin_jobs_links = tuple(\n",
    "        [element['href'] for element in soup.find_all('a', 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')])\n",
    "    \n",
    "    linkedin_companies_links = tuple(\n",
    "        [element['href'] for element in soup.find_all('a', 'hidden-nested-link')])\n",
    "\n",
    "    # I will use sets and tuples instead of lists because they are faster.\n",
    "    soups['jobs_locations'][i] = remove_escape_sequences(linkedin_jobs_locations)\n",
    "    soups['listing_dates'][i] = linkedin_jobs_listing_dates\n",
    "    soups['jobs_titles'][i] = remove_escape_sequences(linkedin_jobs_titles)\n",
    "    soups['companies_names'][i] = remove_escape_sequences(linkedin_jobs_companies_names)\n",
    "    \n",
    "    soups['jobs_links'][i] = linkedin_jobs_links\n",
    "    soups['companies_links'][i] = linkedin_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a112b4b4-9ceb-49c7-a66c-e3d3da3990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting indeed jobs links TOFIX\n",
    "# for i, soup in enumerate(soups[soups['platform'] == 'Indeed']['soup']):\n",
    "    \n",
    "#     indeed_jobs_links: list = []\n",
    "#     indeed_companies_links: list = []\n",
    "\n",
    "#     for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "#         indeed_jobs_links.append(job_link)\n",
    "\n",
    "#     for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "#         company_link = BeautifulSoup(scrape_page(job_link)).find('a', 'css-775knl emf9s7v0')\n",
    "#         indeed_companies_links.append(company_link)\n",
    "#         print(company_link)\n",
    "        \n",
    "#     soups['total_jobs'][i] = len(indeed_jobs_links)\n",
    "#     soups['jobs_links'][i] = indeed_jobs_links\n",
    "#     soups['companies_links'][i] = indeed_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "986d4f0f-1d86-4f41-8125-1e2066e2c9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>jobs_locations</th>\n",
       "      <th>listing_dates</th>\n",
       "      <th>jobs_titles</th>\n",
       "      <th>companies_names</th>\n",
       "      <th>companies_links</th>\n",
       "      <th>jobs_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>238000</td>\n",
       "      <td>(            Fremont, CA          ,           ...</td>\n",
       "      <td>(2023-07-14, 2023-07-14, 2023-07-14, 2023-06-1...</td>\n",
       "      <td>(                    Entry Level - Data Entry ...</td>\n",
       "      <td>(            Get.It Recruit - Administrative  ...</td>\n",
       "      <td>(https://www.linkedin.com/company/get-it-recru...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/entry-leve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>38000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-04-07, 2023-06-15, 2023-04-28, 2023-05-2...</td>\n",
       "      <td>(                    Data Engineer            ...</td>\n",
       "      <td>(            Patreon          ,             Sh...</td>\n",
       "      <td>(https://www.linkedin.com/company/patreon?trk=...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-engin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>144000</td>\n",
       "      <td>(            Palo Alto, CA          ,         ...</td>\n",
       "      <td>(2023-07-10, 2023-05-31, 2023-06-15, 2023-05-2...</td>\n",
       "      <td>(                    Data Scientist           ...</td>\n",
       "      <td>(            Glean          ,             AllT...</td>\n",
       "      <td>(https://www.linkedin.com/company/gleanwork?tr...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>155000</td>\n",
       "      <td>(            Pleasanton, CA          ,        ...</td>\n",
       "      <td>(2023-06-19, 2023-02-20, 2023-05-11, 2023-05-0...</td>\n",
       "      <td>(                    Entry Level Data Analyst ...</td>\n",
       "      <td>(            Flexon Technologies Inc.         ...</td>\n",
       "      <td>(https://www.linkedin.com/company/flexon-techn...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/entry-leve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>ML developer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>995</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-06-05, 2023-06-29, 2023-07-05, 2023-05-1...</td>\n",
       "      <td>(                    Engineer (Full Stack / ML...</td>\n",
       "      <td>(            Sieve          ,             Auro...</td>\n",
       "      <td>(https://www.linkedin.com/company/sievedata?tr...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/engineer-f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup       job_title  \\\n",
       "0  \\n<li>\\n<div class=\"base-card relative w-full ...      Data entry   \n",
       "1  \\n<li>\\n<div class=\"base-card relative w-full ...   Data engineer   \n",
       "2  \\n<li>\\n<div class=\"base-card relative w-full ...  Data scientist   \n",
       "3  \\n<li>\\n<div class=\"base-card relative w-full ...    Data analyst   \n",
       "4  \\n<li>\\n<div class=\"base-card relative w-full ...    ML developer   \n",
       "\n",
       "          country  platform  total_jobs  \\\n",
       "0  European Union  LinkedIn      238000   \n",
       "1  European Union  LinkedIn       38000   \n",
       "2  European Union  LinkedIn      144000   \n",
       "3  European Union  LinkedIn      155000   \n",
       "4  European Union  LinkedIn         995   \n",
       "\n",
       "                                      jobs_locations  \\\n",
       "0  (            Fremont, CA          ,           ...   \n",
       "1  (            San Francisco, CA          ,     ...   \n",
       "2  (            Palo Alto, CA          ,         ...   \n",
       "3  (            Pleasanton, CA          ,        ...   \n",
       "4  (            San Francisco, CA          ,     ...   \n",
       "\n",
       "                                       listing_dates  \\\n",
       "0  (2023-07-14, 2023-07-14, 2023-07-14, 2023-06-1...   \n",
       "1  (2023-04-07, 2023-06-15, 2023-04-28, 2023-05-2...   \n",
       "2  (2023-07-10, 2023-05-31, 2023-06-15, 2023-05-2...   \n",
       "3  (2023-06-19, 2023-02-20, 2023-05-11, 2023-05-0...   \n",
       "4  (2023-06-05, 2023-06-29, 2023-07-05, 2023-05-1...   \n",
       "\n",
       "                                         jobs_titles  \\\n",
       "0  (                    Entry Level - Data Entry ...   \n",
       "1  (                    Data Engineer            ...   \n",
       "2  (                    Data Scientist           ...   \n",
       "3  (                    Entry Level Data Analyst ...   \n",
       "4  (                    Engineer (Full Stack / ML...   \n",
       "\n",
       "                                     companies_names  \\\n",
       "0  (            Get.It Recruit - Administrative  ...   \n",
       "1  (            Patreon          ,             Sh...   \n",
       "2  (            Glean          ,             AllT...   \n",
       "3  (            Flexon Technologies Inc.         ...   \n",
       "4  (            Sieve          ,             Auro...   \n",
       "\n",
       "                                     companies_links  \\\n",
       "0  (https://www.linkedin.com/company/get-it-recru...   \n",
       "1  (https://www.linkedin.com/company/patreon?trk=...   \n",
       "2  (https://www.linkedin.com/company/gleanwork?tr...   \n",
       "3  (https://www.linkedin.com/company/flexon-techn...   \n",
       "4  (https://www.linkedin.com/company/sievedata?tr...   \n",
       "\n",
       "                                          jobs_links  \n",
       "0  (https://www.linkedin.com/jobs/view/entry-leve...  \n",
       "1  (https://www.linkedin.com/jobs/view/data-engin...  \n",
       "2  (https://www.linkedin.com/jobs/view/data-scien...  \n",
       "3  (https://www.linkedin.com/jobs/view/entry-leve...  \n",
       "4  (https://www.linkedin.com/jobs/view/engineer-f...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6883647-4384-4b32-b25b-672a6d704aa8",
   "metadata": {},
   "source": [
    "#### **Loading more jobs & companies data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b1e658d-b403-4471-a546-2d0b4e088067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@context': 'http://schema.org',\n",
       " '@type': 'JobPosting',\n",
       " 'datePosted': '2023-07-14T08:41:44.000Z',\n",
       " 'description': \"Thank you for considering this opportunity! We have an exciting remote work from home position available for Data Entry Research Panelists. Whether you have a background as an administrative assistant, data entry clerk, customer service representative, or in other fields like driving, this job is perfect for you.&lt;br&gt;&lt;br&gt;We are connecting individuals like you with companies that are directly hiring employees to work from the comfort of their homes. You'll have the flexibility to choose between part-time or full-time work, and we offer a wide range of remote opportunities in various career fields.&lt;br&gt;&lt;br&gt;What makes these Legitimate Work From Home Data Entry Jobs so great is that you'll receive training tailored to the specific position you're applying for. No previous data entry or administrative assistant experience is required, but it can definitely be a bonus.&lt;br&gt;&lt;br&gt;&lt;strong&gt;&lt;u&gt;Here's What You Can Expect From The Position&lt;br&gt;&lt;br&gt;&lt;/u&gt;&lt;/strong&gt;Competitive pay: Earn between $35 and $250 per hour for single session studies, and up to $3,000 for multi-session studies.&lt;br&gt;&lt;br&gt;Job requirements: You'll need a computer with internet access and a quiet workspace to ensure you can work without distractions. We value your independence, as you'll be working in an environment without immediate supervision. You should be comfortable following oral and written instructions.&lt;br&gt;&lt;br&gt;We welcome all backgrounds: Whether you come from a healthcare, warehouse, delivery, customer service, or any other industry, we encourage you to apply. We believe in diversity and the opportunity to learn and grow.&lt;br&gt;&lt;br&gt;Ready to get started? You'll need a laptop (webcam preferred), a stable internet connection, and data entry skills. The ability to read, write, and type a minimum of 25 words per minute is required. While previous experience in customer service, administrative assisting, sales, or sales support is helpful, it's not mandatory.&lt;br&gt;&lt;br&gt;Apply today: Don't forget to visit our website and complete the application process. Keep an eye on your inbox for an email from us once you've applied.&lt;br&gt;&lt;br&gt;Additional perks: As a panelist, you'll have the flexibility to participate in discussions online or in-person. Say goodbye to commuting as you choose to work from your residence. There are no minimum hours, allowing you to work part-time or full-time based on your schedule. Plus, you'll enjoy receiving free samples from our sponsors and partners in exchange for your valuable feedback on their products.&lt;br&gt;&lt;br&gt;This position is open to anyone seeking short-term, work-at-home, part-time, or full-time employment. No previous experience is required, and we welcome individuals from all backgrounds and industries, including data entry clerks, administrative assistants, receptionists, sales assistants, customer service agents, warehouse or factory workers, drivers, medical assistants, nurses, call center representatives, and more.&lt;br&gt;&lt;br&gt;Employment Type: Full-Time&lt;br&gt;&lt;br&gt;Salary: $ 35.00 250.00 Per Hour\",\n",
       " 'employmentType': 'FULL_TIME',\n",
       " 'hiringOrganization': {'@type': 'Organization',\n",
       "  'name': 'Get.It Recruit - Administrative',\n",
       "  'sameAs': 'https://www.linkedin.com/company/get-it-recruit-administrative',\n",
       "  'logo': 'https://media.licdn.com/dms/image/C560BAQH6--7eclItzg/company-logo_200_200/0/1672601759825?e=2147483647&amp;v=beta&amp;t=ntBMTCMHmQA5gJ06abvNdDKvgLSqNxHrfi_ICZUjyIo'},\n",
       " 'identifier': {'@type': 'PropertyValue',\n",
       "  'name': 'Get.It Recruit - Administrative',\n",
       "  'value': '466224019415237908'},\n",
       " 'image': 'https://media.licdn.com/dms/image/C560BAQH6--7eclItzg/company-logo_100_100/0/1672601759825?e=2147483647&amp;v=beta&amp;t=7wrBan4F2B7O7qyJQJ6zleIqrqBgEaIeDwo0-NgGfEg',\n",
       " 'industry': 'Human Resources Services',\n",
       " 'jobLocation': {'@type': 'Place',\n",
       "  'address': {'@type': 'PostalAddress',\n",
       "   'addressCountry': 'US',\n",
       "   'addressLocality': 'Fremont',\n",
       "   'addressRegion': 'CA',\n",
       "   'streetAddress': None},\n",
       "  'latitude': 37.5502,\n",
       "  'longitude': -121.98083},\n",
       " 'skills': '',\n",
       " 'title': 'Entry Level - Data Entry (Typist) - Remote | WFH',\n",
       " 'validThrough': '2023-08-13T08:44:09.000Z',\n",
       " 'educationRequirements': {'@type': 'EducationalOccupationalCredential',\n",
       "  'credentialCategory': 'high school'},\n",
       " 'jobLocationType': 'TELECOMMUTE',\n",
       " 'applicantLocationRequirements': {'@type': 'Country',\n",
       "  'name': 'United States'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Collecting more LinkedIn data\n",
    "for i, jobs in enumerate(soups['jobs_links']):\n",
    "\n",
    "    jobs_describtion:        tuple = ()\n",
    "    credential_category:     tuple = ()\n",
    "    job_location_type:       tuple = ()\n",
    "    jobs_employment_type:    tuple = ()\n",
    "    jobs_industry:           tuple = ()\n",
    "    \n",
    "    for job in jobs:\n",
    "\n",
    "        job_data: dict = json.loads(scrape_page_fast(job).find(\n",
    "            'script', attrs= {'type': 'application/ld+json'}).text)\n",
    "        \n",
    "        jobs_describtion      .add(job_data['description'])\n",
    "        credential_categorie  .add(job_data['educationRequirements']['credentialCategory'])\n",
    "        jobs_location_type    .add(job_data['jobLocationType'])\n",
    "        jobs_employment_type  .add(job_data['employmentType'])\n",
    "        jobs_industry         .add(job_data['industry'])\n",
    "\n",
    "    soups['jobs_describtions'][i] = jobs_describtions\n",
    "    soups['credentials_category'][i] = credential_categories\n",
    "    soups['jobs_location_type'][i] = job_location_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03ab6c24-5397-4775-ba32-764be61b26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, soup in enumerate(soups[soups['platform'] == 'Indeed']['soup']):\n",
    "\n",
    "#     indeed_jobs_links: set = set()\n",
    "#     indeed_companies_links: set = set()\n",
    "\n",
    "#     for job in soup.find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', BeautifulSoup((job)['data-jk'])\n",
    "#         indeed_jobs_links.add(job_link)\n",
    "\n",
    "#     for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "#         company_link = scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href']\n",
    "#         indeed_companies_links.add(company_link)\n",
    "\n",
    "#     soups['total_jobs'][i] = len(indeed_jobs_links)\n",
    "#     soups['jobs_links'][i] = indeed_jobs_links\n",
    "#     soups['companies_links'][i] = indeed_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f31a8d8-f416-4be3-80f3-626fe9485f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# for soup in soups[soups['platform'] == 'Indeed']['soup'][:2]:\n",
    "#     job_links = []\n",
    "\n",
    "#     for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "#         job_links.append(job_link)\n",
    "\n",
    "#      for job_link in job_links[:5]:\n",
    "#          print(scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href'])\n",
    "\n",
    "#     print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
