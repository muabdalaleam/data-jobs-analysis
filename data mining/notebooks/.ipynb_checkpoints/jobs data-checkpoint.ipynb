{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {},
   "source": [
    "## <center><strong>Importing <span style= 'color: #48e0dc'>Packeges</span>\n",
    "<sub>*And setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import scrapy\n",
    "import pickle\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from fake_useragent import UserAgent\n",
    "from scrapy.http import HtmlResponse\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from selenium.webdriver.common.by import By\n",
    "from scrapy.utils.log import configure_logging\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "COLORS = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "MAX_PAGES = 300\n",
    "\n",
    "proxies = {'http': 'http://109.254.67.104:9090',\n",
    "           'http': 'http://103.49.202.252:80'}\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1209e763-b75d-4020-a8ba-db561004ec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_jobs_titles = ['Data entry', 'Data engineer',\n",
    "                    'Data scientist', 'Data analyst',\n",
    "                    'ML developer']\n",
    "\n",
    "indeed_countries = ['de', 'uk']#, 'www'] USA don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5cd3f7'> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ac9c7-ed41-484a-9d59-69d43f4b889f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_fast(url: str) -> BeautifulSoup:\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(delay= 10,browser= {\n",
    "        'browser': 'chrome',\n",
    "        'platform': 'windows',\n",
    "        'desktop': True,\n",
    "        'mobile': False}) \n",
    "    \n",
    "    content = scraper.get(url).text \n",
    "    return BeautifulSoup(content)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_page(url: str, retrieve_new_url= False) -> BeautifulSoup:\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    \n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')  \n",
    "    \n",
    "    driver = webdriver.Chrome(options= chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "\n",
    "    html = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    if retrieve_new_url:\n",
    "        new_url = driver.current_url\n",
    "\n",
    "        driver.quit()\n",
    "        return html, new_url\n",
    "        \n",
    "    driver.quit()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05c858c-9b35-48c2-8a2e-f166081768f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35 s Â± 157 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "9.05 s Â± 691 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Comparing the two scrapers speed.\n",
    "%timeit scrape_page_fast('https://linkedin.com')\n",
    "%timeit scrape_page('https://linkedin.com')\n",
    "\n",
    "# We can find that there's about 10X speed for the `scrape_page_fast` ðŸ¤¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indeed_scraper(page: int,\n",
    "#                    country: str,\n",
    "#                    job_title: str) -> BeautifulSoup:\n",
    "\n",
    "#     modefid_page = (page - 1) * 10\n",
    "\n",
    "#     data_job_title = job_title.replace(' ', '+')\n",
    "\n",
    "#     # Pages in indeed.com are zero based indexed\n",
    "#     url: str = f'https://{country}.indeed.com/jobs?q={data_job_title}&sc=0kf%3Aattr%28DSQF7%29%3B&start={modefid_page}' + \\\n",
    "#     '&pp=gQClAAAAAAAAAAAAAAACCXklWwCSAQIBK7YBBwDUbujiRGBahYW5TppjdUz7DjXn3aPZSbT47IoJ5LLbuzpYcXwZdzJ6rKHf' + \\\n",
    "#     '6gPWFkXVGxKKGxW-JAKb8BFo_hZAkBd7trBBTY32J2CrOuA3V9dGD_bre-lArmi9DRYlcah6hvoRfsYUNYSoQwIa8VOMZMxvH-s2Dlh' + \\\n",
    "#     'UPvUP-_Dz9ls4i-OLqVGpGh4AAA&vjk=b997bb0dddadea'\n",
    "\n",
    "#     soup = scrape_page(url)\n",
    "#     loaded_page = int(soup.find('button', 'css-ns2mzi e8ju0x51').text)\n",
    "\n",
    "#     # available_pages = []\n",
    "\n",
    "#     # for button in soup.find_all('button', 'css-1qt7hdn e8ju0x50'):\n",
    "#     #     available_pages.append(int(button.text))\n",
    "\n",
    "#     if (int(page) != loaded_page):\n",
    "#         # We will use KeyError as standard to represent getting out of the max pages.\n",
    "#         raise KeyError\n",
    "        \n",
    "#     print(f'Page {page} Loaded successfully.')\n",
    "\n",
    "#     return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(country:   str, # This scraper scrapes all pages at once\n",
    "                     job_title: str,\n",
    "                     page:      int) -> BeautifulSoup:\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "        'Accept-Encoding': '*',\n",
    "        'Connection': 'keep-alive'}\n",
    "\n",
    "    page: int = (page - 1) * 25\n",
    "    url: str = f'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={job_title}&' + \\\n",
    "    f'location={country}&geoId=90000084&trk=public_jobs_j%20obs-search-bar_search-submit&position=1&pageNum=0&start={page}'\n",
    "\n",
    "    response = requests.get(url, headers= headers)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951bed7-9819-4466-8604-9660043e7744",
   "metadata": {},
   "source": [
    "## <center><strong>Collecting<span style= 'color: #5cd3f7'> jobs </span> data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452ce6e-7b8b-4a82-b3e5-e6911937dea4",
   "metadata": {},
   "source": [
    "**What we will do in this section:**\n",
    "\n",
    "1. Collect all pages we need from each scraper and stack them together as HTML code.\n",
    "2. Scrape for jobs cards links from linkedin and indeed.\n",
    "3. Scrape for the data in those links and store the data in a DataFrame\n",
    "4. We may also consider collecting total results per each platform, country etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3d2-25cd-4321-b091-d2a7914786f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Stack pages together as HTML code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11bf4c2b-d50b-494d-b2e1-5f41012401a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_integer(number_string):\n",
    "\n",
    "    cleaned_string = ''.join(filter(str.isdigit, number_string))\n",
    "    cleaned_string = cleaned_string.rstrip('+')\n",
    "    integer_value = int(cleaned_string)\n",
    "\n",
    "    return integer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "791acf9d-694a-417f-9015-31fceed0babc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LinkedIn scraper.\n",
      "Loading the European Union data.\n",
      "Finished loading Data entry jobs from the European Union.\n",
      "Finished loading Data engineer jobs from the European Union.\n",
      "Finished loading Data scientist jobs from the European Union.\n",
      "Finished loading Data analyst jobs from the European Union.\n",
      "Finished loading ML developer jobs from the European Union.\n",
      "\n",
      "\n",
      "Loading the United States data.\n",
      "Finished loading Data entry jobs from the United States.\n",
      "Finished loading Data engineer jobs from the United States.\n",
      "Finished loading Data scientist jobs from the United States.\n",
      "Finished loading Data analyst jobs from the United States.\n",
      "Finished loading ML developer jobs from the United States.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soups = {'soup':        [],\n",
    "         'job_title':   [],\n",
    "         'country':     [],\n",
    "         'platform':    [],\n",
    "         'total_jobs':  []}\n",
    "\n",
    "flag     : bool = False\n",
    "max_page : int = 4 # TOREMOVE\n",
    "\n",
    "# Linked In\n",
    "print('Starting LinkedIn scraper.')\n",
    "\n",
    "for country in linkedin_countries:\n",
    "\n",
    "    print(f'Loading the {country} data.')\n",
    "    for job_title in data_jobs_titles:\n",
    "        \n",
    "        full_soup   : str = ''\n",
    "        total_jobs  : list = convert_to_integer(scrape_page(f'https://www.linkedin.com/jobs/search?keywords={job_title}&location={country}&' + \n",
    "                                                            f'\\geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0').find(\n",
    "                                                                   'span', 'results-context-header__job-count').text)\n",
    "        \n",
    "        for i in count(0):\n",
    "            page = i + 1\n",
    "            page_soup: BeautifulSoup = linkedin_scraper(country= country,\n",
    "                                                        job_title= job_title,\n",
    "                                                        page= i + page)\n",
    "            full_soup += str(page_soup)\n",
    "            full_soup += ' <br> '\n",
    "\n",
    "            if (page_soup.find_all('li') is None) or (page == max_page):\n",
    "                break\n",
    "\n",
    "        soups['platform']      .append('LinkedIn')\n",
    "        soups['total_jobs']    .append(total_jobs) \n",
    "        soups['soup']          .append(full_soup)\n",
    "        soups['job_title']     .append(job_title)\n",
    "        soups['country']       .append(country)\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs from the {country}.')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ed47b4-02cd-49ea-9eb2-96e4e377b417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indeed scraping TODO\n",
    "# for country in indeed_countries:\n",
    "#     for job_title in data_jobs_titles:\n",
    "#         stacked_pages_soup: str = ''\n",
    "\n",
    "#         for i in count(0):\n",
    "\n",
    "#             try:\n",
    "#                 page = i + 1\n",
    "#                 soup: BeautifulSoup = indeed_scraper(country= country,\n",
    "#                                                      job_title= job_title,\n",
    "#                                                      page= page)\n",
    "\n",
    "#                 stacked_pages_soup += '\\n<br> ' + str(soup)\n",
    "#                 if page > max_page:\n",
    "#                     print('Finished loading current country or job title.\\n')\n",
    "#                     break\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 print('Finished loading current country or job title.\\n')\n",
    "#                 break\n",
    "\n",
    "#         soups['platform'].append('Indeed')\n",
    "#         soups['soup'].append(stacked_pages_soup)\n",
    "#         soups['job_title'].append(job_title)\n",
    "#         soups['country'].append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5dc7-509a-490c-bd59-b341487cb5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Collecting jobs & companies basic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e4128b-06d4-4612-ba0a-3b27b6f134c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_escape_sequences(strings):\n",
    "\n",
    "    new_strings = []\n",
    "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
    "    translator = str.maketrans('', '', escapes)\n",
    "\n",
    "    for string in strings:\n",
    "        new_string = string.translate(translator)\n",
    "        new_strings.append(new_string)\n",
    "        \n",
    "    return tuple(new_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6983a18a-d68e-47f3-b27f-a48420b30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = pd.DataFrame(soups)\n",
    "\n",
    "soups['jobs_locations']    = np.nan\n",
    "soups['listing_dates']     = np.nan\n",
    "soups['jobs_titles']       = np.nan\n",
    "soups['companies_names']   = np.nan\n",
    "\n",
    "soups['companies_links']   = np.nan\n",
    "soups['jobs_links']        = np.nan\n",
    "\n",
    "\n",
    "# Collecting linkedIn jobs links\n",
    "for i, soup in enumerate(soups[soups['platform'] == 'LinkedIn']['soup']):\n",
    "\n",
    "    soup = BeautifulSoup(soup)\n",
    "\n",
    "    linkedin_jobs_locations           = tuple([element.text for element in soup.find_all(\n",
    "        'span', 'job-search-card__location')])\n",
    "\n",
    "    linkedin_jobs_listing_dates       = tuple([element['datetime'] for element in soup.find_all(\n",
    "        'time', 'job-search-card__listdate')])\n",
    "\n",
    "    linkedin_jobs_titles              = tuple([element.text for element in soup.find_all(\n",
    "        'h3', 'base-search-card__title')])\n",
    "\n",
    "    linkedin_jobs_companies_names     = tuple([element.text for element in soup.find_all(\n",
    "        'a', 'hidden-nested-link')])\n",
    "\n",
    "    \n",
    "    linkedin_jobs_links               = tuple([element['href'] for element in soup.find_all(\n",
    "        'a', 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')])\n",
    "    \n",
    "    linkedin_companies_links          = tuple([element['href'] for element in soup.find_all(\n",
    "        'a', 'hidden-nested-link')])\n",
    "\n",
    "    # I will use sets and tuples instead of lists because they are faster.\n",
    "    soups['jobs_locations']    [i] = remove_escape_sequences(linkedin_jobs_locations)\n",
    "    soups['listing_dates']     [i] = linkedin_jobs_listing_dates\n",
    "    soups['jobs_titles']       [i] = remove_escape_sequences(linkedin_jobs_titles)\n",
    "    soups['companies_names']   [i] = remove_escape_sequences(linkedin_jobs_companies_names)\n",
    "\n",
    "    soups['jobs_links']        [i] = linkedin_jobs_links\n",
    "    soups['companies_links']   [i] = linkedin_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a112b4b4-9ceb-49c7-a66c-e3d3da3990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting indeed jobs links TOFIX\n",
    "# for i, soup in enumerate(soups[soups['platform'] == 'Indeed']['soup']):\n",
    "    \n",
    "#     indeed_jobs_links: list = []\n",
    "#     indeed_companies_links: list = []\n",
    "\n",
    "#     for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "#         indeed_jobs_links.append(job_link)\n",
    "\n",
    "#     for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "#         company_link = BeautifulSoup(scrape_page(job_link)).find('a', 'css-775knl emf9s7v0')\n",
    "#         indeed_companies_links.append(company_link)\n",
    "#         print(company_link)\n",
    "        \n",
    "#     soups['total_jobs'][i] = len(indeed_jobs_links)\n",
    "#     soups['jobs_links'][i] = indeed_jobs_links\n",
    "#     soups['companies_links'][i] = indeed_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986d4f0f-1d86-4f41-8125-1e2066e2c9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>jobs_locations</th>\n",
       "      <th>listing_dates</th>\n",
       "      <th>jobs_titles</th>\n",
       "      <th>companies_names</th>\n",
       "      <th>companies_links</th>\n",
       "      <th>jobs_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>305000</td>\n",
       "      <td>(            Fremont, CA          ,           ...</td>\n",
       "      <td>(2023-07-14, 2023-07-14, 2023-07-14, 2023-07-1...</td>\n",
       "      <td>(                    Entry Level - Data Entry ...</td>\n",
       "      <td>(            Get.It Recruit - Administrative  ...</td>\n",
       "      <td>(https://www.linkedin.com/company/get-it-recru...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/entry-leve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>40000</td>\n",
       "      <td>(            San Ramon, CA          ,         ...</td>\n",
       "      <td>(2023-06-15, 2023-04-07, 2023-04-28, 2023-05-2...</td>\n",
       "      <td>(                    Data Engineer            ...</td>\n",
       "      <td>(            ShiftCode Analytics, Inc.        ...</td>\n",
       "      <td>(https://www.linkedin.com/company/shiftcode-an...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-engin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>155000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-06-16, 2023-07-10, 2023-07-06, 2023-07-1...</td>\n",
       "      <td>(                    Machine Learning Scientis...</td>\n",
       "      <td>(            Patterned Learning AI          , ...</td>\n",
       "      <td>(https://www.linkedin.com/company/patterned-le...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/machine-le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>162000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...</td>\n",
       "      <td>(                    Data Analyst             ...</td>\n",
       "      <td>(            Digital Janet          ,         ...</td>\n",
       "      <td>(https://www.linkedin.com/company/digitaljanet...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>ML developer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>1000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-06-05, 2023-06-29, 2023-07-05, 2023-05-1...</td>\n",
       "      <td>(                    Engineer (Full Stack / ML...</td>\n",
       "      <td>(            Sieve          ,             Auro...</td>\n",
       "      <td>(https://www.linkedin.com/company/sievedata?tr...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/engineer-f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup       job_title  \\\n",
       "0  \\n<li>\\n<div class=\"base-card relative w-full ...      Data entry   \n",
       "1  \\n<li>\\n<div class=\"base-card relative w-full ...   Data engineer   \n",
       "2  \\n<li>\\n<div class=\"base-card relative w-full ...  Data scientist   \n",
       "3  \\n<li>\\n<div class=\"base-card relative w-full ...    Data analyst   \n",
       "4  \\n<li>\\n<div class=\"base-card relative w-full ...    ML developer   \n",
       "\n",
       "          country  platform  total_jobs  \\\n",
       "0  European Union  LinkedIn      305000   \n",
       "1  European Union  LinkedIn       40000   \n",
       "2  European Union  LinkedIn      155000   \n",
       "3  European Union  LinkedIn      162000   \n",
       "4  European Union  LinkedIn        1000   \n",
       "\n",
       "                                      jobs_locations  \\\n",
       "0  (            Fremont, CA          ,           ...   \n",
       "1  (            San Ramon, CA          ,         ...   \n",
       "2  (            San Francisco, CA          ,     ...   \n",
       "3  (            San Francisco, CA          ,     ...   \n",
       "4  (            San Francisco, CA          ,     ...   \n",
       "\n",
       "                                       listing_dates  \\\n",
       "0  (2023-07-14, 2023-07-14, 2023-07-14, 2023-07-1...   \n",
       "1  (2023-06-15, 2023-04-07, 2023-04-28, 2023-05-2...   \n",
       "2  (2023-06-16, 2023-07-10, 2023-07-06, 2023-07-1...   \n",
       "3  (2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...   \n",
       "4  (2023-06-05, 2023-06-29, 2023-07-05, 2023-05-1...   \n",
       "\n",
       "                                         jobs_titles  \\\n",
       "0  (                    Entry Level - Data Entry ...   \n",
       "1  (                    Data Engineer            ...   \n",
       "2  (                    Machine Learning Scientis...   \n",
       "3  (                    Data Analyst             ...   \n",
       "4  (                    Engineer (Full Stack / ML...   \n",
       "\n",
       "                                     companies_names  \\\n",
       "0  (            Get.It Recruit - Administrative  ...   \n",
       "1  (            ShiftCode Analytics, Inc.        ...   \n",
       "2  (            Patterned Learning AI          , ...   \n",
       "3  (            Digital Janet          ,         ...   \n",
       "4  (            Sieve          ,             Auro...   \n",
       "\n",
       "                                     companies_links  \\\n",
       "0  (https://www.linkedin.com/company/get-it-recru...   \n",
       "1  (https://www.linkedin.com/company/shiftcode-an...   \n",
       "2  (https://www.linkedin.com/company/patterned-le...   \n",
       "3  (https://www.linkedin.com/company/digitaljanet...   \n",
       "4  (https://www.linkedin.com/company/sievedata?tr...   \n",
       "\n",
       "                                          jobs_links  \n",
       "0  (https://www.linkedin.com/jobs/view/entry-leve...  \n",
       "1  (https://www.linkedin.com/jobs/view/data-engin...  \n",
       "2  (https://www.linkedin.com/jobs/view/machine-le...  \n",
       "3  (https://www.linkedin.com/jobs/view/data-analy...  \n",
       "4  (https://www.linkedin.com/jobs/view/engineer-f...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "soups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6883647-4384-4b32-b25b-672a6d704aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Loading more jobs data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b598359-33f4-4698-893b-aafc3fb1fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting more LinkedIn data\n",
    "\n",
    "# We will collect the: \n",
    "#    describtion, credential category, location type\n",
    "#\n",
    "soups['describtion']         = None\n",
    "soups['credential_category'] = None\n",
    "soups['location_type']       = None\n",
    "soups['employment_type']     = None\n",
    "soups['industry']            = None\n",
    "soups['country']             = None\n",
    "\n",
    "for i, jobs in enumerate([soups['jobs_links'][0]]): # TOREMOVE remove the loop limit\n",
    "\n",
    "    describtion:           list = []\n",
    "    credential_category:   list = []\n",
    "    location_type:         list = []\n",
    "    employment_type:       list = []\n",
    "    industry:              list = []\n",
    "    country:               list = []\n",
    "    \n",
    "    for job in jobs:\n",
    "            \n",
    "        try:\n",
    "            job_data:  dict = json.loads(scrape_page_fast(job).find(\n",
    "                'script', attrs= {'type': 'application/ld+json'}).text)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try: job_data['jobLocationType'] is True\n",
    "        except: job_data['jobLocationType'] = np.nan\n",
    "            \n",
    "        try: job_data['applicantLocationRequirements'] is True\n",
    "        except: job_data['applicantLocationRequirements'] = {'name': np.nan}\n",
    "\n",
    "        try: job_data['educationRequirements'] is True\n",
    "        except: job_data['educationRequirements'] = {'credentialCategory': np.nan}\n",
    "        \n",
    "        describtion               .append(job_data['description'].strip())\n",
    "        credential_category       .append(job_data['educationRequirements']['credentialCategory'])\n",
    "        location_type             .append(job_data['jobLocationType'])\n",
    "        employment_type           .append(job_data['employmentType'].strip())\n",
    "        industry                  .append(job_data['industry'].strip())\n",
    "        country                   .append(job_data['applicantLocationRequirements']['name'])\n",
    "\n",
    "    # Now we are going to convert list into tuples.\n",
    "    soups['describtion']          [i] = tuple(describtion)\n",
    "    soups['credential_category']  [i] = tuple(credential_category)\n",
    "    soups['location_type']        [i] = tuple(location_type)\n",
    "    soups['employment_type']      [i] = tuple(employment_type)\n",
    "    soups['industry']             [i] = tuple(industry)\n",
    "    soups['country']              [i] = tuple(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33364b8-a9ec-492d-a860-7b9ad7c369dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO (loading indeed data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f91752dd-2834-4e33-8351-3b3e5f7df845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>jobs_locations</th>\n",
       "      <th>listing_dates</th>\n",
       "      <th>jobs_titles</th>\n",
       "      <th>companies_names</th>\n",
       "      <th>companies_links</th>\n",
       "      <th>jobs_links</th>\n",
       "      <th>describtion</th>\n",
       "      <th>credential_category</th>\n",
       "      <th>location_type</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>(United States, United States, United States, ...</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>298000</td>\n",
       "      <td>(            Fremont, CA          ,           ...</td>\n",
       "      <td>(2023-07-14, 2023-07-14, 2023-07-14, 2023-06-1...</td>\n",
       "      <td>(                    Entry Level - Data Entry ...</td>\n",
       "      <td>(            Get.It Recruit - Administrative  ...</td>\n",
       "      <td>(https://www.linkedin.com/company/get-it-recru...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/entry-leve...</td>\n",
       "      <td>(Thank you for considering this opportunity! W...</td>\n",
       "      <td>(high school, high school, high school, bachel...</td>\n",
       "      <td>(TELECOMMUTE, TELECOMMUTE, TELECOMMUTE, nan, T...</td>\n",
       "      <td>(FULL_TIME, FULL_TIME, FULL_TIME, PART_TIME, F...</td>\n",
       "      <td>(Human Resources Services, Human Resources Ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>None</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>40000</td>\n",
       "      <td>(            San Ramon, CA          ,         ...</td>\n",
       "      <td>(2023-06-15, 2023-04-07, 2023-04-28, 2023-05-2...</td>\n",
       "      <td>(                    Data Engineer            ...</td>\n",
       "      <td>(            ShiftCode Analytics, Inc.        ...</td>\n",
       "      <td>(https://www.linkedin.com/company/shiftcode-an...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-engin...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>155000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-06-16, 2023-07-10, 2023-07-17, 2023-05-0...</td>\n",
       "      <td>(                    Data Scientist           ...</td>\n",
       "      <td>(            Experfy          ,             Pa...</td>\n",
       "      <td>(https://www.linkedin.com/company/experfy?trk=...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-scien...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>None</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>164000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...</td>\n",
       "      <td>(                    Data Analyst             ...</td>\n",
       "      <td>(            Digital Janet          ,         ...</td>\n",
       "      <td>(https://www.linkedin.com/company/digitaljanet...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/data-analy...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n&lt;li&gt;\\n&lt;div class=\"base-card relative w-full ...</td>\n",
       "      <td>ML developer</td>\n",
       "      <td>None</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>1000</td>\n",
       "      <td>(            San Francisco, CA          ,     ...</td>\n",
       "      <td>(2023-06-05, 2023-06-29, 2023-07-05, 2023-05-1...</td>\n",
       "      <td>(                    Engineer (Full Stack / ML...</td>\n",
       "      <td>(            Sieve          ,             Auro...</td>\n",
       "      <td>(https://www.linkedin.com/company/sievedata?tr...</td>\n",
       "      <td>(https://www.linkedin.com/jobs/view/engineer-f...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup       job_title  \\\n",
       "0  \\n<li>\\n<div class=\"base-card relative w-full ...      Data entry   \n",
       "1  \\n<li>\\n<div class=\"base-card relative w-full ...   Data engineer   \n",
       "2  \\n<li>\\n<div class=\"base-card relative w-full ...  Data scientist   \n",
       "3  \\n<li>\\n<div class=\"base-card relative w-full ...    Data analyst   \n",
       "4  \\n<li>\\n<div class=\"base-card relative w-full ...    ML developer   \n",
       "\n",
       "                                             country  platform  total_jobs  \\\n",
       "0  (United States, United States, United States, ...  LinkedIn      298000   \n",
       "1                                               None  LinkedIn       40000   \n",
       "2                                               None  LinkedIn      155000   \n",
       "3                                               None  LinkedIn      164000   \n",
       "4                                               None  LinkedIn        1000   \n",
       "\n",
       "                                      jobs_locations  \\\n",
       "0  (            Fremont, CA          ,           ...   \n",
       "1  (            San Ramon, CA          ,         ...   \n",
       "2  (            San Francisco, CA          ,     ...   \n",
       "3  (            San Francisco, CA          ,     ...   \n",
       "4  (            San Francisco, CA          ,     ...   \n",
       "\n",
       "                                       listing_dates  \\\n",
       "0  (2023-07-14, 2023-07-14, 2023-07-14, 2023-06-1...   \n",
       "1  (2023-06-15, 2023-04-07, 2023-04-28, 2023-05-2...   \n",
       "2  (2023-06-16, 2023-07-10, 2023-07-17, 2023-05-0...   \n",
       "3  (2023-02-20, 2023-05-11, 2023-05-08, 2023-01-2...   \n",
       "4  (2023-06-05, 2023-06-29, 2023-07-05, 2023-05-1...   \n",
       "\n",
       "                                         jobs_titles  \\\n",
       "0  (                    Entry Level - Data Entry ...   \n",
       "1  (                    Data Engineer            ...   \n",
       "2  (                    Data Scientist           ...   \n",
       "3  (                    Data Analyst             ...   \n",
       "4  (                    Engineer (Full Stack / ML...   \n",
       "\n",
       "                                     companies_names  \\\n",
       "0  (            Get.It Recruit - Administrative  ...   \n",
       "1  (            ShiftCode Analytics, Inc.        ...   \n",
       "2  (            Experfy          ,             Pa...   \n",
       "3  (            Digital Janet          ,         ...   \n",
       "4  (            Sieve          ,             Auro...   \n",
       "\n",
       "                                     companies_links  \\\n",
       "0  (https://www.linkedin.com/company/get-it-recru...   \n",
       "1  (https://www.linkedin.com/company/shiftcode-an...   \n",
       "2  (https://www.linkedin.com/company/experfy?trk=...   \n",
       "3  (https://www.linkedin.com/company/digitaljanet...   \n",
       "4  (https://www.linkedin.com/company/sievedata?tr...   \n",
       "\n",
       "                                          jobs_links  \\\n",
       "0  (https://www.linkedin.com/jobs/view/entry-leve...   \n",
       "1  (https://www.linkedin.com/jobs/view/data-engin...   \n",
       "2  (https://www.linkedin.com/jobs/view/data-scien...   \n",
       "3  (https://www.linkedin.com/jobs/view/data-analy...   \n",
       "4  (https://www.linkedin.com/jobs/view/engineer-f...   \n",
       "\n",
       "                                         describtion  \\\n",
       "0  (Thank you for considering this opportunity! W...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                 credential_category  \\\n",
       "0  (high school, high school, high school, bachel...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                       location_type  \\\n",
       "0  (TELECOMMUTE, TELECOMMUTE, TELECOMMUTE, nan, T...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                     employment_type  \\\n",
       "0  (FULL_TIME, FULL_TIME, FULL_TIME, PART_TIME, F...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            industry  \n",
       "0  (Human Resources Services, Human Resources Ser...  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d43d2-cbb6-4f33-b1c6-74673bb20552",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Loading more companies data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f5e8b79-9b8e-4821-84fe-060c0986c428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_castable_to_number(s):\n",
    "    return s.isdigit() or (s.isnumeric() if hasattr(s, 'isnumeric') else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4c292415-dc99-4536-9e80-c3b6637646b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_proxy() -> dict:\n",
    "\n",
    "    headers: dict = {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.google.com/',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'}\n",
    "\n",
    "    url = 'https://proxylist.geonode.com/api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc'\n",
    "\n",
    "    response = requests.get(url, headers= headers)\n",
    "    proxies = response.json()\n",
    "\n",
    "    return proxies['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f220a-4162-4322-9c9c-f018fba3e2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_random_proxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c63e0661-2e72-4fc9-9ab9-aaa92c36ba73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/company/planet-forward?trk=public_jobs_jserp-result_job-search-card-subtitle\n",
      "<html><head>\n",
      "<script type=\"text/javascript\">\n",
      "var _0x26bb58=_0x3a5d;function _0x2511(){var _0x2338e3=\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     followers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m discription \u001b[38;5;241m=\u001b[39m \u001b[43mcompany_html\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     48\u001b[0m employees   \u001b[38;5;241m=\u001b[39m company_data[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(company_data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for companies in [soups['companies_links'][0]]: # TOREMOVE\n",
    "\n",
    "    company_descriptions   : list = []\n",
    "    company_foundyears     : list = []\n",
    "    company_interests      : list = []\n",
    "    company_employees      : list = []\n",
    "    company_followers      : list = []\n",
    "\n",
    "\n",
    "    for company_url in set(companies): # We are using `set` type to elemenate the duplicates.\n",
    "\n",
    "        ua = UserAgent()\n",
    "\n",
    "        headers: dict = {\n",
    "            'User-Agent': ua.random,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'}\n",
    "\n",
    "        print(company_url)\n",
    "        \n",
    "        foundyear = None\n",
    "        interest  = None\n",
    "\n",
    "        # To repeat scraping the page if any thing unexpected happens\n",
    "        company_url = company_url.split('?')[0]\n",
    "        response    = requests.get(company_url,\n",
    "                                   headers= headers,\n",
    "                                   proxies= proxies)\n",
    "\n",
    "        company_html = BeautifulSoup(response.content,'html.parser')\n",
    "        company_data = company_html.find_all(\n",
    "            'dd', class_='font-sans text-md text-color-text break-words overflow-hidden')\n",
    "\n",
    "        print(str(company_html)[:100])\n",
    "        \n",
    "        try:\n",
    "            followers = company_html.find(\n",
    "                'p', class_= '!text-xs text-color-text-low-emphasis leading-[1.33333] m-0 truncate').text.strip()\n",
    "        except:\n",
    "            followers = None\n",
    "\n",
    "        \n",
    "        discription = company_html.find('meta', attrs= {'name': 'description'})['content']\n",
    "        employees   = company_data[2].text.strip()\n",
    "\n",
    "        if len(company_data) >= 6:\n",
    "            if is_castable_to_number(company_data[5].text.strip()):\n",
    "                foundyear = company_data[5].text.strip()\n",
    "        \n",
    "        if len(company_data) >= 7:\n",
    "                interest = company_data[6].text.strip()\n",
    "\n",
    "\n",
    "        company_descriptions   .append(discription)\n",
    "        company_foundyears     .append(foundyear)\n",
    "        company_interests      .append(interest)\n",
    "        company_employees      .append(employees)\n",
    "        company_followers      .append(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82ff35e9-7578-4f4c-8a2f-03d2e0c45290",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Renewable: Wind and Solar, Nuclear and Fossil Power, Power Delivery: Transmission & Distribution, Power Generation, Civil & Environmental Engineering, Manufacturing & Light Industrial, Oil & Gas, Information Technology, HR/Administrative, Engineering, Automotive, Aerospace and Defense, Aviation, and Construction',\n",
       " 'Teachers, Hiring, K-12, Education, Urban, Schools, Matching, Marketplace, parents, families, learningpod, remotelearning, learningpods, pandemicpods, and school leadership',\n",
       " 'Staffing, Recruiting, Sourcing, Interviewing, Vetting, Candidate Assessment, Performance Assessment, Property Management, Sales, Sales Management, Frontline Management, Career Launchers, and Industry Builders',\n",
       " 'Electric Vehicle Charging Solutions, Condominium Car Chargers, Electric Vehicle Supply Equipment (EVSE), Apartment Car Chargers, Fleet Charging, and EV Fleets',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Spanning the Stack from Firmware to the UI, Embedded Systems and Firmware Development, Managed Services, Software and Test Development, Augmented Reality, Autonomous Vehicle Development, Robotics, Embedded Devices, Cloud Technologies, Test Automation, Networking, Mobile Development, Visual and UX Design, and IT Services',\n",
       " None,\n",
       " 'Property Managment',\n",
       " None,\n",
       " 'Personal Styling, Direct to Consumer, eCommerce, Retail, Apparel, Technology, Big Data, Petite, Fashion, Engineering, AI, Machine Learning, and Merchandising',\n",
       " 'Recruiting, Staffing, Engineering Services, Technical Recruiting, Human Resources, Technology, Software, and Engineering',\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_interests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653a6b6-5d38-44a8-b3ee-aadc5b900e40",
   "metadata": {},
   "source": [
    "## <center><strong><span style= 'color: #5cd3f7'>Saving</span> the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b4383-5de1-4f27-8929-3429a2db3111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
