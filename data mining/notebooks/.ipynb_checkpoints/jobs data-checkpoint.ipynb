{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {},
   "source": [
    "## <center><strong><span style= \"color: #51fcc6\">Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {},
   "source": [
    "## <center><strong>Importing <span style= \"color: #48e0dc\">Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"#51fcc6\", \"#48e0dc\", \"#5cd3f7\", \"#4895e0\", \"#517afc\"]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "MAX_PAGES = 300\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= \"color: #5cd3f7\"> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95942737-c384-4a2a-980d-02c24e559224",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_titles = [\"Data entry\", \"Data engineer\",\n",
    "                    \"Data scientist\", \"Data analyst\",\n",
    "                    \"ML devoloper\"]\n",
    "\n",
    "indeed_countries = ['uk', 'de', 'www'] # USA don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url: str,\n",
    "                ip_and_port: tuple[str]= None,\n",
    "                retrieve_new_url= None) -> BeautifulSoup:\n",
    "\n",
    "    if ip_and_port:\n",
    "        firefox_options = Options()\n",
    "        firefox_options.add_argument(f'--proxy-server={ip_and_port[0]}:{ip_and_port[1]}')\n",
    "        webdriver_service = Service(GeckoDriverManager().install())\n",
    "        driver = webdriver.Firefox(service= webdriver_service, options= firefox_options)\n",
    "\n",
    "    else:\n",
    "        driver = webdriver.Firefox(service= webdriver_service, options= firefox_options)\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    # Wait until the page is fully loaded\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    \n",
    "    html = driver.page_source\n",
    "\n",
    "    if retrieve_new_url:\n",
    "\n",
    "        new_url = driver.current_url\n",
    "        driver.quit()\n",
    "\n",
    "        return BeautifulSoup(html), new_url\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_scraper(page: int,\n",
    "                   country: str,\n",
    "                   data_job_title: str) -> BeautifulSoup:\n",
    "\n",
    "    page = (page - 1) * 10\n",
    "    data_job_title = data_job_title.replace(' ', '+')\n",
    "\n",
    "    # Pages in indeed.com are zero based indexed\n",
    "    url: str = f'https://{country}.indeed.com/jobs?q={data_job_title}&sc=0kf%3Aattr%28DSQF7%29%3B&start=f{page}' + \\\n",
    "    '&pp=gQClAAAAAAAAAAAAAAACCXklWwCSAQIBK7YBBwDUbujiRGBahYW5TppjdUz7DjXn3aPZSbT47IoJ5LLbuzpYcXwZdzJ6rKHf' + \\\n",
    "    '6gPWFkXVGxKKGxW-JAKb8BFo_hZAkBd7trBBTY32J2CrOuA3V9dGD_bre-lArmi9DRYlcah6hvoRfsYUNYSoQwIa8VOMZMxvH-s2Dlh' + \\\n",
    "    'UPvUP-_Dz9ls4i-OLqVGpGh4AAA&vjk=b997bb0dddadea'\n",
    "\n",
    "    soup = scrape_page(url)\n",
    "\n",
    "    loaded_page = int(soup.find('button', 'css-ns2mzi e8ju0x51').text)\n",
    "    available_pages = []\n",
    "                       \n",
    "    for button in soup.find_all('button', 'css-1qt7hdn e8ju0x50'):\n",
    "        available_pages.append(int(button.text))\n",
    "\n",
    "    print(url)\n",
    "    print(loaded_page)\n",
    "    print((page / 10) + 1)\n",
    "\n",
    "    if ((page / 10) + 1 != loaded_page) or ((page / 10) + 1 not in available_pages):\n",
    "        # We will use KeyError as standard to represent getting out of the max pages.\n",
    "        raise KeyError\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(page: int,\n",
    "                     country: str,\n",
    "                     data_job_title: str) -> BeautifulSoup:\n",
    "\n",
    "    page = (page - 1) * 50\n",
    "\n",
    "    url: str = 'https://www.linkedin.com/jobs/search/?currentJobId=3643054242&geoId=103644278&' + \\\n",
    "              f'keywords={data_job_title}&location={country}&refresh=true&start={page}'\n",
    "\n",
    "    soup, new_url = scrape_page(url, retrieve_new_url= True)\n",
    "\n",
    "    loaded_page = int(new_url.split('=')[-1])\n",
    "    print(loaded_page)\n",
    "\n",
    "    if (page / 10) + 1 != loaded_page:\n",
    "        # We will use KeyError as standard to represent getting out of the max pages.\n",
    "        raise KeyError\n",
    "\n",
    "    # return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7830bb99-34cb-41b4-846d-67c3de863ecf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://uae.indeed.com/jobs?q=Data+analyst&sc=0kf%3Aattr%28DSQF7%29%3B&start=f10&pp=gQClAAAAAAAAAAAAAAACCXklWwCSAQIBK7YBBwDUbujiRGBahYW5TppjdUz7DjXn3aPZSbT47IoJ5LLbuzpYcXwZdzJ6rKHf6gPWFkXVGxKKGxW-JAKb8BFo_hZAkBd7trBBTY32J2CrOuA3V9dGD_bre-lArmi9DRYlcah6hvoRfsYUNYSoQwIa8VOMZMxvH-s2DlhUPvUP-_Dz9ls4i-OLqVGpGh4AAA&vjk=b997bb0dddadea\n",
      "1\n",
      "2.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mindeed_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muae\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData analyst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 23\u001b[0m, in \u001b[0;36mindeed_scraper\u001b[1;34m(page, country, data_job_title)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m((page \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (page \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m!=\u001b[39m loaded_page:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# We will use KeyError as standard to represent getting out of the max pages.\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  soup\n",
      "\u001b[1;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indeed_scraper(2, 'uae', 'Data analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "13d629e2-ea07-47f7-98ae-4d2fd1d5405c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'https://www.linkedin.com/jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlinkedin_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUnited States\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData scientist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[94], line 13\u001b[0m, in \u001b[0;36mlinkedin_scraper\u001b[1;34m(page, country, data_job_title)\u001b[0m\n\u001b[0;32m      8\u001b[0m url: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.linkedin.com/jobs/search/?currentJobId=3643054242&geoId=103644278&\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m      9\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_job_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&location=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&refresh=true&start=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m soup, new_url \u001b[38;5;241m=\u001b[39m scrape_page(url, retrieve_new_url\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m loaded_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(loaded_page)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (page \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m!=\u001b[39m loaded_page:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# We will use KeyError as standard to represent getting out of the max pages.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'https://www.linkedin.com/jobs'"
     ]
    }
   ],
   "source": [
    "linkedin_scraper(2, 'United States', 'Data scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4eef8-2d5d-4430-8fc0-07b91af3865c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
