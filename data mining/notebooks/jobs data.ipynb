{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {},
   "source": [
    "## <center><strong>Importing <span style= 'color: #48e0dc'>Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "COLORS = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "MAX_PAGES = 300\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ip_port_list = [('75.84.199.80', '80'),\n",
    "                ('172.173.241.207', '80'),\n",
    "                ('209.145.60.213', '80'),\n",
    "                ('86.109.3.28', '80'),\n",
    "                ('65.111.241.211', '80'),\n",
    "                ('144.34.162.125', '80')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5cd3f7'> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95942737-c384-4a2a-980d-02c24e559224",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_titles = ['Data entry', 'Data engineer']\n",
    "                    # 'Data scientist', 'Data analyst',\n",
    "                    # 'ML devoloper']\n",
    "\n",
    "indeed_countries = ['de', 'uk']#, 'www'] USA don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']\n",
    "ip_port = random.choice(ip_port_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url: str, ip_and_port: tuple[str] = None, retrieve_new_url=None) -> BeautifulSoup:\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    if ip_and_port:\n",
    "        chrome_options.add_argument(f'--proxy-server={ip_and_port[0]}:{ip_and_port[1]}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            if retrieve_new_url:\n",
    "                new_url = driver.current_url\n",
    "                driver.quit()\n",
    "                return soup, new_url\n",
    "            \n",
    "            driver.quit()\n",
    "            return soup\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Retrying ({retry + 1}/{max_retries}) after {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    print(\"Maximum retries exceeded. Unable to scrape the page.\")\n",
    "    driver.quit()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_scraper(page: int,\n",
    "                   country: str,\n",
    "                   job_title: str) -> BeautifulSoup:\n",
    "\n",
    "    modefid_page = (page - 1) * 10\n",
    "\n",
    "    data_job_title = job_title.replace(' ', '+')\n",
    "\n",
    "    # Pages in indeed.com are zero based indexed\n",
    "    url: str = f'https://{country}.indeed.com/jobs?q={data_job_title}&sc=0kf%3Aattr%28DSQF7%29%3B&start={modefid_page}' + \\\n",
    "    '&pp=gQClAAAAAAAAAAAAAAACCXklWwCSAQIBK7YBBwDUbujiRGBahYW5TppjdUz7DjXn3aPZSbT47IoJ5LLbuzpYcXwZdzJ6rKHf' + \\\n",
    "    '6gPWFkXVGxKKGxW-JAKb8BFo_hZAkBd7trBBTY32J2CrOuA3V9dGD_bre-lArmi9DRYlcah6hvoRfsYUNYSoQwIa8VOMZMxvH-s2Dlh' + \\\n",
    "    'UPvUP-_Dz9ls4i-OLqVGpGh4AAA&vjk=b997bb0dddadea'\n",
    "\n",
    "    soup = scrape_page(url)\n",
    "    loaded_page = int(soup.find('button', 'css-ns2mzi e8ju0x51').text)\n",
    "\n",
    "    # available_pages = []\n",
    "\n",
    "    # for button in soup.find_all('button', 'css-1qt7hdn e8ju0x50'):\n",
    "    #     available_pages.append(int(button.text))\n",
    "\n",
    "    if (int(page) != loaded_page):\n",
    "        # We will use KeyError as standard to represent getting out of the max pages.\n",
    "        raise KeyError\n",
    "        \n",
    "    print(f'Page {page} Loaded successfully.')\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccd78d2a-263b-4fdd-99f1-f91d541596ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_content(driver):\n",
    "\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'loader loader--show')\n",
    "        continue_ = True\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, 'infinite-scroller__show-more-button infinite-scroller__show-more-button--visible')\n",
    "            continue_ = True\n",
    "\n",
    "        except:\n",
    "            continue_ = False\n",
    "\n",
    "    return  continue_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(country: str, # This scraper scrapes all pages at once\n",
    "                     job_title: str) -> BeautifulSoup:\n",
    "\n",
    "    # I know that code logic is wrong but at least it's running completly fine.\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    url: str = f'https://www.linkedin.com/jobs/search/?keywords={job_title}&' + \\\n",
    "                f'location={country}&refresh=true&start=0&position=1&pageNum=0'\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            wait.until(lambda driver: check_for_content(driver))\n",
    "            # print(is_element_present(driver))\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    driver.quit()\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eaa7ad-0c7f-4b61-9608-7d0e54367505",
   "metadata": {},
   "source": [
    "## <center><strong><span style= 'color: #4895e0'>Collecting</span> jobs data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452ce6e-7b8b-4a82-b3e5-e6911937dea4",
   "metadata": {},
   "source": [
    "**Now we will need for three steps to collect the data we need:**\n",
    "\n",
    "1. Collect all pages we need from each scraper and stack them together as HTML code.\n",
    "2. Scrape for jobs cards links from linkedin and indeed.\n",
    "3. Scrape for the data in those links and store the data in a DataFrame then in SQLite\n",
    "4. We may also consider collecting total results per each platform, country etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3d2-25cd-4321-b091-d2a7914786f4",
   "metadata": {},
   "source": [
    "#### **Stack pages together as HTML code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "791acf9d-694a-417f-9015-31fceed0babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Loaded successfully.\n",
      "Page 2 Loaded successfully.\n",
      "Finished loading current country or job title.\n",
      "\n",
      "Page 1 Loaded successfully.\n",
      "Page 2 Loaded successfully.\n",
      "Page 3 Loaded successfully.\n",
      "Page 4 Loaded successfully.\n",
      "Page 5 Loaded successfully.\n",
      "Page 6 Loaded successfully.\n",
      "Page 7 Loaded successfully.\n",
      "Page 8 Loaded successfully.\n",
      "Finished loading current country or job title.\n",
      "\n",
      "Page 1 Loaded successfully.\n",
      "Page 2 Loaded successfully.\n",
      "Page 3 Loaded successfully.\n",
      "Page 4 Loaded successfully.\n",
      "Page 5 Loaded successfully.\n",
      "Page 6 Loaded successfully.\n",
      "Page 7 Loaded successfully.\n",
      "Page 8 Loaded successfully.\n",
      "Finished loading current country or job title.\n",
      "\n",
      "Page 1 Loaded successfully.\n",
      "Page 2 Loaded successfully.\n",
      "Page 3 Loaded successfully.\n",
      "Page 4 Loaded successfully.\n",
      "Page 5 Loaded successfully.\n",
      "Page 6 Loaded successfully.\n",
      "Page 7 Loaded successfully.\n",
      "Page 8 Loaded successfully.\n",
      "Finished loading current country or job title.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soups = {'soup': [],\n",
    "         'job_title': [],\n",
    "         'country': [],\n",
    "         'platform': []}\n",
    "\n",
    "flag: bool = False\n",
    "max_page: int = 7\n",
    "\n",
    "for country in indeed_countries:\n",
    "    for job_title in data_jobs_titles:\n",
    "        stacked_pages_soup: str = ''\n",
    "\n",
    "        for i in count(0):\n",
    "\n",
    "            try:\n",
    "                page = i + 1\n",
    "                soup: BeautifulSoup = indeed_scraper(country= country,\n",
    "                                                     job_title= job_title,\n",
    "                                                     page= page)\n",
    "                \n",
    "                stacked_pages_soup += '\\n<br> ' + str(soup)\n",
    "                if page > max_page:\n",
    "                    print('Finished loading current country or job title.\\n')\n",
    "                    break\n",
    "\n",
    "            except KeyError as e:\n",
    "                print('Finished loading current country or job title.\\n')\n",
    "                break\n",
    "\n",
    "        soups['platform'].append('Indeed')\n",
    "        soups['soup'].append(stacked_pages_soup)\n",
    "        soups['job_title'].append(job_title)\n",
    "        soups['country'].append(country)\n",
    "\n",
    "for country in linkedin_countries:\n",
    "    for job_title in data_jobs_titles:\n",
    "\n",
    "        soup: BeautifulSoup = linkedin_scraper(country= country,\n",
    "                                job_title= job_title)\n",
    "\n",
    "        soups['platform'].append('LinkedIn')\n",
    "        soups['soup'].append(soup)\n",
    "        soups['job_title'].append(job_title)\n",
    "        soups['country'].append(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89f6777d-a8d5-4011-8d45-db50ecb8dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n&lt;br&gt; &lt;html class=\"js-focus-visible\" data-js-...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>de</td>\n",
       "      <td>Indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n&lt;br&gt; &lt;html class=\"js-focus-visible\" data-js-...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>de</td>\n",
       "      <td>Indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n&lt;br&gt; &lt;html class=\"js-focus-visible\" data-js-...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>uk</td>\n",
       "      <td>Indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n&lt;br&gt; &lt;html class=\"js-focus-visible\" data-js-...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>uk</td>\n",
       "      <td>Indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup      job_title  \\\n",
       "0  \\n<br> <html class=\"js-focus-visible\" data-js-...     Data entry   \n",
       "1  \\n<br> <html class=\"js-focus-visible\" data-js-...  Data engineer   \n",
       "2  \\n<br> <html class=\"js-focus-visible\" data-js-...     Data entry   \n",
       "3  \\n<br> <html class=\"js-focus-visible\" data-js-...  Data engineer   \n",
       "4  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...     Data entry   \n",
       "5  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...  Data engineer   \n",
       "\n",
       "          country  platform  \n",
       "0              de    Indeed  \n",
       "1              de    Indeed  \n",
       "2              uk    Indeed  \n",
       "3              uk    Indeed  \n",
       "4  European Union  LinkedIn  \n",
       "5  European Union  LinkedIn  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(soups).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5dc7-509a-490c-bd59-b341487cb5b4",
   "metadata": {},
   "source": [
    "#### **Collecting jobs links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6983a18a-d68e-47f3-b27f-a48420b30d35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=114.0.5735.199)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tGetHandleVerifier [0x00A5A813+48355]\n",
      "\t(No symbol) [0x009EC4B1]\n",
      "\t(No symbol) [0x008F5358]\n",
      "\t(No symbol) [0x008DD293]\n",
      "\t(No symbol) [0x0093E37B]\n",
      "\t(No symbol) [0x0094C473]\n",
      "\t(No symbol) [0x0093A536]\n",
      "\t(No symbol) [0x009182DC]\n",
      "\t(No symbol) [0x009193DD]\n",
      "\tGetHandleVerifier [0x00CBAABD+2539405]\n",
      "\tGetHandleVerifier [0x00CFA78F+2800735]\n",
      "\tGetHandleVerifier [0x00CF456C+2775612]\n",
      "\tGetHandleVerifier [0x00AE51E0+616112]\n",
      "\t(No symbol) [0x009F5F8C]\n",
      "\t(No symbol) [0x009F2328]\n",
      "\t(No symbol) [0x009F240B]\n",
      "\t(No symbol) [0x009E4FF7]\n",
      "\tBaseThreadInitThunk [0x772600C9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773A7B1E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773A7AEE+238]\n",
      "\n",
      "Retrying (1/3) after 5 seconds...\n",
      "Error: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=114.0.5735.199)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tGetHandleVerifier [0x00A5A813+48355]\n",
      "\t(No symbol) [0x009EC4B1]\n",
      "\t(No symbol) [0x008F5358]\n",
      "\t(No symbol) [0x008DD293]\n",
      "\t(No symbol) [0x0093E37B]\n",
      "\t(No symbol) [0x0094C473]\n",
      "\t(No symbol) [0x0093A536]\n",
      "\t(No symbol) [0x009182DC]\n",
      "\t(No symbol) [0x009193DD]\n",
      "\tGetHandleVerifier [0x00CBAABD+2539405]\n",
      "\tGetHandleVerifier [0x00CFA78F+2800735]\n",
      "\tGetHandleVerifier [0x00CF456C+2775612]\n",
      "\tGetHandleVerifier [0x00AE51E0+616112]\n",
      "\t(No symbol) [0x009F5F8C]\n",
      "\t(No symbol) [0x009F2328]\n",
      "\t(No symbol) [0x009F240B]\n",
      "\t(No symbol) [0x009E4FF7]\n",
      "\tBaseThreadInitThunk [0x772600C9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773A7B1E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773A7AEE+238]\n",
      "\n",
      "Retrying (2/3) after 5 seconds...\n",
      "Error: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=114.0.5735.199)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tGetHandleVerifier [0x00A5A813+48355]\n",
      "\t(No symbol) [0x009EC4B1]\n",
      "\t(No symbol) [0x008F5358]\n",
      "\t(No symbol) [0x008DD293]\n",
      "\t(No symbol) [0x0093E37B]\n",
      "\t(No symbol) [0x0094C473]\n",
      "\t(No symbol) [0x0093A536]\n",
      "\t(No symbol) [0x009182DC]\n",
      "\t(No symbol) [0x009193DD]\n",
      "\tGetHandleVerifier [0x00CBAABD+2539405]\n",
      "\tGetHandleVerifier [0x00CFA78F+2800735]\n",
      "\tGetHandleVerifier [0x00CF456C+2775612]\n",
      "\tGetHandleVerifier [0x00AE51E0+616112]\n",
      "\t(No symbol) [0x009F5F8C]\n",
      "\t(No symbol) [0x009F2328]\n",
      "\t(No symbol) [0x009F240B]\n",
      "\t(No symbol) [0x009E4FF7]\n",
      "\tBaseThreadInitThunk [0x772600C9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773A7B1E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x773A7AEE+238]\n",
      "\n",
      "Retrying (3/3) after 5 seconds...\n",
      "Maximum retries exceeded. Unable to scrape the page.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     job_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m###\u001b[39m\u001b[38;5;124m'\u001b[39m, job_id[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-jk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m     indeed_jobs_links\u001b[38;5;241m.\u001b[39madd(job_link)\n\u001b[1;32m---> 22\u001b[0m     indeed_companies_links\u001b[38;5;241m.\u001b[39madd(\u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_link\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjcs-JobTitle css-jspxzf eu4oa1w0\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m soups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m total_jobs\n\u001b[0;32m     25\u001b[0m soups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobs_links\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m indeed_jobs_links\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "soups = pd.DataFrame(soups) # I will convert the soups dict to a df so we can do masking easly.\n",
    "\n",
    "soups['total_jobs'] = np.nan\n",
    "soups['jobs_links'] = np.nan\n",
    "soups['companies_links'] = np.nan\n",
    "\n",
    "indeed_jobs_links: set = set()\n",
    "indeed_companies_links: set = set()\n",
    "\n",
    "linkedin_jobs_links: set = set()\n",
    "linkedin_companies_links: set = set()\n",
    "\n",
    "\n",
    "# Collecting indeed jobs links\n",
    "for i, soup in enumerate(soups[soups['platform'] == 'Indeed']['soup']):\n",
    "    total_jobs = len(BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'))\n",
    "\n",
    "    for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "        job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "        indeed_jobs_links.append(job_link)\n",
    "\n",
    "    for job_link in job_links[:5]:\n",
    "        print(scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href'])\n",
    "    # for job_id in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0')[:5]: # TOREMOVE\n",
    "    #     job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job_id['data-jk'])\n",
    "    #     company_link = scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href']\n",
    "        \n",
    "        indeed_jobs_links.add(job_link)\n",
    "        indeed_companies_links.add(company_link)\n",
    "\n",
    "    soups['total_jobs'][i] = total_jobs\n",
    "    soups['jobs_links'][i] = indeed_jobs_links\n",
    "    soups['companies_links'][i] = indeed_companies_links\n",
    "\n",
    "\n",
    "\n",
    "# Collecting linkedIn jobs links\n",
    "# for i, soup in enumerate(soups[soups['platform'] == 'LinkedIn']['soup']):\n",
    "    \n",
    "#     soup = BeautifulSoup(soup)\n",
    "#     job_card_class = 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]'\n",
    "#     total_jobs = len(soup.find_all('a', job_card_class))\n",
    "\n",
    "#     for job_card in soup.find_all('a', job_card_class)[:5]: # TOREMOVE\n",
    "#         job_link = job_card['href']\n",
    "\n",
    "#         linkedin_jobs_links.add(job_link)\n",
    "#         linkedin_companies_links.add(scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href'])\n",
    "\n",
    "#     soups['total_jobs'][i] = total_jobs\n",
    "#     soups['jobs_links'][i] = linkedin_jobs_links\n",
    "#     soups['companies_links'][i] = linkedin_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03ab6c24-5397-4775-ba32-764be61b26c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=0f4e49c98c0536f1',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=19f3a3be9f35218f',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=1df95ac216434def',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=297a5f03a72d28ba',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=40e35f9a902792ed',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=446ba27dfffdbe36',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=48746eece7f6d42f',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=4be75a2883ca49b4',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=5010e97afafcfbf9',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=605f5275b7921be9',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=973410652b64f9a8',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=a34fc6b420419272',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=bbae1d266770b45a',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=dd8c3832576dcd38',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=dfb22eace90ab68a',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=e01edbb8769d2dbe',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=e48dcb9bb5363f91',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ec52d6d183156757',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ec8d17677e9a27db',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=fb95c004ed0f0de3'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f31a8d8-f416-4be3-80f3-626fe9485f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "for soup in soups[soups['platform'] == 'Indeed']['soup'][:2]:\n",
    "    job_links = []\n",
    "\n",
    "    for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "        job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "        job_links.append(job_link)\n",
    "\n",
    "     for job_link in job_links[:5]:\n",
    "         print(scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href'])\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8439570-b1d7-4985-bf5a-100db8b92168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(job_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c96c3fb-a2ed-4151-967b-568a14c588a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f939445-7283-4aa5-a576-d31be2b2598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html = scrape_page('https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=26c4d3754f443acb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbcc62-bafc-4fdd-96b5-18d420130a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html.find_all('div','jobsearch-JobMetadataFooter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
