{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77d6d00-1121-4b75-ba3a-9b19daaf6496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center><strong><span style= 'color: #51fcc6'>Notebook </span>Describtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067967-13d6-4976-8547-06ec1e6fb0ba",
   "metadata": {},
   "source": [
    "Hello There In this notebokk we are going to scrape the Full time and part time jobs data for the data related jobs like:<br>\n",
    "Data analyst, ML dev, Data scientist, Data engineer etc ..<br><br>\n",
    "\n",
    "**Important note:**\n",
    "We will analyze some countries only for indeed becuase indeed needs to specify which country to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b5f0-0b9e-4e51-96c7-b0c92ae000d0",
   "metadata": {},
   "source": [
    "## <center><strong>Importing <span style= 'color: #48e0dc'>Packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea612333-8338-4676-a332-959faf73ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import sqlite3\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beca8e68-9929-4d27-863f-f715f5610838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_9496\\1277823779.py:8: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "COLORS = ['#51fcc6', '#48e0dc', '#5cd3f7', '#4895e0', '#517afc']\n",
    "\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64',\n",
    "            'uint16', 'uint32', 'uint64']\n",
    "MAX_PAGES = 300\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ip_port_list = [('75.84.199.80', '80'),\n",
    "                ('172.173.241.207', '80'),\n",
    "                ('209.145.60.213', '80'),\n",
    "                ('86.109.3.28', '80'),\n",
    "                ('65.111.241.211', '80'),\n",
    "                ('144.34.162.125', '80')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556ce34-26e3-4fa6-b26c-330970e723bb",
   "metadata": {},
   "source": [
    "## <center><strong>Setting up the<span style= 'color: #5cd3f7'> web scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95942737-c384-4a2a-980d-02c24e559224",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_titles = ['Data entry', 'Data engineer',\n",
    "                    'Data scientist', 'Data analyst',\n",
    "                    'ML devoloper']\n",
    "\n",
    "indeed_countries = ['de', 'uk']#, 'www'] USA don't have to be spesfied in the URL so we will use 'www' instead\n",
    "linkedin_countries = ['European Union', 'United States']\n",
    "ip_port = random.choice(ip_port_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720f110a-3276-4b47-9da0-f65599e632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url: str, ip_and_port: tuple[str] = None, retrieve_new_url=None) -> BeautifulSoup:\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    if ip_and_port:\n",
    "        chrome_options.add_argument(f'--proxy-server={ip_and_port[0]}:{ip_and_port[1]}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            if retrieve_new_url:\n",
    "                new_url = driver.current_url\n",
    "                driver.quit()\n",
    "                return soup, new_url\n",
    "            \n",
    "            driver.quit()\n",
    "            return soup\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Retrying ({retry + 1}/{max_retries}) after {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    print(\"Maximum retries exceeded. Unable to scrape the page.\")\n",
    "    driver.quit()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcad281-40d3-44a8-9832-6b8362d1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indeed_scraper(page: int,\n",
    "#                    country: str,\n",
    "#                    job_title: str) -> BeautifulSoup:\n",
    "\n",
    "#     modefid_page = (page - 1) * 10\n",
    "\n",
    "#     data_job_title = job_title.replace(' ', '+')\n",
    "\n",
    "#     # Pages in indeed.com are zero based indexed\n",
    "#     url: str = f'https://{country}.indeed.com/jobs?q={data_job_title}&sc=0kf%3Aattr%28DSQF7%29%3B&start={modefid_page}' + \\\n",
    "#     '&pp=gQClAAAAAAAAAAAAAAACCXklWwCSAQIBK7YBBwDUbujiRGBahYW5TppjdUz7DjXn3aPZSbT47IoJ5LLbuzpYcXwZdzJ6rKHf' + \\\n",
    "#     '6gPWFkXVGxKKGxW-JAKb8BFo_hZAkBd7trBBTY32J2CrOuA3V9dGD_bre-lArmi9DRYlcah6hvoRfsYUNYSoQwIa8VOMZMxvH-s2Dlh' + \\\n",
    "#     'UPvUP-_Dz9ls4i-OLqVGpGh4AAA&vjk=b997bb0dddadea'\n",
    "\n",
    "#     soup = scrape_page(url)\n",
    "#     loaded_page = int(soup.find('button', 'css-ns2mzi e8ju0x51').text)\n",
    "\n",
    "#     # available_pages = []\n",
    "\n",
    "#     # for button in soup.find_all('button', 'css-1qt7hdn e8ju0x50'):\n",
    "#     #     available_pages.append(int(button.text))\n",
    "\n",
    "#     if (int(page) != loaded_page):\n",
    "#         # We will use KeyError as standard to represent getting out of the max pages.\n",
    "#         raise KeyError\n",
    "        \n",
    "#     print(f'Page {page} Loaded successfully.')\n",
    "\n",
    "#     return  soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd78d2a-263b-4fdd-99f1-f91d541596ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_content(driver):\n",
    "\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'loader loader--show')\n",
    "        continue_ = True\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, 'infinite-scroller__show-more-button infinite-scroller__show-more-button--visible')\n",
    "            continue_ = True\n",
    "\n",
    "        except:\n",
    "            continue_ = False\n",
    "\n",
    "    return  continue_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a6040f-fd32-488e-9886-534281049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(country: str, # This scraper scrapes all pages at once\n",
    "                     job_title: str) -> BeautifulSoup:\n",
    "\n",
    "    # I know that code logic is wrong but at least it's running completly fine.\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    url: str = f'https://www.linkedin.com/jobs/search/?keywords={job_title}&' + \\\n",
    "                f'location={country}&refresh=true&start=0&position=1&pageNum=0'\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            wait.until(lambda driver: check_for_content(driver))\n",
    "            # print(is_element_present(driver))\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    driver.quit()\n",
    "\n",
    "    return  soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eaa7ad-0c7f-4b61-9608-7d0e54367505",
   "metadata": {},
   "source": [
    "## <center><strong><span style= 'color: #4895e0'>Collecting</span> jobs data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452ce6e-7b8b-4a82-b3e5-e6911937dea4",
   "metadata": {},
   "source": [
    "**Now we will need for three steps to collect the data we need:**\n",
    "\n",
    "1. Collect all pages we need from each scraper and stack them together as HTML code.\n",
    "2. Scrape for jobs cards links from linkedin and indeed.\n",
    "3. Scrape for the data in those links and store the data in a DataFrame then in SQLite\n",
    "4. We may also consider collecting total results per each platform, country etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3d2-25cd-4321-b091-d2a7914786f4",
   "metadata": {},
   "source": [
    "#### **Stack pages together as HTML code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791acf9d-694a-417f-9015-31fceed0babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LinkedIn scraper.\n",
      "Loading the European Union data.\n",
      "Finished loading Data entry jobs from the European Union.\n",
      "Finished loading Data engineer jobs from the European Union.\n",
      "Finished loading Data scientist jobs from the European Union.\n",
      "Finished loading Data analyst jobs from the European Union.\n",
      "Finished loading ML devoloper jobs from the European Union.\n",
      "\n",
      "\n",
      "Loading the United States data.\n",
      "Finished loading Data entry jobs from the United States.\n",
      "Finished loading Data engineer jobs from the United States.\n",
      "Finished loading Data scientist jobs from the United States.\n",
      "Finished loading Data analyst jobs from the United States.\n",
      "Finished loading ML devoloper jobs from the United States.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soups = {'soup': [],\n",
    "         'job_title': [],\n",
    "         'country': [],\n",
    "         'platform': []}\n",
    "\n",
    "flag: bool = False\n",
    "max_page: int = 7\n",
    "\n",
    "# Indeed scraping TODO\n",
    "# for country in indeed_countries:\n",
    "#     for job_title in data_jobs_titles:\n",
    "#         stacked_pages_soup: str = ''\n",
    "\n",
    "#         for i in count(0):\n",
    "\n",
    "#             try:\n",
    "#                 page = i + 1\n",
    "#                 soup: BeautifulSoup = indeed_scraper(country= country,\n",
    "#                                                      job_title= job_title,\n",
    "#                                                      page= page)\n",
    "                \n",
    "#                 stacked_pages_soup += '\\n<br> ' + str(soup)\n",
    "#                 if page > max_page:\n",
    "#                     print('Finished loading current country or job title.\\n')\n",
    "#                     break\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 print('Finished loading current country or job title.\\n')\n",
    "#                 break\n",
    "\n",
    "#         soups['platform'].append('Indeed')\n",
    "#         soups['soup'].append(stacked_pages_soup)\n",
    "#         soups['job_title'].append(job_title)\n",
    "#         soups['country'].append(country)\n",
    "\n",
    "# Linked In\n",
    "print('Starting LinkedIn scraper.')\n",
    "for country in linkedin_countries:\n",
    "    \n",
    "    print(f'Loading the {country} data.')\n",
    "    for job_title in data_jobs_titles:\n",
    "\n",
    "        \n",
    "        soup: BeautifulSoup = linkedin_scraper(country= country,\n",
    "                                job_title= job_title)\n",
    "\n",
    "        soups['platform'].append('LinkedIn')\n",
    "        soups['soup'].append(soup)\n",
    "        soups['job_title'].append(job_title)\n",
    "        soups['country'].append(country)\n",
    "\n",
    "        print(f'Finished loading {job_title} jobs from the {country}.')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f6777d-a8d5-4011-8d45-db50ecb8dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>ML devoloper</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>ML devoloper</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup       job_title  \\\n",
       "0  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...      Data entry   \n",
       "1  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...   Data engineer   \n",
       "2  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...  Data scientist   \n",
       "3  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    Data analyst   \n",
       "4  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    ML devoloper   \n",
       "5  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...      Data entry   \n",
       "6  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...   Data engineer   \n",
       "7  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...  Data scientist   \n",
       "8  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    Data analyst   \n",
       "9  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    ML devoloper   \n",
       "\n",
       "          country  platform  \n",
       "0  European Union  LinkedIn  \n",
       "1  European Union  LinkedIn  \n",
       "2  European Union  LinkedIn  \n",
       "3  European Union  LinkedIn  \n",
       "4  European Union  LinkedIn  \n",
       "5   United States  LinkedIn  \n",
       "6   United States  LinkedIn  \n",
       "7   United States  LinkedIn  \n",
       "8   United States  LinkedIn  \n",
       "9   United States  LinkedIn  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(soups).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5dc7-509a-490c-bd59-b341487cb5b4",
   "metadata": {},
   "source": [
    "#### **Collecting jobs links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9bb798d-69f7-42c4-ba13-ccb313075552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soup</th>\n",
       "      <th>job_title</th>\n",
       "      <th>country</th>\n",
       "      <th>platform</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>jobs_links</th>\n",
       "      <th>companies_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>ML devoloper</td>\n",
       "      <td>European Union</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data entry</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[[\\n, &lt;meta content=\"d_jobs_guest_search\" nam...</td>\n",
       "      <td>ML devoloper</td>\n",
       "      <td>United States</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                soup       job_title  \\\n",
       "0  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...      Data entry   \n",
       "1  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...   Data engineer   \n",
       "2  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...  Data scientist   \n",
       "3  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    Data analyst   \n",
       "4  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    ML devoloper   \n",
       "5  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...      Data entry   \n",
       "6  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...   Data engineer   \n",
       "7  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...  Data scientist   \n",
       "8  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    Data analyst   \n",
       "9  [[[\\n, <meta content=\"d_jobs_guest_search\" nam...    ML devoloper   \n",
       "\n",
       "          country  platform  total_jobs  jobs_links  companies_links  \n",
       "0  European Union  LinkedIn         0.0         NaN              NaN  \n",
       "1  European Union  LinkedIn         NaN         NaN              NaN  \n",
       "2  European Union  LinkedIn         NaN         NaN              NaN  \n",
       "3  European Union  LinkedIn         NaN         NaN              NaN  \n",
       "4  European Union  LinkedIn         NaN         NaN              NaN  \n",
       "5   United States  LinkedIn         NaN         NaN              NaN  \n",
       "6   United States  LinkedIn         NaN         NaN              NaN  \n",
       "7   United States  LinkedIn         NaN         NaN              NaN  \n",
       "8   United States  LinkedIn         NaN         NaN              NaN  \n",
       "9   United States  LinkedIn         NaN         NaN              NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6983a18a-d68e-47f3-b27f-a48420b30d35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:1159\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_with_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;66;03m# We have a scalar (or for MultiIndex or object-dtype, scalar-like)\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m#  key that is not present in self.index.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:1225\u001b[0m, in \u001b[0;36mSeries._set_with_engine\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;66;03m# this is equivalent to self._values[key] = value\u001b[39;00m\n\u001b[1;32m-> 1225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\managers.py:2042\u001b[0m, in \u001b[0;36mSingleBlockManager.setitem_inplace\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m-> 2042\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\base.py:193\u001b[0m, in \u001b[0;36mSingleDataManager.setitem_inplace\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    191\u001b[0m     value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m--> 193\u001b[0m \u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     linkedin_companies_links\u001b[38;5;241m.\u001b[39mappend(company_link)\n\u001b[0;32m     43\u001b[0m soups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(linkedin_jobs_links)\n\u001b[1;32m---> 44\u001b[0m \u001b[43msoups\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjobs_links\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m linkedin_jobs_links\n\u001b[0;32m     45\u001b[0m soups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompanies_links\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m linkedin_companies_links\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:1179\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, LossySetitemError):\n\u001b[0;32m   1177\u001b[0m     \u001b[38;5;66;03m# The key was OK, but we cannot set the value losslessly\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidIndexError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# cases with MultiIndex don't get here bc they raise KeyError\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# e.g. test_basic_getitem_setitem_corner\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:1262\u001b[0m, in \u001b[0;36mSeries._set_values\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Index, Series)):\n\u001b[0;32m   1260\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_update_cacher()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\managers.py:394\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_no_reference(\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# if being referenced -> perform Copy-on-Write and clear the reference\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# this method is only called if there is a single block -> hardcoded 0\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\blocks.py:1067\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[1;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m casted\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(casted) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1065\u001b[0m         \u001b[38;5;66;03m# NumPy 1.25 deprecation: https://github.com/numpy/numpy/pull/10615\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m         casted \u001b[38;5;241m=\u001b[39m casted[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m-> 1067\u001b[0m     \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m casted\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "soups['total_jobs'] = np.nan\n",
    "soups['jobs_links'] = np.nan\n",
    "soups['companies_links'] = np.nan\n",
    "\n",
    "\n",
    "# Collecting indeed jobs links TOFIX\n",
    "# for i, soup in enumerate(soups[soups['platform'] == 'Indeed']['soup']):\n",
    "    \n",
    "#     indeed_jobs_links: list = []\n",
    "#     indeed_companies_links: list = []\n",
    "\n",
    "#     for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "#         job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "#         indeed_jobs_links.append(job_link)\n",
    "\n",
    "#     for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "#         company_link = BeautifulSoup(scrape_page(job_link)).find('a', 'css-775knl emf9s7v0')\n",
    "#         indeed_companies_links.append(company_link)\n",
    "#         print(company_link)\n",
    "        \n",
    "#     soups['total_jobs'][i] = len(indeed_jobs_links)\n",
    "#     soups['jobs_links'][i] = indeed_jobs_links\n",
    "#     soups['companies_links'][i] = indeed_companies_links\n",
    "\n",
    "\n",
    "# Collecting linkedIn jobs links\n",
    "for i, soup in enumerate(soups[soups['platform'] == 'LinkedIn']['soup']):\n",
    "\n",
    "    linkedin_jobs_links: list = []\n",
    "    linkedin_companies_links: list = []\n",
    "\n",
    "    for job in soup.find_all('a', 'disabled ember-view job-card-container__link job-card-list__title'):\n",
    "        job_link = 'https://www.linkedin.com###'.replace('###', job['href'])\n",
    "        linkedin_jobs_links.append(job_link)\n",
    "\n",
    "    for job_link in linkedin_jobs_links[:5]: # TOREMOVE\n",
    "        company_link =  'https://www.linkedin.com###'.replace('###', BeautifulSoup(\n",
    "            scrape_page(job_link)).find('a', 'ember-view link-without-visited-state inline-block t-black')['href'])\n",
    "\n",
    "        print(company_link)\n",
    "        linkedin_companies_links.append(company_link)\n",
    "\n",
    "    soups['total_jobs'][i] = len(linkedin_jobs_links)\n",
    "    soups['jobs_links'][i] = linkedin_jobs_links\n",
    "    soups['companies_links'][i] = linkedin_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ecd47cf0-37fb-4c0a-bc2a-3abc5bcfe071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=297a5f03a72d28ba',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=dfb22eace90ab68a',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=1df95ac216434def',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ec52d6d183156757',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=4be75a2883ca49b4',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ce840e854164d5f5',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=7a31018777b2a0fb',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=0ce08416199b1cb1',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=148d931cb5870ea9',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=88849361571a22a8',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=68040c6377ffaaf5',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=722a2feac0caa4db',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=db7619ac3e3a8b6f',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=48066749109741a3',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=db332094622e43af',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=cff0cc1a077aa95d',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=6cff9e542faed768',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=efd7de9072adbe04',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=709bc1b193a31e80',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=4913f43f8b8e860f',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=e946a4763d3f77c8',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=1b9d310b126878b9',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ac115f18e82d69bd',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=fc48eb6dd5bdda72',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=63938c621938c984',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=d2495a21b0a17b34',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=777b2dc2205f2616')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed_jobs_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03ab6c24-5397-4775-ba32-764be61b26c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=0f4e49c98c0536f1',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=19f3a3be9f35218f',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=1df95ac216434def',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=297a5f03a72d28ba',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=40e35f9a902792ed',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=446ba27dfffdbe36',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=48746eece7f6d42f',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=4be75a2883ca49b4',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=5010e97afafcfbf9',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=605f5275b7921be9',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=973410652b64f9a8',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=a34fc6b420419272',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=bbae1d266770b45a',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=dd8c3832576dcd38',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=dfb22eace90ab68a',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=e01edbb8769d2dbe',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=e48dcb9bb5363f91',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ec52d6d183156757',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=ec8d17677e9a27db',\n",
       " 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=fb95c004ed0f0de3'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, soup in enumerate(soups[soups['platform'] == 'Indeed']['soup']):\n",
    "\n",
    "    indeed_jobs_links: set = set()\n",
    "    indeed_companies_links: set = set()\n",
    "\n",
    "    for job in soup.find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "        job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', BeautifulSoup((job)['data-jk'])\n",
    "        indeed_jobs_links.add(job_link)\n",
    "\n",
    "    for job_link in indeed_jobs_links[:5]: # WE WILL REMOVE THIS LIMIT\n",
    "        company_link = scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href']\n",
    "        indeed_companies_links.add(company_link)\n",
    "\n",
    "    soups['total_jobs'][i] = len(indeed_jobs_links)\n",
    "    soups['jobs_links'][i] = indeed_jobs_links\n",
    "    soups['companies_links'][i] = indeed_companies_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f31a8d8-f416-4be3-80f3-626fe9485f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "for soup in soups[soups['platform'] == 'Indeed']['soup'][:2]:\n",
    "    job_links = []\n",
    "\n",
    "    for job in BeautifulSoup(soup).find_all('a', 'jcs-JobTitle css-jspxzf eu4oa1w0'):\n",
    "        job_link = 'https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=###'.replace('###', job['data-jk'])\n",
    "        job_links.append(job_link)\n",
    "\n",
    "     for job_link in job_links[:5]:\n",
    "         print(scrape_page(job_link).find('a', 'css-775knl emf9s7v0')['href'])\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8439570-b1d7-4985-bf5a-100db8b92168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(job_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c96c3fb-a2ed-4151-967b-568a14c588a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f939445-7283-4aa5-a576-d31be2b2598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html = scrape_page('https://www.indeed.com/jobs?q=d&l=&from=searchOnHP&vjk=26c4d3754f443acb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbcc62-bafc-4fdd-96b5-18d420130a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html.find_all('div','jobsearch-JobMetadataFooter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
